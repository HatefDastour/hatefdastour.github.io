

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE) &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_11/ENGG680_C11S03';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.4. Linear and Quadratic Discriminant Analyses" href="ENGG680_C11S04.html" />
    <link rel="prev" title="11.2. Principal Components Analysis (PCA)" href="ENGG680_C11S02.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ENGG680_C09.html">9. An Introduction to Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S02.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S04.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ENGG680_C10.html">10. Tree-Based Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S02.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ENGG680_C12.html">12. Introduction to Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-t-sne">11.3.1. Understanding t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanisms-of-t-sne-simplified">11.3.2. Mechanisms of t-SNE (Simplified)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanisms-of-t-sne-alternative-version-optional-content">11.3.3. Mechanisms of t-SNE (Alternative version - Optional Content)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages-of-t-sne">11.3.4. Advantages and Disadvantages of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-use-cases-of-t-sne">11.3.5. Applications and Use Cases of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caveats-and-considerations-of-t-sne">11.3.6. Caveats and Considerations of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-t-sne-with-scikit-learn-sklearn">11.3.7. Using t-SNE with scikit-learn (sklearn)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#swiss-roll-example">11.3.8. Swiss Roll Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-perplexitys-influence-on-t-sne-optional-content">11.3.9. Exploring Perplexity’s Influence on t-SNE (Optional Content)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="t-distributed-stochastic-neighbor-embedding-t-sne">
<h1><span class="section-number">11.3. </span>t-Distributed Stochastic Neighbor Embedding (t-SNE)<a class="headerlink" href="#t-distributed-stochastic-neighbor-embedding-t-sne" title="Permalink to this heading">#</a></h1>
<p>t-Distributed Stochastic Neighbor Embedding (t-SNE) is a formidable technique for visualizing high-dimensional data in a lower-dimensional space. This method is widely used for data exploration and analysis because it preserves the local intricacies within the data, which helps to identify clusters and patterns. Here is an in-depth exploration of how t-SNE works <span id="id1">[<a class="reference internal" href="../References.html#id25" title="T. Tony Cai and Rong Ma. Theoretical foundations of t-sne for visualizing high-dimensional clustered data. J. Mach. Learn. Res., jan 2022.">Cai and Ma, 2022</a>, <a class="reference internal" href="../References.html#id56" title="Van Hoan Do and Stefan Canzar. A generalization of t-sne and umap to single-cell multimodal omics. Genome Biology, 22(1):130, May 2021. doi:10.1186/s13059-021-02356-5.">Do and Canzar, 2021</a>]</span>:</p>
<div class="section" id="understanding-t-sne">
<h2><span class="section-number">11.3.1. </span>Understanding t-SNE<a class="headerlink" href="#understanding-t-sne" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a non-linear dimensionality reduction technique that focuses on preserving the pairwise similarities between data points. Its strength lies in its ability to capture complex, non-linear relationships within the data. Unlike some other techniques, t-SNE is not aimed at feature extraction or transformation; its main purpose is visual representation <span id="id2">[<a class="reference internal" href="../References.html#id108" title="Dmitry Kobak and Philipp Berens. The art of using t-sne for single-cell transcriptomics. Nature communications, 10(1):5416, 2019.">Kobak and Berens, 2019</a>, <a class="reference internal" href="../References.html#id199" title="Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008.">Van der Maaten and Hinton, 2008</a>]</span>.</p>
<ol class="arabic simple">
<li><p><strong>Establishing Pairwise Similarities</strong>: The first step of t-SNE is to compute the pairwise similarities among data points in the original high-dimensional space. Usually, these similarities are based on Euclidean distances or other relevant distance metrics.</p></li>
<li><p><strong>Mapping Similarities in Reduced Dimensions</strong>: The next step of t-SNE is to map these pairwise similarities onto a lower-dimensional space, while maintaining the intrinsic relationships. This mapping tries to bring similar data points closer and push dissimilar points further away, creating a coherent representation.</p></li>
<li><p><strong>Embracing Probability Distributions</strong>: t-SNE uses probability distributions to model the similarities in both the high-dimensional and low-dimensional spaces. It creates a probability distribution that captures the similarities between data points in the original high-dimensional space, and another distribution that suits the reduced, low-dimensional space.</p></li>
<li><p><strong>Minimizing Divergence</strong>: The core of t-SNE is to minimize the divergence, a measure of difference, between these two probability distributions. Through this intricate process, the algorithm ensures that data points with high similarities in the original space also have high similarities in the reduced dimensions.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Definition - Perplexity</p>
<p>Perplexity is a parameter that controls how t-SNE balances the attention between local and global aspects of the data. It can be loosely interpreted as the number of close neighbors each point has in the high-dimensional space. The perplexity value affects the shape and size of the clusters in the low-dimensional representation. A higher perplexity value tends to produce clearer clusters, but it may also distort the global structure of the data. A lower perplexity value preserves more of the global structure, but it may also merge distinct clusters together. There is no optimal perplexity value for all datasets, and it is recommended to try different values and see how they affect the results <span id="id3">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
</div>
<p><font color='Blue'><b>Example:</b></font>
Let’s say we have a bunch of 10,000 pictures with cats, dogs, birds, and fish. Each picture has 784 details about the pixels in a 28x28 black-and-white image. Now, we want to simplify this data to see it on a simple 2D plot.</p>
<p>To do this, we use a method called t-SNE, but we have to pick a number called perplexity. This number decides whether t-SNE should focus more on how close or how far animals are from each other in the pictures.</p>
<p>If we go for a low perplexity, like 5, t-SNE cares a lot about how close animals are in each picture. This might make it miss the big picture, so animals that are actually far apart in the pictures might end up close together on the plot. For example, pictures of cats and dogs might look like they belong to the same group, while birds and fish might be in different groups.</p>
<p>On the other hand, if we go for a high perplexity, like 50, t-SNE focuses more on the big picture. This might cause it to miss the details, so animals that are close together in the pictures might end up far apart on the plot. In this case, pictures of cats and dogs might be in different groups, while birds and fish might end up together.</p>
<p>A moderate perplexity, say 30, tries to find a balance. It considers both the closeness and the farness of animals in the pictures. This often gives a better picture where you can see clear groups of different animals, and there’s also some variety within each group.</p>
</div>
<div class="section" id="mechanisms-of-t-sne-simplified">
<h2><span class="section-number">11.3.2. </span>Mechanisms of t-SNE (Simplified)<a class="headerlink" href="#mechanisms-of-t-sne-simplified" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a nonlinear dimensionality reduction technique that aims to preserve the local structure of high-dimensional data in a lower-dimensional space. It consists of four main steps <span id="id4">[<a class="reference internal" href="../References.html#id25" title="T. Tony Cai and Rong Ma. Theoretical foundations of t-sne for visualizing high-dimensional clustered data. J. Mach. Learn. Res., jan 2022.">Cai and Ma, 2022</a>, <a class="reference internal" href="../References.html#id56" title="Van Hoan Do and Stefan Canzar. A generalization of t-sne and umap to single-cell multimodal omics. Genome Biology, 22(1):130, May 2021. doi:10.1186/s13059-021-02356-5.">Do and Canzar, 2021</a>, <a class="reference internal" href="../References.html#id108" title="Dmitry Kobak and Philipp Berens. The art of using t-sne for single-cell transcriptomics. Nature communications, 10(1):5416, 2019.">Kobak and Berens, 2019</a>, <a class="reference internal" href="../References.html#id199" title="Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008.">Van der Maaten and Hinton, 2008</a>]</span></p>
<ol class="arabic simple">
<li><p><strong>Computing pairwise similarities</strong>: For each data point, t-SNE computes the conditional probabilities that represent the similarity of a data point to another point in the high-dimensional space. These probabilities depend on the distance metrics between the data points. The perplexity parameter controls the number of neighbors each point has, affecting the shape of the Gaussian distribution used to compute these conditional probabilities.</p></li>
<li><p><strong>Mapping similarities</strong>: In this step, t-SNE tries to find a lower-dimensional representation of the data points that preserves their pairwise similarities as much as possible. Each data point is mapped to a point in the lower-dimensional space, which can have two or three dimensions. The algorithm starts with random initialization and iteratively refines the positions to minimize the divergence between the similarities in high and low dimensions.</p></li>
<li><p><strong>Using probability distributions</strong>: In this step, the algorithm models the pairwise similarities in both high-dimensional and low-dimensional spaces using probability distributions. The high-dimensional similarities are modeled by a joint probability distribution that is obtained by symmetrizing the conditional probabilities. The low-dimensional similarities are modeled by a joint probability distribution that is constructed with a Student’s t-distribution with one degree of freedom to measure the similarity between two points in the low-dimensional space. Using the t-distribution with heavier tails than a Gaussian allows t-SNE to overcome the crowding problem that occurs when mapping high-dimensional data into lower dimensions.</p></li>
<li><p><strong>Minimizing divergence</strong>: In this step, the algorithm minimizes the divergence between the probability distributions, which indicates how well the low-dimensional representation preserves the high-dimensional similarities. The divergence is measured by the Kullback-Leibler divergence, which is asymmetric and penalizes larger differences when the high-dimensional similarity is large than when the low-dimensional similarity is. This property makes t-SNE focus more on preserving the local data structure than the global structure. The algorithm uses gradient descent to minimize the Kullback-Leibler divergence, stopping either when it reaches a local minimum or after a fixed number of iterations.</p></li>
</ol>
</div>
<div class="section" id="mechanisms-of-t-sne-alternative-version-optional-content">
<h2><span class="section-number">11.3.3. </span>Mechanisms of t-SNE (Alternative version - Optional Content)<a class="headerlink" href="#mechanisms-of-t-sne-alternative-version-optional-content" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a nonlinear dimensionality reduction technique that aims to preserve the local structure of high-dimensional data in a lower-dimensional space. It consists of four main steps <span id="id5">[<a class="reference internal" href="../References.html#id25" title="T. Tony Cai and Rong Ma. Theoretical foundations of t-sne for visualizing high-dimensional clustered data. J. Mach. Learn. Res., jan 2022.">Cai and Ma, 2022</a>, <a class="reference internal" href="../References.html#id56" title="Van Hoan Do and Stefan Canzar. A generalization of t-sne and umap to single-cell multimodal omics. Genome Biology, 22(1):130, May 2021. doi:10.1186/s13059-021-02356-5.">Do and Canzar, 2021</a>, <a class="reference internal" href="../References.html#id108" title="Dmitry Kobak and Philipp Berens. The art of using t-sne for single-cell transcriptomics. Nature communications, 10(1):5416, 2019.">Kobak and Berens, 2019</a>, <a class="reference internal" href="../References.html#id199" title="Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008.">Van der Maaten and Hinton, 2008</a>]</span>:</p>
<ol class="arabic">
<li><p><strong>Finding Neighbors</strong>: In this step, the algorithm calculates how likely each data point is to choose another point as its neighbor, based on how close they are in the original space. The closer they are, the more likely they are to be neighbors. The perplexity parameter controls how many neighbors each point has, affecting how the algorithm sees the data. Mathematically, the algorithm computes the conditional probabilities that represent the likelihood of a data point selecting another point as its neighbor in the high-dimensional space. These probabilities depend on the Euclidean distances or other distance metrics between the data points. For each point <span class="math notranslate nohighlight">\(i\)</span>, the probability of choosing point <span class="math notranslate nohighlight">\(j\)</span> as its neighbor is denoted as <span class="math notranslate nohighlight">\(p_{j|i}\)</span>. The perplexity parameter affects the shape of the Gaussian distribution used to compute these conditional probabilities. A Gaussian distribution is a bell-shaped curve that shows how likely a value is to occur. A higher perplexity value means a wider and flatter Gaussian curve, which means that more points are considered as neighbors. A lower perplexity value means a narrower and taller Gaussian curve, which means that fewer points are considered as neighbors.</p></li>
<li><p><strong>Mapping Neighbors in Lower Dimensions</strong>: In this step, the algorithm tries to find a new representation of the data points in a lower-dimensional space, such as a 2D or 3D plot, that keeps their neighbor relationships as much as possible. The algorithm starts with random positions for the points in the lower-dimensional space and gradually adjusts them to make them more similar to their neighbors in the original space. Mathematically, the algorithm tries to find a lower-dimensional representation of the data points that preserves their pairwise similarities as much as possible. Each data point <span class="math notranslate nohighlight">\(i\)</span> is mapped to a point <span class="math notranslate nohighlight">\(y_i\)</span> in the lower-dimensional space, which can have two or three dimensions. The algorithm starts with random initialization of <span class="math notranslate nohighlight">\(y_i\)</span> values and iteratively refines them to minimize the divergence between the similarities in high and low dimensions.</p></li>
<li><p><strong>Using Probability Distributions</strong>: In this step, the algorithm uses probability distributions to model how similar the data points are in both the original and the lower-dimensional spaces. The original similarities are modeled by a probability distribution that is based on the neighbor likelihoods calculated in the first step. The lower-dimensional similarities are modeled by a probability distribution that is based on the distances between the points in the lower-dimensional space. The algorithm uses a special type of probability distribution that can handle the differences between high and low dimensions better than a normal distribution. Mathematically, the algorithm models the pairwise similarities in both high-dimensional and low-dimensional spaces using probability distributions. The high-dimensional similarities are modeled by a joint probability distribution <span class="math notranslate nohighlight">\(P\)</span>, which is obtained by symmetrizing the conditional probabilities <span class="math notranslate nohighlight">\(p_{j|i}\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-6a529c33-d111-4ed0-81ca-22d71785f34f">
<span class="eqno">(11.17)<a class="headerlink" href="#equation-6a529c33-d111-4ed0-81ca-22d71785f34f" title="Permalink to this equation">#</a></span>\[\begin{equation} P_{ij} =  \frac{p_{j|i} + p_{i|j}}{2n}.\end{equation}\]</div>
<p>The low-dimensional similarities are modeled by a joint probability distribution <span class="math notranslate nohighlight">\(Q\)</span>, which is constructed with a Student’s t-distribution with one degree of freedom to measure the similarity between two points in the low-dimensional space as</p>
<div class="amsmath math notranslate nohighlight" id="equation-9568c6f6-37b7-40e4-8d87-e0e2358685d5">
<span class="eqno">(11.18)<a class="headerlink" href="#equation-9568c6f6-37b7-40e4-8d87-e0e2358685d5" title="Permalink to this equation">#</a></span>\[\begin{equation} q_{ij} =\frac{\left( 1 + \|y_i - y_j \|^2 \right)^{-1}}{Z},\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is a normalization constant. A joint probability distribution is a way to show how likely two events are to occur together. A Student’s t-distribution is a type of probability distribution that has heavier tails than a normal distribution, which means that it gives more weight to extreme values. Using the t-distribution with heavier tails than a Gaussian allows t-SNE to overcome the crowding problem that occurs when mapping high-dimensional data into lower dimensions. The crowding problem is when many points in the high-dimensional space are squeezed into a small area in the low-dimensional space, making them indistinguishable.</p>
</li>
<li><p><strong>Reducing Divergence</strong>: In this step, the algorithm tries to make the probability distributions of the original and the lower-dimensional similarities as close as possible, which means that the lower-dimensional representation preserves the original structure of the data well. The algorithm measures how close the probability distributions are by using a divergence metric, which is a way to quantify the difference between two probability distributions. The algorithm uses a type of divergence metric that focuses more on the local structure of the data than the global structure. The algorithm changes the positions of the points in the lower-dimensional space to minimize the divergence metric, stopping either when it reaches a good enough solution or after a certain number of tries. Mathematically, the algorithm minimizes the divergence between the probability distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, which indicates how well the low-dimensional representation preserves the high-dimensional similarities. The divergence is measured by the Kullback-Leibler divergence, which is defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-b19fe679-90ac-40b8-b250-59d2123aa5ba">
<span class="eqno">(11.19)<a class="headerlink" href="#equation-b19fe679-90ac-40b8-b250-59d2123aa5ba" title="Permalink to this equation">#</a></span>\[\begin{equation}KL(P||Q) = \sum_{ij} P_{ij} \log(P_{ij} / Q_{ij}).\end{equation}\]</div>
<p>The Kullback-Leibler divergence is asymmetric and penalizes larger differences between <span class="math notranslate nohighlight">\(P_{ij}\)</span> and <span class="math notranslate nohighlight">\(Q_{ij}\)</span> more when <span class="math notranslate nohighlight">\(P_{ij}\)</span> is large than when <span class="math notranslate nohighlight">\(Q_{ij}\)</span> is. This property makes t-SNE focus more on preserving the local data structure than the global structure. The algorithm uses gradient descent to minimize the Kullback-Leibler divergence with respect to <span class="math notranslate nohighlight">\(y_i\)</span> values, stopping either when it reaches a local minimum or after a fixed number of iterations. Gradient descent is a method to find the optimal solution by moving in the direction of the steepest decrease of the divergence metric. A local minimum is a point where the divergence metric is lower than all nearby points. A fixed number of iterations is a limit on how many times the algorithm can try to improve the solution.</p>
</li>
</ol>
<div class="important admonition">
<p class="admonition-title">Note</p>
<p>The function is called the Kullback-Leibler divergence, and it is a way to measure how different two probability distributions are. A probability distribution is a way to show how likely something is to happen. For example, if you flip a fair coin, the probability distribution of the outcome is 50% heads and 50% tails. If you flip a biased coin, the probability distribution of the outcome may be different, such as 70% heads and 30% tails.</p>
<p>The Kullback-Leibler divergence compares two probability distributions, <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, and tells you how much information is lost or gained when you use <span class="math notranslate nohighlight">\(Q\)</span> instead of <span class="math notranslate nohighlight">\(P\)</span>. For example, if you use the biased coin instead of the fair coin, you lose some information about the true outcome. The Kullback-Leibler divergence is calculated by adding up the differences between the probabilities of each possible outcome in <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, multiplied by the logarithm of the ratio of those probabilities. The logarithm is a mathematical function that makes the differences more noticeable when they are large, and less noticeable when they are small. The function is written as</p>
<div class="amsmath math notranslate nohighlight" id="equation-215c05cb-e48a-4310-8e5f-540a42e812ac">
<span class="eqno">(11.20)<a class="headerlink" href="#equation-215c05cb-e48a-4310-8e5f-540a42e812ac" title="Permalink to this equation">#</a></span>\[\begin{equation}KL(P||Q) = \sum_{ij} P_{ij} \log(P_{ij} / Q_{ij}).\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(P_{ij}\)</span> and <span class="math notranslate nohighlight">\(Q_{ij}\)</span> are the probabilities of the outcome <span class="math notranslate nohighlight">\(ij\)</span> in <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, respectively. The sum is over all possible outcomes <span class="math notranslate nohighlight">\(ij\)</span>. The Kullback-Leibler divergence is always greater than or equal to zero, and it is zero only when <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> are exactly the same. The larger the Kullback-Leibler divergence, the more different <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> are.</p>
</div>
</div>
<div class="section" id="advantages-and-disadvantages-of-t-sne">
<h2><span class="section-number">11.3.4. </span>Advantages and Disadvantages of t-SNE<a class="headerlink" href="#advantages-and-disadvantages-of-t-sne" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a popular technique for nonlinear dimensionality reduction and data visualization. It has some advantages and disadvantages that you should be aware of before using it <span id="id6">[<a class="reference internal" href="../References.html#id66" title="Christina Ellis. When to use t-sne. https://crunchingthedata.com/when-to-use-t-sne/, 2023. [Online; accessed 01-August-2023].">Ellis, 2023</a>]</span>. Here are some of them:</p>
<!-- - **Advantages**:
  - **Cluster Preservation**: t-SNE preserves the local neighborhoods in your data, meaning that data points that are close together in the original feature space are also close together in the reduced visualization space. This makes it easy to identify clusters and patterns in your data, especially compared to other techniques such as PCA or MDS that may distort the local structure of the data.
  - **Nonlinearity Discovery**: t-SNE can capture complex nonlinear patterns in your data, unlike linear techniques such as PCA, which project your data onto a lower-dimensional subspace that preserves the maximum variance. t-SNE uses a nonlinear mapping function that preserves the pairwise similarities between data points, allowing t-SNE to reveal hidden structures and relationships in your data that may not be visible in a linear projection.

- **Disadvantages**:
  - **Perplexity Parameter**: t-SNE requires you to choose a perplexity parameter that controls how many neighbors each point has, and how much weight is given to preserving local versus global structure in your data. The perplexity parameter affects the shape of the Gaussian distribution that is used to compute the conditional probabilities in the high-dimensional space, and it can have a significant impact on the quality and interpretability of your visualization. Choosing a suitable perplexity value can be difficult, as it depends on the characteristics and density of your data, and there is no clear-cut rule or optimal value for it.
  
  - **Stochastic Nature**: t-SNE introduces randomness into its algorithmic mechanics, resulting in slightly different outcomes for multiple runs of the algorithm, depending on the initial random initialization of the low-dimensional representation and the stochastic gradient descent optimization process. This means that t-SNE may not produce consistent or reproducible results, and it may also create spurious clusters or artifacts that do not reflect the true structure of your data. To overcome this issue, you may need to run the algorithm multiple times and compare or average the results, or use other methods to assess the robustness and validity of your visualization. -->
<ul class="simple">
<li><p><strong>Advantages</strong>:</p>
<ul>
<li><p><strong>Cluster Preservation</strong>: t-SNE keeps the data points that are close together in the original space close together in the new space, meaning that you can easily see groups and patterns in your data, better than other methods that may change the shape of the data.</p></li>
<li><p><strong>Nonlinearity Discovery</strong>: t-SNE can find complex patterns in your data that are not straight lines, unlike other methods that only keep the most important features of your data. t-SNE uses a special function that keeps the similarities between data points, allowing t-SNE to show hidden structures and relationships in your data that may not be visible in a straight line projection.</p></li>
</ul>
</li>
<li><p><strong>Disadvantages</strong>:</p>
<ul>
<li><p><strong>Perplexity Parameter</strong>: t-SNE requires you to choose a perplexity parameter that controls how many neighbors each point has, and how much attention is given to keeping the local versus the global structure of your data. The perplexity parameter affects the shape of the curve that is used to calculate the likelihood of a data point choosing another point as its neighbor in the original space, and it can have a big impact on the quality and meaning of your visualization. Choosing a good perplexity value can be hard, as it depends on the features and density of your data, and there is no clear rule or best value for it.</p></li>
<li><p><strong>Stochastic Nature</strong>: t-SNE introduces randomness into its process, resulting in slightly different outcomes for different runs of the method, depending on the initial random positions of the points in the new space and the random changes of the positions to make them more similar to their neighbors in the original space. This means that t-SNE may not produce consistent or reliable results, and it may also create false groups or artifacts that do not reflect the true structure of your data. To overcome this issue, you may need to run the method multiple times and compare or average the results, or use other methods to check the strength and validity of your visualization.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="applications-and-use-cases-of-t-sne">
<h2><span class="section-number">11.3.5. </span>Applications and Use Cases of t-SNE<a class="headerlink" href="#applications-and-use-cases-of-t-sne" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a powerful technique for nonlinear dimensionality reduction and data visualization. It has many applications and use cases in various domains and fields, such as biology, computer vision, natural language processing, and more <span id="id7">[<a class="reference internal" href="../References.html#id120" title="Wentian Li, Jane E Cerise, Yaning Yang, and Henry Han. Application of t-sne to human genetic data. Journal of bioinformatics and computational biology, 15(04):1750017, 2017.">Li <em>et al.</em>, 2017</a>, <a class="reference internal" href="../References.html#id162" title="Alexander Platzer. Visualization of snps with t-sne. PloS one, 8(2):e56883, 2013.">Platzer, 2013</a>]</span>. Here are some of them:</p>
<ul class="simple">
<li><p><strong>Visualizing Clusters and Patterns</strong>: t-SNE allows you to see high-dimensional data in a lower-dimensional space, such as a 2D or 3D plot, that can be easily shown and understood. t-SNE keeps the data points that are close together in the original space close together in the new space, meaning that you can easily see groups and patterns in your data, better than other methods that may change the shape of the data. t-SNE has been widely used for seeing groups and patterns in various types of data, such as genes , cells , DNA , pictures , words , and more.</p></li>
<li><p><strong>Exploring Data Relationships</strong>: t-SNE helps you to explore relationships between data points and groups, such as how similar or different they are, how they are related, how they depend on each other, etc. t-SNE shows the similarities between data points based on how close or far they are in the original space, and puts them in a new space using a probability-based method. This helps t-SNE to show hidden structures and relationships in your data that may not be clear in a straight line projection or a simple distance table. t-SNE can also help you to discover new things or ideas about your data by allowing you to play with the visualization, such as zooming, rotating, coloring, labeling, etc.</p></li>
<li><p><strong>Anomaly Detection and Complexity Analysis</strong>: t-SNE can help you to find anomalies or outliers within datasets and provide insights into the complicated structures of complex data. t-SNE uses a special function that keeps the similarities between data points, which means that it can handle curves and high-dimensions in your data better than straight line methods such as PCA. This helps t-SNE to find anomalies or outliers that may be different from the normal patterns or groups in your data, and highlight them in the visualization . t-SNE can also help you to understand the complexity of your data by showing you how different features or dimensions affect the variation or structure of your data, and how they interact with each other .</p></li>
</ul>
</div>
<div class="section" id="caveats-and-considerations-of-t-sne">
<h2><span class="section-number">11.3.6. </span>Caveats and Considerations of t-SNE<a class="headerlink" href="#caveats-and-considerations-of-t-sne" title="Permalink to this heading">#</a></h2>
<p>t-SNE is a nonlinear dimensionality reduction technique that has some limitations and challenges that you should be aware of before using it <span id="id8">[<a class="reference internal" href="../References.html#id103" title="Bo Kang, Dario Garcia Garcia, Jefrey Lijffijt, Raúl Santos-Rodríguez, and Tijl De Bie. Conditional t-sne: more informative t-sne embeddings. Machine Learning, 110:2905–2940, 2021.">Kang <em>et al.</em>, 2021</a>, <a class="reference internal" href="../References.html#id154" title="Varsha Nemade, Sunil Pathak, and Ashutosh Kumar Dubey. A systematic literature review of breast cancer diagnosis using machine intelligence techniques. Archives of Computational Methods in Engineering, 29(6):4401–4430, 2022.">Nemade <em>et al.</em>, 2022</a>]</span>. Here are some of them:</p>
<ul class="simple">
<li><p><strong>Local Emphasis</strong>: t-SNE does not always preserve the global structure of the data in the lower-dimensional space, as it focuses more on preserving the local neighborhoods in the data. This may distort or lose the global structure of the data, such as hierarchies, clusters, or trends. t-SNE also uses an asymmetric divergence measure, which means that it penalizes large differences between high-dimensional and low-dimensional similarities more when the high-dimensional similarities are large than when they are small. This may create isolated clusters that do not reflect the true structure of the data, and may also fail to capture global relationships or hierarchies in the data.</p></li>
<li><p><strong>Computational Demand</strong>: t-SNE may pose computational difficulties and scalability issues for very large datasets, as it requires computing pairwise similarities between all data points in both the high-dimensional and low-dimensional spaces, which can be very expensive and time-consuming. The computational complexity of t-SNE is <span class="math notranslate nohighlight">\(O(n^2)\)</span>, where n is the number of data points. Although there are some methods to speed up t-SNE, such as using approximate nearest neighbors or exploiting sparsity, they may introduce errors or trade-offs in the quality of the visualization.</p></li>
<li><p><strong>Interpretational Nuances</strong>: t-SNE may not be easy or intuitive to interpret the lower-dimensional space produced by t-SNE, as it uses a nonlinear mapping function that preserves the pairwise similarities between data points and cannot be easily related to the original feature space. Moreover, t-SNE may produce different results depending on various factors, such as the perplexity parameter, the initial random initialization, the optimization process, and the random seed. This means that t-SNE may not produce consistent or reproducible results, and it may also create spurious clusters or artifacts that do not reflect the true structure of the data.</p></li>
</ul>
</div>
<div class="section" id="using-t-sne-with-scikit-learn-sklearn">
<h2><span class="section-number">11.3.7. </span>Using t-SNE with scikit-learn (sklearn)<a class="headerlink" href="#using-t-sne-with-scikit-learn-sklearn" title="Permalink to this heading">#</a></h2>
<p>Scikit-learn’s <code class="docutils literal notranslate"><span class="pre">TSNE</span></code> class implements the t-SNE algorithm in a user-friendly manner. Here’s an overview of how the algorithm works within scikit-learn:</p>
<ol class="arabic simple">
<li><p><strong>Importing the Necessary Module:</strong>
Start by importing the <code class="docutils literal notranslate"><span class="pre">TSNE</span></code> class from the <code class="docutils literal notranslate"><span class="pre">sklearn.manifold</span></code> module:</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Creating an Instance:</strong>
Instantiate the <code class="docutils literal notranslate"><span class="pre">TSNE</span></code> class with the desired parameters. Key parameters include:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_components</span></code>: The number of dimensions for the visualization (usually 2 or 3).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">perplexity</span></code>: A hyperparameter that balances preserving local and global structures.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>: The step size for gradient descent optimization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_iter</span></code>: The number of iterations for the optimization process.</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Fitting and Transforming:</strong>
Use the <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> method to apply t-SNE to your high-dimensional data. This method takes your data as input and returns the transformed data in the lower-dimensional space. To test this we use the “load_digits” dataset <span id="id9">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>  <span class="c1"># Import TSNE</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Load the digits dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Create a TSNE instance</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Initialize t-SNE with 2 components</span>

<span class="c1"># Fit and transform the data with t-SNE to project from 64 to 2 dimensions</span>
<span class="n">X_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Print shapes before and after t-SNE</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original data shape:&quot;</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Projected data shape:&quot;</span><span class="p">,</span> <span class="n">X_tsne</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Get a list of 10 distinct colors from a Seaborn colormap</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Spectral&quot;</span><span class="p">,</span> <span class="n">n_colors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">))</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_tsne</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X_tsne</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                          <span class="n">hue</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span>
                          <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Set labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Component 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Component 2&#39;</span><span class="p">,</span>
       <span class="n">title</span><span class="o">=</span><span class="s1">&#39;t-SNE Projection of Handwritten Digits&#39;</span><span class="p">)</span>

<span class="c1"># Display the plot with tight layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># Show the plot</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original data shape: (1797, 64)
Projected data shape: (1797, 2)
</pre></div>
</div>
<img alt="../_images/98cb49d63be8c016a20619a09401671f85828aaa104f0125f109136cbcc1c9c9.png" src="../_images/98cb49d63be8c016a20619a09401671f85828aaa104f0125f109136cbcc1c9c9.png" />
</div>
</div>
<p>The plot shows that t-SNE has preserved the local neighborhoods in the data, meaning that data points that are close together in the original feature space are also close together in the reduced visualization space. This makes it easy to identify clusters and patterns in the data, such as the separation of different digits, the similarity of some digits, and the variation within some digits. The plot also shows that t-SNE has captured some nonlinear patterns in the data, such as the curvature of some clusters, the overlap of some clusters, and the outliers of some clusters. The plot also shows that t-SNE has not preserved the global structure of the data, such as the relative distances or orientations of the clusters, and that it may have created some spurious clusters or artifacts that do not reflect the true structure of the data, such as the isolated points or the gaps between clusters.</p>
</div>
<div class="section" id="swiss-roll-example">
<h2><span class="section-number">11.3.8. </span>Swiss Roll Example<a class="headerlink" href="#swiss-roll-example" title="Permalink to this heading">#</a></h2>
<p>In this example, we will compare two popular nonlinear dimensionality reduction techniques: T-distributed Stochastic Neighbor Embedding (t-SNE) and Locally Linear Embedding (LLE). We will use the Swiss Roll dataset, which is a synthetic dataset that consists of a 2-dimensional manifold embedded in a 3-dimensional space, resembling a Swiss roll cake. The dataset has a nonlinear structure that cannot be captured by linear techniques such as PCA. We will apply t-SNE and LLE to the dataset and visualize the results in a 2-dimensional space. We will also explore how t-SNE and LLE perform on a modified version of the dataset that contains a void, which is a region where no data points are present <span id="id10">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Source code:</span>
<span class="c1"># https://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">manifold</span>

<span class="c1"># Generate Swiss Roll dataset</span>
<span class="n">sr_points</span><span class="p">,</span> <span class="n">sr_color</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_swiss_roll</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># 3D Subplot</span>
<span class="n">ax3d</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax3d</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sr_points</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sr_points</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">sr_points</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
             <span class="n">c</span><span class="o">=</span><span class="n">sr_color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">)</span>
<span class="n">ax3d</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Swiss Roll in 3D&quot;</span><span class="p">)</span>

<span class="c1"># 2D Subplot using t-SNE</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sr_points_2d</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sr_points</span><span class="p">)</span>
<span class="n">ax2d</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">ax2d</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sr_points_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sr_points_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">sr_color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">)</span>
<span class="n">ax2d</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;t-SNE Projection in 2D&quot;</span><span class="p">)</span>

<span class="c1"># Show the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0c83e36fd09aa0482eb1cafcb03ac83b4a02906e2142405d8b71bed3259ba424.png" src="../_images/0c83e36fd09aa0482eb1cafcb03ac83b4a02906e2142405d8b71bed3259ba424.png" />
</div>
</div>
<p>Here is what the code does:</p>
<ul class="simple">
<li><p>The code imports matplotlib.pyplot, which is a Python library for creating plots and graphs.</p></li>
<li><p>The code also imports datasets and manifold from sklearn, which are Python modules for generating and manipulating data and applying manifold learning techniques, respectively.</p></li>
<li><p>The code uses the datasets module to generate a synthetic dataset called the Swiss Roll, which consists of 1500 data points that form a 2-dimensional manifold embedded in a 3-dimensional space, resembling a Swiss roll cake. The dataset has a nonlinear structure that cannot be captured by linear techniques such as PCA. The code also assigns a color to each data point based on its position along the manifold.</p></li>
<li><p>The code creates a figure with two subplots using matplotlib.pyplot, one for showing the Swiss Roll in 3D and the other for showing its projection in 2D.</p></li>
<li><p>The code uses the manifold module to create a TSNE instance with two components and a random state of 0, which means that it will use the t-SNE algorithm to project the 3-dimensional data into a 2-dimensional space, and that it will use the same random seed for the initial random initialization and the stochastic gradient descent optimization process.</p></li>
<li><p>The code fits and transforms the data with t-SNE, which means that it computes the pairwise similarities between the data points in both the high-dimensional and low-dimensional spaces, and minimizes the divergence between them using the Kullback-Leibler divergence as the measure.</p></li>
<li><p>The code uses matplotlib.pyplot to create a scatter plot for the 3D subplot, which shows the Swiss Roll in 3D, colored by the position along the manifold, and with some aesthetic options such as size, alpha, and colormap.</p></li>
<li><p>The code also uses matplotlib.pyplot to create a scatter plot for the 2D subplot, which shows the t-SNE projection in 2D, colored by the same position along the manifold, and with the same aesthetic options as the 3D subplot.</p></li>
<li><p>The code sets the titles of the subplots as “Swiss Roll in 3D” and “t-SNE Projection in 2D”, which describe the purpose and content of the visualizations.</p></li>
<li><p>The code displays the plots with a tight layout, which adjusts the spacing and margins of the plots to fit the figure size.</p></li>
</ul>
<p>The plots show that t-SNE has preserved the local neighborhoods in the data, meaning that data points that are close together in the original feature space are also close together in the reduced visualization space. This makes it easy to identify clusters and patterns in the data, such as the separation of different regions of the Swiss roll. The plots also show that t-SNE has captured some nonlinear patterns in the data, such as the curvature of the Swiss roll, and that it has not distorted or lost the global structure of the data, such as the shape of the Swiss roll. The plots also show that t-SNE has not created any spurious clusters or artifacts that do not reflect the true structure of the data, such as isolated points or gaps between clusters.</p>
<p>While t-SNE managed to maintain the overall structure of the data, it fell short in accurately representing the inherent continuity of our initial dataset. This was evident as t-SNE appeared to unnecessarily cluster certain groups of points together, which did not align with the continuous nature of the original data <span id="id11">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
</div>
<div class="section" id="exploring-perplexitys-influence-on-t-sne-optional-content">
<h2><span class="section-number">11.3.9. </span>Exploring Perplexity’s Influence on t-SNE (Optional Content)<a class="headerlink" href="#exploring-perplexitys-influence-on-t-sne-optional-content" title="Permalink to this heading">#</a></h2>
<p>Perplexity is a parameter that controls how many neighbors each data point has, and how much emphasis is placed on preserving local versus global structure in the data. Different perplexity values can have different effects on t-SNE’s behavior, depending on the characteristics and density of the data. To understand this better, let’s look at two examples: the two concentric circles and the S-curve datasets.</p>
<p>The two concentric circles dataset consists of two rings of data points, one inside the other, forming a 2-dimensional manifold embedded in a 3-dimensional space. The S-curve dataset consists of a twisted curve of data points, forming a 1-dimensional manifold embedded in a 3-dimensional space. Both datasets have a nonlinear structure that cannot be captured by linear techniques such as PCA.</p>
<p>As we increase the perplexity value, we can see a clear trend: the resulting shapes become more distinct and defined. This means that t-SNE preserves the local neighborhoods in the data better, and separates the different regions of the manifold more clearly. However, this does not mean that the size, distance, and shape of the clusters are always accurate or consistent, as they can vary due to factors such as initialization and perplexity choices. This variation does not always carry a direct interpretation <span id="id12">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>For example, t-SNE applied to the two concentric circles dataset with higher perplexities successfully uncovers the meaningful topology of the concentric circles. However, there may be slight differences in the circle size and distance from the original configuration. On the other hand, t-SNE applied to the S-curve dataset with higher perplexities may still produce visible deviations in the shapes, drifting away from the S-curve topology <span id="id13">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>For a more in-depth exploration of the intricate interplay between parameters, you can refer to “How to Use t-SNE Effectively” <span id="id14">[<a class="reference internal" href="../References.html#id204" title="Martin Wattenberg, Fernanda Viégas, and Ian Johnson. How to use t-sne effectively. Distill, 2016. URL: http://distill.pub/2016/misread-tsne, doi:10.23915/distill.00002.">Wattenberg <em>et al.</em>, 2016</a>]</span>. This resource not only engages in a thorough discourse on the influence of diverse parameters but also offers interactive plots to facilitate a nuanced understanding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Narine Kokhlikyan &lt;narine@slice.com&gt;</span>
<span class="c1"># License: BSD</span>
<span class="c1"># The code is available at: https://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">NullFormatter</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">manifold</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">generate_and_plot_dataset</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color_map</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color_map</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color_map</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">apply_tsne_and_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">perplexity</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">color_map</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">tsne</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span>
                         <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="n">perplexity</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">iterations</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset, perplexity = </span><span class="si">{</span><span class="n">perplexity</span><span class="si">}</span><span class="s2">, iterations = </span><span class="si">{</span><span class="n">iterations</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">t1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2g</span><span class="si">}</span><span class="s2"> sec&quot;</span><span class="p">)</span>

    <span class="c1"># Create a mapping for colors based on unique class labels</span>
    <span class="n">unique_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">label_to_color</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">color</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">,</span> <span class="n">color_map</span><span class="p">)}</span>

    <span class="c1"># Map class labels to colors</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_to_color</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Perplexity = </span><span class="si">{</span><span class="n">perplexity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>


<span class="c1"># Set parameters</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">perplexities</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="n">iterations_circles</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">iterations_s_curve</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">iterations_uniform_grid</span> <span class="o">=</span> <span class="mi">400</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Generate and plot the circles dataset</span>
<span class="n">X_circles</span><span class="p">,</span> <span class="n">y_circles</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">color_map_circles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">]</span>
<span class="n">generate_and_plot_dataset</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_circles</span><span class="p">,</span> <span class="n">y_circles</span><span class="p">,</span> <span class="n">color_map_circles</span><span class="p">,</span> <span class="s2">&quot;Original Circles&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">perplexity</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">perplexities</span><span class="p">):</span>
    <span class="n">apply_tsne_and_plot</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_circles</span><span class="p">,</span> <span class="n">y_circles</span><span class="p">,</span> <span class="n">perplexity</span><span class="p">,</span> <span class="n">iterations_circles</span><span class="p">,</span> <span class="n">color_map_circles</span><span class="p">)</span>

<span class="c1"># Generate and plot the S-curve dataset</span>
<span class="n">X_s_curve</span><span class="p">,</span> <span class="n">color_s_curve</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_s_curve</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_s_curve</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_s_curve</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color_s_curve</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original S-curve&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">perplexity</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">perplexities</span><span class="p">):</span>
    <span class="n">apply_tsne_and_plot</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_s_curve</span><span class="p">,</span> <span class="n">color_s_curve</span><span class="p">,</span> <span class="n">perplexity</span><span class="p">,</span> <span class="n">iterations_s_curve</span><span class="p">,</span> <span class="n">color_s_curve</span><span class="p">)</span>


<span class="c1"># Generate and plot a 2D uniform grid</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)))</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">X_uniform_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
<span class="n">color_uniform_grid</span> <span class="o">=</span> <span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_uniform_grid</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_uniform_grid</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color_uniform_grid</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">NullFormatter</span><span class="p">())</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original Uniform Grid&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">perplexity</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">perplexities</span><span class="p">):</span>
    <span class="n">apply_tsne_and_plot</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_uniform_grid</span><span class="p">,</span> <span class="n">color_uniform_grid</span><span class="p">,</span> <span class="n">perplexity</span><span class="p">,</span>
                        <span class="n">iterations_uniform_grid</span><span class="p">,</span> <span class="n">color_uniform_grid</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>circles, perplexity = 5 in 0.093 sec
circles, perplexity = 30 in 0.17 sec
circles, perplexity = 50 in 0.19 sec
circles, perplexity = 100 in 0.19 sec
S-curve, perplexity = 5 in 0.099 sec
S-curve, perplexity = 30 in 0.17 sec
S-curve, perplexity = 50 in 0.19 sec
S-curve, perplexity = 100 in 0.19 sec
uniform grid, perplexity = 5 in 0.12 sec
uniform grid, perplexity = 30 in 0.21 sec
uniform grid, perplexity = 50 in 0.22 sec
uniform grid, perplexity = 100 in 0.23 sec
</pre></div>
</div>
<img alt="../_images/d426a82d5de6e68b9409762ec02c11714fd926221b2869320f2156b9b01d1c9a.png" src="../_images/d426a82d5de6e68b9409762ec02c11714fd926221b2869320f2156b9b01d1c9a.png" />
</div>
</div>
<p>Let’s break down the results:</p>
<ol class="arabic simple">
<li><p><strong>Circles Dataset</strong>:</p>
<ul class="simple">
<li><p>Perplexity values: 5, 30, 50, 100</p></li>
<li><p>For each perplexity value, t-SNE is applied to the circles dataset (where points are distributed in two concentric circles).</p></li>
<li><p>The execution times are relatively low, ranging from approximately 0.07 to 0.17 seconds.</p></li>
<li><p>As perplexity increases, t-SNE generally takes a bit more time, likely because the algorithm needs to compute probabilities over a larger neighborhood.</p></li>
</ul>
</li>
<li><p><strong>S-curve Dataset</strong>:</p>
<ul class="simple">
<li><p>Perplexity values: 5, 30, 50, 100</p></li>
<li><p>The S-curve dataset is a 3D curve embedded in a 3D space.</p></li>
<li><p>Similar to the circles dataset, the execution times are reasonable, ranging from around 0.08 to 0.19 seconds.</p></li>
<li><p>As with the circles dataset, increasing perplexity tends to increase execution time.</p></li>
</ul>
</li>
<li><p><strong>Uniform Grid Dataset</strong>:</p>
<ul class="simple">
<li><p>Perplexity values: 5, 30, 50, 100</p></li>
<li><p>This dataset consists of points arranged in a 2D uniform grid.</p></li>
<li><p>The execution times are again relatively low, ranging from approximately 0.11 to 0.2 seconds.</p></li>
<li><p>As perplexity increases, the execution time slightly increases as well.</p></li>
</ul>
</li>
</ol>
<p>The output is a set of graphs that show the original datasets and the t-SNE results with different perplexity values. You can see how the perplexity parameter affects the outcome of the t-SNE algorithm. A low perplexity value tends to emphasize local aspects of the data, while a high perplexity value tends to emphasize global aspects of the data. There is no optimal value for perplexity, as it depends on the nature and scale of the data. You can experiment with different values and see how they change the visualization.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_11"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C11S02.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11.2. </span>Principal Components Analysis (PCA)</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C11S04.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11.4. </span>Linear and Quadratic Discriminant Analyses</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-t-sne">11.3.1. Understanding t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanisms-of-t-sne-simplified">11.3.2. Mechanisms of t-SNE (Simplified)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mechanisms-of-t-sne-alternative-version-optional-content">11.3.3. Mechanisms of t-SNE (Alternative version - Optional Content)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages-of-t-sne">11.3.4. Advantages and Disadvantages of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-use-cases-of-t-sne">11.3.5. Applications and Use Cases of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caveats-and-considerations-of-t-sne">11.3.6. Caveats and Considerations of t-SNE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-t-sne-with-scikit-learn-sklearn">11.3.7. Using t-SNE with scikit-learn (sklearn)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#swiss-roll-example">11.3.8. Swiss Roll Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-perplexitys-influence-on-t-sne-optional-content">11.3.9. Exploring Perplexity’s Influence on t-SNE (Optional Content)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>