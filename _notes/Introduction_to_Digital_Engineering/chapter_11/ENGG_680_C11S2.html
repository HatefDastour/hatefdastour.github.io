

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>11.2. Principal Components Analysis (PCA) &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_11/ENGG_680_C11S2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)" href="ENGG_680_C11S3.html" />
    <link rel="prev" title="11.1. Introduction to Dimensionality Reduction" href="ENGG_680_C11S1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S2.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S3.html">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Principal Components Analysis (PCA)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">11.2.1. Mathematical Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-essence-of-principal-components-analysis-pca">11.2.2. The Essence of Principal Components Analysis (PCA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-dimensionality-reduction">11.2.3. PCA for Dimensionality Reduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilizing-pca-for-visualization-handwritten-digits">11.2.3.1. Utilizing PCA for Visualization: Handwritten Digits</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="principal-components-analysis-pca">
<h1><span class="section-number">11.2. </span>Principal Components Analysis (PCA)<a class="headerlink" href="#principal-components-analysis-pca" title="Permalink to this headline">#</a></h1>
<p>Principal Components Analysis (PCA) is a widely used statistical technique that plays a crucial role in reducing the dimensionality of high-dimensional data while retaining its inherent variability. This is accomplished by transforming the original variables into a new set of uncorrelated variables referred to as principal components. These principal components are linear combinations of the original variables, and they are ordered in a way that the first component explains the maximum variance, the second component explains the second highest variance, and so on.</p>
<div class="section" id="mathematical-formulation">
<h2><span class="section-number">11.2.1. </span>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">#</a></h2>
<p>PCA is a powerful tool when dealing with a large set of interconnected variables, enabling the compression of this set into a smaller group of representative variables that collectively capture the major part of the initial set’s variability. This is particularly valuable for tasks such as data visualization and exploratory analysis <span id="id1">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>Consider a scenario where we possess data on n observations, each with measurements across a collection of p features denoted as <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>, <span class="math notranslate nohighlight">\(X_p\)</span>.</p>
<ol class="arabic">
<li><p>Given a dataset represented by an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix X, the loading vector of the first principal component is determined by solving the subsequent optimization problem:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8f238f67-48a9-4d99-bbb4-6f88901c4552">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-8f238f67-48a9-4d99-bbb4-6f88901c4552" title="Permalink to this equation">#</a></span>\[\begin{align}
   \max_{\phi_{11}, \ldots, \phi_{p1}} \left\{ \frac{1}{n} \sum_{i=1}^{n} \left( \sum_{j=1}^{p} \phi_{j1} x_{ij} \right)^2 \right\}, \quad \text{subject to} \sum_{j=1}^{p} \phi_{j1}^2.
   \end{align}\]</div>
<p>Introducing an intermediate variable <span class="math notranslate nohighlight">\(z_{ij} = \sum_{j=1}^{p} \phi_{j1} x_{ij}\)</span> allows us to reformulate the problem:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5b4a26b2-e6d4-4d35-af5f-49afdfc45737">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-5b4a26b2-e6d4-4d35-af5f-49afdfc45737" title="Permalink to this equation">#</a></span>\[\begin{align}
   \max_{\phi_{11}, \ldots, \phi_{p1}} \left\{ \frac{1}{n} \sum_{i=1}^{n} z_{ij}^2 \right\}, \quad \text{subject to} \sum_{j=1}^{p} \phi_{j1}^2.
   \end{align}\]</div>
<p>Importantly, <span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^{n} z_{ij}^2 = 0\)</span> due to <span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^{n} x_{ij} = 0\)</span>. Furthermore, the terms <span class="math notranslate nohighlight">\(z_{ij}\)</span> are commonly known as the <strong>scores</strong> of the first principal component.</p>
</li>
<li><p>The second principal component can be understood as the linear combination of <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>, <span class="math notranslate nohighlight">\(X_p\)</span> that possesses the highest <strong>variance</strong> among all linear combinations that are <strong>uncorrelated</strong> with the first principal component, denoted as <span class="math notranslate nohighlight">\(Z_1\)</span>. The scores of the second principal component, represented as <span class="math notranslate nohighlight">\(z_{12}\)</span>, <span class="math notranslate nohighlight">\(z_{22}\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>, <span class="math notranslate nohighlight">\(z_{n2}\)</span>, are given by the formula:</p>
<div class="math notranslate nohighlight">
\[z_{i2} = \sum_{j=1}^{p} \phi_{j2} x_{ij}\]</div>
<p>In this context, <span class="math notranslate nohighlight">\(\phi_{2} = \left[ \phi_{12}, \phi_{22}, \ldots, \phi_{p2} \right]\)</span> indicates the loading vector associated with the second principal component. Essentially, this vector captures the weightings assigned to each feature when constructing the second principal component.</p>
</li>
</ol>
</div>
<div class="section" id="the-essence-of-principal-components-analysis-pca">
<h2><span class="section-number">11.2.2. </span>The Essence of Principal Components Analysis (PCA)<a class="headerlink" href="#the-essence-of-principal-components-analysis-pca" title="Permalink to this headline">#</a></h2>
<p>Here’s a concise explanation of PCA <span id="id2">[<a class="reference internal" href="../References.html#id67" title="I.T. Jolliffe. Principal Component Analysis. Springer Series in Statistics. Springer New York, 2013. ISBN 9781475719048. URL: https://books.google.ca/books?id=-ongBwAAQBAJ.">Jolliffe, 2013</a>]</span>:</p>
<p><strong>Objective</strong>: PCA aims to find a set of orthogonal axes (principal components) in the high-dimensional space that captures the most significant variance in the data.</p>
<p><strong>Steps</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Data Standardization</strong>: Before performing PCA, it’s common to standardize the data to have zero mean and unit variance. This is crucial as it ensures that variables with larger scales don’t disproportionately influence the analysis.</p></li>
<li><p><strong>Covariance Matrix</strong>: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between variables.</p></li>
<li><p><strong>Eigenvalue Decomposition</strong>: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors are the directions of the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each component.</p></li>
<li><p><strong>Principal Component Scores</strong>: Project the original data onto the principal component axes to obtain the principal component scores. These scores represent the data in the new coordinate system defined by the principal components.</p></li>
</ol>
<p><strong>Applications</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Dimensionality Reduction</strong>: PCA is used to reduce the number of dimensions while retaining the most important information. This is particularly helpful when dealing with high-dimensional data that can be difficult to visualize or analyze.</p></li>
<li><p><strong>Data Visualization</strong>: By plotting the data using the first two principal components as axes, it’s possible to visualize the inherent structure of the data in a lower-dimensional space.</p></li>
<li><p><strong>Noise Reduction</strong>: PCA can help reduce noise in the data by focusing on the components that explain the most variance and potentially ignoring noise-dominated components.</p></li>
<li><p><strong>Feature Selection</strong>: In some cases, PCA can be used as a form of feature selection, as it provides insights into which original features contribute most to the principal components.</p></li>
</ol>
<p><strong>Limitations</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Loss of Interpretability</strong>: While PCA is powerful for dimensionality reduction, the resulting principal components might not have a direct interpretation in terms of the original features.</p></li>
<li><p><strong>Linearity Assumption</strong>: PCA assumes that the underlying relationships in the data are linear. If the data exhibits nonlinear relationships, PCA might not be the most suitable technique.</p></li>
<li><p><strong>Information Loss</strong>: Although PCA retains most of the variance, there is some amount of information loss when reducing dimensions.</p></li>
</ol>
<div class="admonition-pca-algorithm admonition">
<p class="admonition-title">PCA algorithm</p>
<p>The PC (Principal Component) algorithm is a technique used for dimensionality reduction and feature extraction. It aims to find a set of orthogonal axes, called principal components, along which the data varies the most. These principal components are linear combinations of the original features. Below, I’ll outline the mathematical steps of the PC algorithm <span id="id3">[<a class="reference internal" href="../References.html#id69" title="Tony Phillips. Principal component analysis. https://mathvoices.ams.org/featurecolumn/2021/08/01/principal-component-analysis/, 2023. [Online; accessed 01-August-2023].">Phillips, 2023</a>]</span>:</p>
<p><strong>Step 1: Data Preprocessing</strong></p>
<ul class="simple">
<li><p>Collect and preprocess the dataset if needed. Ensure that the data is centered (i.e., subtract the mean from each feature) to have a mean of zero.</p></li>
</ul>
<p><strong>Step 2: Calculate the Covariance Matrix</strong></p>
<ul>
<li><p>Compute the covariance matrix (<span class="math notranslate nohighlight">\( \Sigma \)</span>) of the preprocessed data. The covariance between two features <span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( x_j \)</span> is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e6a81e8-e0e2-4c83-9be0-413116f5ee2c">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-6e6a81e8-e0e2-4c83-9be0-413116f5ee2c" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Cov}(x_i, x_j) = \frac{1}{n-1} \sum_{k=1}^{n} (x_i^{(k)} - \bar{x}_i)(x_j^{(k)} - \bar{x}_j) \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i^{(k)} \)</span> and <span class="math notranslate nohighlight">\( x_j^{(k)} \)</span> are the values of features <span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( x_j \)</span> for data point <span class="math notranslate nohighlight">\( k \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{x}_i \)</span> and <span class="math notranslate nohighlight">\( \bar{x}_j \)</span> are the means of features <span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( x_j \)</span> across all data points.</p></li>
</ul>
</li>
<li><p>The covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span> is a symmetric matrix where each element <span class="math notranslate nohighlight">\( \Sigma_{ij} \)</span> represents the covariance between features <span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( x_j \)</span>.</p></li>
</ul>
<p><strong>Step 3: Eigendecomposition</strong></p>
<ul class="simple">
<li><p>Perform an eigendecomposition on the covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span> to find its eigenvalues (<span class="math notranslate nohighlight">\( \lambda_1, \lambda_2, \ldots, \lambda_p \)</span>) and corresponding eigenvectors (<span class="math notranslate nohighlight">\( \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p \)</span>), where <span class="math notranslate nohighlight">\( p \)</span> is the number of features.</p></li>
</ul>
<p><strong>Step 4: Sort Eigenvalues and Eigenvectors</strong></p>
<ul class="simple">
<li><p>Sort the eigenvalues in descending order, and arrange the corresponding eigenvectors accordingly.</p></li>
</ul>
<p><strong>Step 5: Select Principal Components</strong></p>
<ul class="simple">
<li><p>Choose the top <span class="math notranslate nohighlight">\( k \)</span> eigenvectors corresponding to the <span class="math notranslate nohighlight">\( k \)</span> largest eigenvalues to form a matrix <span class="math notranslate nohighlight">\( \mathbf{V}_k \)</span>. These are the principal components that capture the most variance in the data.</p></li>
</ul>
<p><strong>Step 6: Projection</strong></p>
<ul>
<li><p>Project the original data onto the subspace spanned by the selected <span class="math notranslate nohighlight">\( k \)</span> principal components to obtain the reduced-dimensional representation of the data. The projection of a data point <span class="math notranslate nohighlight">\( \mathbf{x} \)</span> onto the subspace is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1819a56c-fd0c-402c-b39a-72685e1731e7">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-1819a56c-fd0c-402c-b39a-72685e1731e7" title="Permalink to this equation">#</a></span>\[\begin{equation} \mathbf{X}_{\text{new}} = \mathbf{X} \cdot \mathbf{V}_k \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \mathbf{X}_{\text{new}} \)</span> is the reduced-dimensional data.</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{X} \)</span> is the original centered data.</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{V}_k \)</span> is the matrix of the top <span class="math notranslate nohighlight">\( k \)</span> principal components.</p></li>
</ul>
</li>
</ul>
<p>The result is a transformed dataset that retains most of the variance while reducing the dimensionality.</p>
<p>The PC algorithm is a fundamental technique in dimensionality reduction and feature extraction, often used as a precursor to other machine learning tasks to reduce computational complexity and noise in the data.</p>
</div>
<div class="admonition-calculation-of-covarinace admonition">
<p class="admonition-title">Calculation of Covarinace</p>
<p>The covariance between two variables measures the extent to which they vary together. Mathematically, the covariance between two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is calculated as <span id="id4">[<a class="reference internal" href="../References.html#id68" title="R.S. Witte and J.S. Witte. Statistics. Wiley, 2017. ISBN 9781119254515. URL: https://books.google.ca/books?id=KcxjDwAAQBAJ.">Witte and Witte, 2017</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-41866f11-83f3-4b51-9685-c059a2dea254">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-41866f11-83f3-4b51-9685-c059a2dea254" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( y_i \)</span> are the values of variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{x} \)</span> and <span class="math notranslate nohighlight">\( \bar{y} \)</span> are the means of variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> across all data points.</p></li>
</ul>
<p>Covariance can also be calculated using matrix operations. If you have a dataset with multiple variables, you can calculate the covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6accf892-8c5b-4e55-aa2f-42baeacf3b14">
<span class="eqno">(11.6)<a class="headerlink" href="#equation-6accf892-8c5b-4e55-aa2f-42baeacf3b14" title="Permalink to this equation">#</a></span>\[\begin{equation} \Sigma = \frac{1}{n-1} \cdot (\mathbf{X} - \mathbf{\bar{x}})^\top \cdot (\mathbf{X} - \mathbf{\bar{x}}) \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \mathbf{X} \)</span> is an <span class="math notranslate nohighlight">\( n \times p \)</span> matrix representing the dataset, where <span class="math notranslate nohighlight">\( n \)</span> is the number of data points and <span class="math notranslate nohighlight">\( p \)</span> is the number of variables.</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{\bar{x}} \)</span> is a <span class="math notranslate nohighlight">\( 1 \times p \)</span> vector containing the means of each variable.</p></li>
</ul>
<p>The resulting covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span> is a <span class="math notranslate nohighlight">\( p \times p \)</span> symmetric matrix. The element <span class="math notranslate nohighlight">\( \Sigma_{ij} \)</span> represents the covariance between variables <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<p>In Python, you can calculate the covariance matrix using the <code class="docutils literal notranslate"><span class="pre">np.cov</span></code> function from the NumPy library. Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define a dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span> <span class="o">...</span><span class="p">])</span>

<span class="c1"># Calculate the covariance matrix</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p>The diagonal elements of the covariance matrix represent the variances of individual variables, while the off-diagonal elements represent the covariances between pairs of variables.</p>
</div>
<!-- Definitions of mean, standard deviation, and z-score, along with an explanation of how the z-score is used to standardize datasets:

1. **Mean ($\mu$):**
The mean, often referred to as the average, is the sum of all the values in a dataset divided by the total number of values.

Mathematically:
\begin{equation} \mu = \frac{1}{n} \sum_{i=1}^{n} x_i \end{equation},
where:
- $\mu$ represents the mean.
- $n$ is the number of data points in the dataset.
- $x_i$ are the individual data points.

2. **Standard Deviation ($\sigma$):**
The standard deviation measures the dispersion or spread of values in a dataset. It quantifies how much the individual data points deviate from the mean.

Mathematically:
\begin{equation} \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2} \end{equation},
where:
- $\sigma$ represents the standard deviation.
- $n$ is the number of data points in the dataset.
- $x_i$ are the individual data points.
- $\mu$ is the mean of the dataset.

3. **Z-Score ($z$):**
The z-score is a measure of how many standard deviations a particular data point is away from the mean. It's often used to standardize and compare values across different datasets, as it transforms the original data into a standardized distribution with a mean of 0 and a standard deviation of 1.

Mathematically:
\begin{equation} z = \frac{x - \mu}{\sigma} \end{equation}
where:
- $z$ is the z-score of a specific data point $x$.
- $x$ is the individual data point.
- $\mu$ is the mean of the dataset.
- $\sigma$ is the standard deviation of the dataset.

**Using Z-Scores for Standardization:**
To standardize a dataset using z-scores, you subtract the mean from each data point and then divide by the standard deviation. This process centers the data around the mean and scales it by the standard deviation. This is useful for comparing and analyzing data points in a consistent way, regardless of the original scale of the data. Standardized data has a mean of 0 and a standard deviation of 1, which facilitates comparisons between different datasets or data points. --><p><font color='Blue'><b>Example:</b></font> Let’s consider a small dataset with two variables (X1 and X2) and three observations:</p>
<p><strong>Step 1: Define the Dataset</strong>
The new original dataset is given by the matrix <span class="math notranslate nohighlight">\( A \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-729f6f55-649f-4676-bf5b-9e7ca724dbb8">
<span class="eqno">(11.7)<a class="headerlink" href="#equation-729f6f55-649f-4676-bf5b-9e7ca724dbb8" title="Permalink to this equation">#</a></span>\[\begin{equation}
A = \begin{bmatrix}
11 &amp; 22 \\
13 &amp; 24 \\
15 &amp; 26 \\
\end{bmatrix}
\end{equation}\]</div>
<p>This matrix represents a dataset with three data points, where each row corresponds to a data point and each column corresponds to a feature.</p>
<p><strong>Step 2: Calculate the Mean of Each Column</strong>
Calculate the mean of each feature (column) to obtain the column-wise means:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5f2db140-1dfb-4d94-80ae-b17dedf0977a">
<span class="eqno">(11.8)<a class="headerlink" href="#equation-5f2db140-1dfb-4d94-80ae-b17dedf0977a" title="Permalink to this equation">#</a></span>\[\begin{equation}
M = \begin{bmatrix}
\frac{11+13+15}{3} \\
\frac{22+24+26}{3} \\
\end{bmatrix}
= \begin{bmatrix}
13 \\
24 \\
\end{bmatrix}
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\( M \)</span> represents the mean vector, where each element <span class="math notranslate nohighlight">\( M_i \)</span> corresponds to the mean of feature <span class="math notranslate nohighlight">\( i \)</span>.</p>
<p><strong>Step 3: Center Columns by Subtracting Column Means</strong>
Center the data by subtracting the mean vector <span class="math notranslate nohighlight">\( M \)</span> from each data point in matrix <span class="math notranslate nohighlight">\( A \)</span>, resulting in the centered matrix <span class="math notranslate nohighlight">\( C \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dd7b499d-741b-43c0-ac44-28d98ddc61ae">
<span class="eqno">(11.9)<a class="headerlink" href="#equation-dd7b499d-741b-43c0-ac44-28d98ddc61ae" title="Permalink to this equation">#</a></span>\[\begin{equation}
C = A - M =
\begin{bmatrix}
11-13 &amp; 22-24 \\
13-13 &amp; 24-24 \\
15-13 &amp; 26-24 \\
\end{bmatrix}
= \begin{bmatrix}
-2 &amp; -2 \\
0 &amp; 0 \\
2 &amp; 2 \\
\end{bmatrix}
\end{equation}\]</div>
<p><strong>Step 4: Calculate Covariance Matrix of Centered Matrix</strong>
Calculate the covariance matrix <span class="math notranslate nohighlight">\( V \)</span> of the centered matrix <span class="math notranslate nohighlight">\( C \)</span> by multiplying the transpose of <span class="math notranslate nohighlight">\( C \)</span> by <span class="math notranslate nohighlight">\( C \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-26777d68-9fbf-4790-b953-2013bda86da6">
<span class="eqno">(11.10)<a class="headerlink" href="#equation-26777d68-9fbf-4790-b953-2013bda86da6" title="Permalink to this equation">#</a></span>\[\begin{equation}
V = C^T \cdot C =
\begin{bmatrix}
-2 &amp; 0 &amp; 2 \\
-2 &amp; 0 &amp; 2 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
-2 &amp; -2 \\
0 &amp; 0 \\
2 &amp; 2 \\
\end{bmatrix}
= \begin{bmatrix}
8 &amp; 8 \\
8 &amp; 8 \\
\end{bmatrix}
\end{equation}\]</div>
<p>The covariance matrix <span class="math notranslate nohighlight">\( V \)</span> captures the relationships between the features and their variations.</p>
<p><strong>Step 5: Eigendecomposition of Covariance Matrix</strong>
Perform the eigendecomposition of the covariance matrix <span class="math notranslate nohighlight">\( V \)</span> to find eigenvalues and eigenvectors. Eigenvalues (<span class="math notranslate nohighlight">\( values \)</span>) represent the importance of each eigenvector, and eigenvectors (<span class="math notranslate nohighlight">\( vectors \)</span>) define the directions of the new feature space:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8249a599-e636-4b86-bb86-4949ea3ac0d4">
<span class="eqno">(11.11)<a class="headerlink" href="#equation-8249a599-e636-4b86-bb86-4949ea3ac0d4" title="Permalink to this equation">#</a></span>\[\begin{equation} values = \begin{bmatrix}
16 \\
0 \\
\end{bmatrix} \end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-93a37fb3-5d8e-4a5e-ad97-7d24f84a00a8">
<span class="eqno">(11.12)<a class="headerlink" href="#equation-93a37fb3-5d8e-4a5e-ad97-7d24f84a00a8" title="Permalink to this equation">#</a></span>\[\begin{equation} vectors = \begin{bmatrix}
0.70710678 &amp; -0.70710678 \\
0.70710678 &amp; 0.70710678 \\
\end{bmatrix} \end{equation}\]</div>
<p><strong>Step 6: Project Data</strong>
Project the centered data matrix <span class="math notranslate nohighlight">\( C \)</span> onto the eigenvectors to obtain the transformed data <span class="math notranslate nohighlight">\( P \)</span>. The transformation is performed by calculating the dot product between the transposed eigenvectors and the transposed centered data matrix:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f35e8bf8-638c-4721-827e-afae19c500c7">
<span class="eqno">(11.13)<a class="headerlink" href="#equation-f35e8bf8-638c-4721-827e-afae19c500c7" title="Permalink to this equation">#</a></span>\[\begin{equation}
P = vectors^T \cdot C^T =
\begin{bmatrix}
0.70710678 &amp; 0.70710678 \\
-0.70710678 &amp; 0.70710678 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
-2 &amp; -2 &amp; 2 \\
-2 &amp; 0 &amp; 2 \\
\end{bmatrix}
= \begin{bmatrix}
-4 &amp; -2 &amp; 2 \\
0 &amp; 0 &amp; 0 \\
\end{bmatrix}
\end{equation}\]</div>
<p>The resulting matrix <span class="math notranslate nohighlight">\( P \)</span> represents the data projected onto the new feature space defined by the eigenvectors. This transformation effectively reduces the dimensionality while retaining the maximum variance in the data.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>The matrix <span class="math notranslate nohighlight">\( P \)</span> represents the transformed data obtained after applying Principal Component Analysis (PCA) to the original dataset. Each row of <span class="math notranslate nohighlight">\( P \)</span> corresponds to a data point in the original dataset, and each column corresponds to a principal component.</p>
<p>In the context of PCA, the principal components are orthogonal directions in the original feature space that capture the most significant variations in the data. The first principal component (column 1 of <span class="math notranslate nohighlight">\( P \)</span>) captures the maximum variance, the second principal component (column 2 of <span class="math notranslate nohighlight">\( P \)</span>) captures the second highest variance, and so on.</p>
<p>The values in the matrix <span class="math notranslate nohighlight">\( P \)</span> represent the projections of the original data points onto these principal components. These projections are in a new coordinate system defined by the principal components, where the axes are aligned with the directions of maximum variance in the data.</p>
<p>Interpreting the matrix <span class="math notranslate nohighlight">\( P \)</span>:</p>
<ul class="simple">
<li><p>Each row in <span class="math notranslate nohighlight">\( P \)</span> represents a transformed data point.</p></li>
<li><p>The values in each row represent how the original features have been projected onto the principal components.</p></li>
<li><p>The magnitude of each value indicates the contribution of the corresponding principal component to the transformation of that data point.</p></li>
</ul>
<p>For example, if the first column of <span class="math notranslate nohighlight">\( P \)</span> has relatively large values for a particular data point, it means that this data point has a significant projection along the direction of the first principal component, capturing a large amount of the overall variance in the data. Similarly, the second column of <span class="math notranslate nohighlight">\( P \)</span> captures the second principal component’s contribution to each data point’s transformation.</p>
<p>In summary, the matrix <span class="math notranslate nohighlight">\( P \)</span> provides a reduced-dimensional representation of the original data in terms of the principal components. It captures the essential information while discarding less significant variations in the data, which is the primary goal of PCA.</p>
</div>
<p><span style="color: Blue;"><strong>Example:</strong></span> We can employ the numpy library in Python to perform Principal Component Analysis (PCA), as illustrated in the preceding instance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="c1"># Define a matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">26</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">28</span><span class="p">]])</span>

<span class="c1"># Step 1: Print the original dataset</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Step 1: Original Dataset:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Step 2: Calculate the mean of each column</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 2: Mean of Each Column:</span><span class="se">\n</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 3: Center columns by subtracting column means</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">M</span>
<span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 3: Centered Columns:</span><span class="se">\n</span><span class="si">{</span><span class="n">C</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 4: Calculate covariance matrix of centered matrix</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 4: Covariance Matrix of Centered Matrix:</span><span class="se">\n</span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 5: Eigendecomposition of covariance matrix</span>
<span class="n">values</span><span class="p">,</span> <span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 5: Eigendecomposition of Covariance Matrix&quot;</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eigenvalues:</span><span class="se">\n</span><span class="si">{</span><span class="n">values</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eigenvectors:</span><span class="se">\n</span><span class="si">{</span><span class="n">vectors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 6: Project data</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 6: Projected Data:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Step 1: Original Dataset:</span>
array([[11, 24],
       [13, 26],
       [15, 28]])

<span class=" -Color -Color-Bold">Step 2: Mean of Each Column:</span>
<span class=" -Color -Color-Bold">[13. 26.]</span>

<span class=" -Color -Color-Bold">Step 3: Centered Columns:</span>
<span class=" -Color -Color-Bold">[[-2. -2.]</span>
<span class=" -Color -Color-Bold"> [ 0.  0.]</span>
<span class=" -Color -Color-Bold"> [ 2.  2.]]</span>

<span class=" -Color -Color-Bold">Step 4: Covariance Matrix of Centered Matrix:</span>
<span class=" -Color -Color-Bold">[[4. 4.]</span>
<span class=" -Color -Color-Bold"> [4. 4.]]</span>

<span class=" -Color -Color-Bold">Step 5: Eigendecomposition of Covariance Matrix</span>
<span class=" -Color -Color-Bold">Eigenvalues:</span>
<span class=" -Color -Color-Bold">[8. 0.]</span>
<span class=" -Color -Color-Bold">Eigenvectors:</span>
<span class=" -Color -Color-Bold">[[ 0.70710678 -0.70710678]</span>
<span class=" -Color -Color-Bold"> [ 0.70710678  0.70710678]]</span>

<span class=" -Color -Color-Bold">Step 6: Projected Data:</span>
array([[-2.82842712,  0.        ],
       [ 0.        ,  0.        ],
       [ 2.82842712,  0.        ]])
</pre></div>
</div>
</div>
</div>
<p><font color='Blue'><b>Example:</b></font> We can utilize the scikit-learn library in Python to conduct Principal Component Analysis (PCA), as shown in the previous example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Define a matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">26</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">28</span><span class="p">]])</span>

<span class="c1"># Step 1: Print the original dataset</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Step 1: Original Dataset:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Step 2: Create the PCA instance with desired number of components</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>

<span class="c1"># Step 3: Fit the PCA model on the data</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Step 3: Fit PCA Model on Data&quot;</span><span class="p">)</span>

<span class="c1"># Step 4: Access principal components (eigenvectors) and explained variance</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 4: Access Principal Components and Explained Variance&quot;</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Principal Components (Eigenvectors):&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Explained Variance for Each Principal Component:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>

<span class="c1"># Step 5: Eigendecomposition of Covariance Matrix</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 5: Eigendecomposition of Covariance Matrix&quot;</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Eigenvalues:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Eigenvectors:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>

<span class="c1"># Step 6: Transform data using PCA</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 6: Transform Data using PCA (Projection)&quot;</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Transformed Data (Projected onto Principal Components):&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Step 1: Original Dataset:</span>
array([[11, 24],
       [13, 26],
       [15, 28]])
<span class=" -Color -Color-Bold">Step 3: Fit PCA Model on Data</span>

<span class=" -Color -Color-Bold">Step 4: Access Principal Components and Explained Variance</span>
<span class=" -Color -Color-Bold">Principal Components (Eigenvectors):</span>
array([[ 0.70710678,  0.70710678],
       [-0.70710678,  0.70710678]])
<span class=" -Color -Color-Bold">Explained Variance for Each Principal Component:</span>
array([8., 0.])

<span class=" -Color -Color-Bold">Step 5: Eigendecomposition of Covariance Matrix</span>
<span class=" -Color -Color-Bold">Eigenvalues:</span>
array([8., 0.])
<span class=" -Color -Color-Bold">Eigenvectors:</span>
array([[ 0.70710678,  0.70710678],
       [-0.70710678,  0.70710678]])

<span class=" -Color -Color-Bold">Step 6: Transform Data using PCA (Projection)</span>
<span class=" -Color -Color-Bold">Transformed Data (Projected onto Principal Components):</span>
array([[-2.828427, -0.      ],
       [ 0.      ,  0.      ],
       [ 2.828427,  0.      ]])
</pre></div>
</div>
</div>
</div>
<p>Principal Component Analysis (PCA) stands as a swift and adaptable unsupervised technique for condensing the dimensionality of data. We caught a glimpse of its functionality in the earlier discussion on Scikit-Learn.</p>
<p><font color='Blue'><b>Example:</b></font> Let’s focus on a two-dimensional dataset. For the purpose of illustration, let’s examine the subsequent set of 400 data points <span id="id5">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Aqua&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;DodgerBlue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/14ab774eed2a40f0e97a82370abefbf554f960da50d03e3d44e42804890050f3.png" src="../_images/14ab774eed2a40f0e97a82370abefbf554f960da50d03e3d44e42804890050f3.png" />
</div>
</div>
<p>By visual inspection, it becomes evident that a nearly linear connection exists between the x and y variables in this dataset. However, the objective here differs slightly: instead of aiming to <em>predict</em> y values based on x values, the focus of unsupervised learning is to uncover the <em>relationship</em> between the x and y variables <span id="id6">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>.</p>
<p>Principal Component Analysis (PCA) serves as the tool to quantify this relationship. It achieves this by identifying the primary axes within the data and utilizing them to encapsulate the dataset’s characteristics  <span id="id7">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>.</p>
<p>To carry out PCA using Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">PCA</span></code> estimator, follow these steps:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">plt_setting</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">plot_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Function to plot an arrow on the given axis</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">start</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">end</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">end</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Perform PCA with 2 components</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a scatter plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;GreenYellow&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;Olive&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">)</span>

<span class="c1"># Visualize the principal components using ax.arrow()</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>  <span class="c1"># Scale vector by a factor of 3 times the square root of the variance</span>
    <span class="n">plot_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="o">**</span><span class="n">plt_setting</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/65f2cb359446851cd954124682379acadcfdb409ec564c020d07a08fd244d53a.png" src="../_images/65f2cb359446851cd954124682379acadcfdb409ec564c020d07a08fd244d53a.png" />
</div>
</div>
<p>These vectors embody the essence of the <em>principal axes</em> within the dataset. The length of each vector serves as a gauge for the axis’s significance in characterizing the data’s distribution—more precisely, it quantifies the variance of the data when projected onto that particular axis. The outcome of projecting each data point onto these principal axes gives birth to what we refer to as the “principal components” of the data <span id="id8">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>.</p>
<p>If we plot these principal components beside the original data, we see the plots shown here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Plot original data and principal components</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Aqua&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;DodgerBlue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">plot_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">start</span><span class="o">=</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="o">**</span><span class="n">plt_setting</span><span class="p">)</span>

<span class="c1"># Plot principal components</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;MistyRose&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;OrangeRed&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plot_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="o">**</span><span class="n">plt_setting</span><span class="p">)</span>
<span class="n">plot_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">start</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">plt_setting</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;component 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;component 2&#39;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;principal components&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2bac07dd7c70c492e54f24e21922a381a7f95df641a0a6d8427a76755905141a.png" src="../_images/2bac07dd7c70c492e54f24e21922a381a7f95df641a0a6d8427a76755905141a.png" />
</div>
</div>
<p>The process of transitioning from data axes to principal axes involves an affine transformation, which encompasses a combination of translation, rotation, and uniform scaling operations <span id="id9">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>.</p>
<p>Although this method for discovering principal components might initially appear as a mathematical concept, its implications extend far beyond theoretical curiosity. In fact, it plays a pivotal role in various applications within the domains of machine learning and data exploration <span id="id10">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>]</span>.</p>
</div>
<div class="section" id="pca-for-dimensionality-reduction">
<h2><span class="section-number">11.2.3. </span>PCA for Dimensionality Reduction<a class="headerlink" href="#pca-for-dimensionality-reduction" title="Permalink to this headline">#</a></h2>
<p>Utilizing PCA to reduce dimensionality entails nullifying one or more of the least significant principal components. This leads to a reduced-dimensional representation of the data while retaining the highest possible variance within the data.</p>
<p>Here’s an illustration of employing PCA for dimensionality reduction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize PCA with the desired number of components (in this case, 1)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit the PCA model to the data</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Transform the original data using the trained PCA model</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Print the shape of the original and transformed data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape:   &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformed shape:&quot;</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original shape:    (400, 2)
Transformed shape: (400, 1)
</pre></div>
</div>
</div>
</div>
<p>The data after transformation now exists in a singular dimension. To grasp the implications of this reduction in dimensionality, we can reverse the transformation of this condensed data and generate a visualization by comparing it with the initial dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Aqua&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;DodgerBlue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;OrangeRed&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;DarkRed&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4a099378f0dda95f82e4f3f599561b0db8d525593b99cd61d9db2c72f359fc2a.png" src="../_images/4a099378f0dda95f82e4f3f599561b0db8d525593b99cd61d9db2c72f359fc2a.png" />
</div>
</div>
<p>The blue dots represent the original dataset, while the red dots depict the projected version.
This clearly illustrates the essence of PCA-driven dimensionality reduction: it eliminates the data’s less significant principal axes, retaining solely the component(s) with the greatest variability.
The proportion of variance that gets removed (related to the extent of dot dispersion around the line shown in this figure) serves as an approximate gauge of the extent of “information” discarded in this dimensionality reduction process.</p>
<p>In several aspects, this downsized dataset adequately captures the most vital connections among the points: even with a 50% reduction in data dimensionality, the fundamental relationships between data points are largely conserved.</p>
<div class="section" id="utilizing-pca-for-visualization-handwritten-digits">
<h3><span class="section-number">11.2.3.1. </span>Utilizing PCA for Visualization: Handwritten Digits<a class="headerlink" href="#utilizing-pca-for-visualization-handwritten-digits" title="Permalink to this headline">#</a></h3>
<p>Although the true impact of dimensionality reduction might not be entirely clear when limited to only two dimensions, its importance becomes significantly more conspicuous when applied to data with a high number of dimensions.</p>
<p>Let’s examine the “load_digits” dataset provided by the sklearn library. Within this dataset, each data point represents an 8x8 image containing a numerical digit. This dataset is a prime candidate for showcasing the power of PCA-based visualization <span id="id11">[<a class="reference internal" href="../References.html#id2" title="J. VanderPlas. Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, 2023. ISBN 9781098121228. URL: https://books.google.ca/books?id=h_UtzwEACAAJ.">VanderPlas, 2023</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">ravel</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;A selection from the 64-dimensional digits dataset&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _digits_dataset:

Optical recognition of handwritten digits dataset
--------------------------------------------------

**Data Set Characteristics:**

    :Number of Instances: 1797
    :Number of Attributes: 64
    :Attribute Information: 8x8 image of integer pixels in the range 0..16.
    :Missing Attribute Values: None
    :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr)
    :Date: July; 1998

This is a copy of the test set of the UCI ML hand-written digits datasets
https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits

The data set contains images of hand-written digits: 10 classes where
each class refers to a digit.

Preprocessing programs made available by NIST were used to extract
normalized bitmaps of handwritten digits from a preprinted form. From a
total of 43 people, 30 contributed to the training set and different 13
to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of
4x4 and the number of on pixels are counted in each block. This generates
an input matrix of 8x8 where each element is an integer in the range
0..16. This reduces dimensionality and gives invariance to small
distortions.

For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.
T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
1994.

.. topic:: References

  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
    Graduate Studies in Science and Engineering, Bogazici University.
  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.
  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.
    Linear dimensionalityreduction using relevance weighted LDA. School of
    Electrical and Electronic Engineering Nanyang Technological University.
    2005.
  - Claudio Gentile. A New Approximate Maximal Margin Classification
    Algorithm. NIPS. 2000.
</pre></div>
</div>
<img alt="../_images/2c337e6a1b340f88722f126e9ce462006eb8862f6f0ea39241c9c816db18c274.png" src="../_images/2c337e6a1b340f88722f126e9ce462006eb8862f6f0ea39241c9c816db18c274.png" />
</div>
</div>
<p>To develop a better understanding of the interconnections among these points, we can employ PCA to transform them into a more manageable number of dimensions, such as two:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Load the digits dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Apply PCA to project data to 2 dimensions</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># project from 64 to 2 dimensions</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Print shapes before and after PCA</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original data shape:&quot;</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Projected data shape:&quot;</span><span class="p">,</span> <span class="n">projected</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Get a list of 10 distinct colors from a Seaborn colormap</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Spectral&quot;</span><span class="p">,</span> <span class="n">n_colors</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">))</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                          <span class="n">hue</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span>
                          <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Set labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Component 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Component 2&#39;</span><span class="p">,</span>
       <span class="n">title</span><span class="o">=</span><span class="s1">&#39;PCA Projection of Handwritten Digits&#39;</span><span class="p">)</span>

<span class="c1"># Display the plot with tight layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original data shape: (1797, 64)
Projected data shape: (1797, 2)
</pre></div>
</div>
<img alt="../_images/d55e90b327875842f37d14fc7c21a61ae97560fcad4fdf3d015eaf41f2e68388.png" src="../_images/d55e90b327875842f37d14fc7c21a61ae97560fcad4fdf3d015eaf41f2e68388.png" />
</div>
</div>
<p>Consider the significance of these components: the complete dataset forms a cloud of points in a 64-dimensional space, while these specific points represent the projection of each data point along the directions characterized by the highest variance. Fundamentally, we’ve identified the optimal scaling and rotation within the 64-dimensional domain, enabling us to visualize the arrangement of the digits within two dimensions. Importantly, this process is unsupervised—meaning, it’s accomplished without using any label information {cite:p}`vanderplas2023python, sklearnUserGuide.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_11"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C11S1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11.1. </span>Introduction to Dimensionality Reduction</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C11S3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11.3. </span>t-Distributed Stochastic Neighbor Embedding (t-SNE)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">11.2.1. Mathematical Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-essence-of-principal-components-analysis-pca">11.2.2. The Essence of Principal Components Analysis (PCA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-dimensionality-reduction">11.2.3. PCA for Dimensionality Reduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilizing-pca-for-visualization-handwritten-digits">11.2.3.1. Utilizing PCA for Visualization: Handwritten Digits</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>