

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>11.4. Linear and Quadratic Discriminant Analyses &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_11/ENGG_680_C11S4';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.5. Recursive Feature Elimination (RFE)" href="ENGG_680_C11S5.html" />
    <link rel="prev" title="11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)" href="ENGG_680_C11S3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">13.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear and Quadratic Discriminant Analyses</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">11.4.1. Linear Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-lda-with-scikit-learn-sklearn">11.4.1.1. Using LDA with scikit-learn (sklearn)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-discriminant-analysis">11.4.2. Quadratic Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-key-steps-of-quadratic-discriminant-analysis-qda">11.4.2.1. The Key Steps of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-quadratic-discriminant-analysis-qda">11.4.2.2. Advantages of Quadratic Discriminant Analysis (QDA)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-quadratic-discriminant-analysis-qda">11.4.3. Limitations of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-quadratic-discriminant-analysis-qda">11.4.4. Applications of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-and-similarities-between-linear-discriminant-analysis-lda-and-quadratic-discriminant-analysis-qda">11.4.5. Differences and Similarities between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-qda-with-scikit-learn-sklearn">11.4.5.1. Using QDA with scikit-learn (sklearn)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-and-quadratic-discriminant-analyses">
<h1><span class="section-number">11.4. </span>Linear and Quadratic Discriminant Analyses<a class="headerlink" href="#linear-and-quadratic-discriminant-analyses" title="Permalink to this heading">#</a></h1>
<section id="linear-discriminant-analysis">
<h2><span class="section-number">11.4.1. </span>Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Permalink to this heading">#</a></h2>
<p>Linear Discriminant Analysis (LDA) is a dimensionality reduction technique that is particularly useful for classification problems. Unlike some other dimensionality reduction techniques that focus solely on preserving variance or pairwise distances, LDA aims to find a lower-dimensional space that maximizes class separability <span id="id1">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id119" title="Jia Li. Linear discriminant analysis. http://www.stat.ucla.edu/ ywu/research/documents/BOOKS/LinearDiscriminantAnalysis.pdf, 2023. [Online; accessed 01-August-2023].">Li, 2023</a>]</span>. LDA explicitly considers class labels, making it suitable for classification tasks <span id="id2">[<a class="reference internal" href="../References.html#id219" title="H. Zhao, Z. Lai, H. Leung, and X. Zhang. Feature Learning and Understanding: Algorithms and Applications. Information Fusion and Data Science. Springer International Publishing, 2020. ISBN 9783030407940. URL: https://books.google.ca/books?id=eBLbDwAAQBAJ.">Zhao <em>et al.</em>, 2020</a>]</span>. It also helps to mitigate the curse of dimensionality by reducing the dimensionality while preserving class information <span id="id3">[<a class="reference internal" href="../References.html#id218" title="D. Zhang, F. Song, Y. Xu, and Z. Liang. Advanced Pattern Recognition Technologies with Applications to Biometrics. Advances in Information and Communication Technology Education. Medical Information Science Reference, 2009. ISBN 9781605662015. URL: https://books.google.ca/books?id=InQK2wXcpF0C.">Zhang <em>et al.</em>, 2009</a>]</span>. LDA can be applied to various domains, such as face recognition, bioinformatics <span id="id4">[<a class="reference internal" href="../References.html#id189" title="Alok Sharma and Kuldip K Paliwal. Linear discriminant analysis for the small sample size problem: an overview. International Journal of Machine Learning and Cybernetics, 6:443–454, 2015.">Sharma and Paliwal, 2015</a>]</span>, and chemical shift analysis <span id="id5">[<a class="reference internal" href="../References.html#id174" title="Javier A Romero, Paulina Putko, Mateusz Urbańczyk, Krzysztof Kazimierczuk, and Anna Zawadzka-Kazimierczuk. Linear discriminant analysis reveals hidden patterns in nmr chemical shifts of intrinsically disordered proteins. PLoS computational biology, 18(10):e1010258, 2022.">Romero <em>et al.</em>, 2022</a>]</span>.</p>
<p><strong>Objective:</strong></p>
<p>The main goal of LDA is to find a linear transformation of the original feature space that maximizes the separation between different classes while minimizing the variance within each class. This objective is also known as Fisher’s criterion or the Rayleigh quotient. It can be mathematically expressed as <span id="id6">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c4d27d1c-c7de-4a35-b2fe-f93601fa1c6a">
<span class="eqno">(11.21)<a class="headerlink" href="#equation-c4d27d1c-c7de-4a35-b2fe-f93601fa1c6a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\max_W \frac{W^T S_B W}{W^T S_W W},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the transformation matrix, <span class="math notranslate nohighlight">\(S_B\)</span> is the between-class scatter matrix, and <span class="math notranslate nohighlight">\(S_W\)</span> is the within-class scatter matrix. The optimal <span class="math notranslate nohighlight">\(W\)</span> can be obtained by solving the generalized eigenvalue problem:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a2ee23f1-e4ed-430f-af26-2ad5992e0393">
<span class="eqno">(11.22)<a class="headerlink" href="#equation-a2ee23f1-e4ed-430f-af26-2ad5992e0393" title="Permalink to this equation">#</a></span>\[\begin{equation}
(S_W^{-1} S_B) W = \lambda W,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> are the eigenvalues and <span class="math notranslate nohighlight">\(W\)</span> are the eigenvectors <span id="id7">[<a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>]</span>. The eigenvectors corresponding to the largest eigenvalues are selected as the new axes of the transformed feature space.</p>
<p><strong>Key Steps:</strong></p>
<ol class="arabic simple">
<li><p><strong>Compute Class Means:</strong> Calculate the mean vector for each class in the original feature space.</p></li>
<li><p><strong>Compute Scatter Matrices:</strong></p>
<ul class="simple">
<li><p>Within-Class Scatter Matrix (SW): This matrix measures the variance within each class. It’s computed by summing up the individual scatter matrices for each class.</p></li>
<li><p>Between-Class Scatter Matrix (SB): This matrix measures the separation between classes. It’s computed by calculating the scatter between the class means and the overall mean.</p></li>
</ul>
</li>
<li><p><strong>Calculate Eigenvalues and Eigenvectors:</strong></p>
<ul class="simple">
<li><p>Compute the eigenvalues and eigenvectors of the matrix <code class="docutils literal notranslate"><span class="pre">(SW^-1)</span> <span class="pre">*</span> <span class="pre">SB</span></code>. These eigenvalues represent the discriminative power of the corresponding eigenvectors.</p></li>
</ul>
</li>
<li><p><strong>Select Top Eigenvectors:</strong></p>
<ul class="simple">
<li><p>Sort the eigenvalues in descending order and select the top <code class="docutils literal notranslate"><span class="pre">k</span></code> eigenvectors corresponding to the largest eigenvalues. These eigenvectors become the new axes of the transformed feature space.</p></li>
</ul>
</li>
<li><p><strong>Project Data:</strong></p>
<ul class="simple">
<li><p>Project the original data points onto the selected eigenvectors to obtain the reduced-dimensional representation.</p></li>
</ul>
</li>
</ol>
<p>The key steps of LDA are as follows <span id="id8">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id119" title="Jia Li. Linear discriminant analysis. http://www.stat.ucla.edu/ ywu/research/documents/BOOKS/LinearDiscriminantAnalysis.pdf, 2023. [Online; accessed 01-August-2023].">Li, 2023</a>, <a class="reference internal" href="../References.html#id174" title="Javier A Romero, Paulina Putko, Mateusz Urbańczyk, Krzysztof Kazimierczuk, and Anna Zawadzka-Kazimierczuk. Linear discriminant analysis reveals hidden patterns in nmr chemical shifts of intrinsically disordered proteins. PLoS computational biology, 18(10):e1010258, 2022.">Romero <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id219" title="H. Zhao, Z. Lai, H. Leung, and X. Zhang. Feature Learning and Understanding: Algorithms and Applications. Information Fusion and Data Science. Springer International Publishing, 2020. ISBN 9783030407940. URL: https://books.google.ca/books?id=eBLbDwAAQBAJ.">Zhao <em>et al.</em>, 2020</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Compute Class Means:</strong> Calculate the mean vector for each class in the original feature space. This can be done by averaging the feature values of all the samples belonging to each class.</p></li>
<li><p><strong>Compute Scatter Matrices:</strong></p>
<ul class="simple">
<li><p>Within-Class Scatter Matrix (SW): This matrix measures the variance within each class. It’s computed by summing up the individual scatter matrices for each class. The scatter matrix for a class is obtained by multiplying the difference between each sample and the class mean by its transpose.</p></li>
<li><p>Between-Class Scatter Matrix (SB): This matrix measures the separation between classes. It’s computed by calculating the scatter between the class means and the overall mean. The overall mean is obtained by averaging the feature values of all the samples in the dataset.</p></li>
</ul>
</li>
<li><p><strong>Calculate Eigenvalues and Eigenvectors:</strong></p>
<ul class="simple">
<li><p>Compute the eigenvalues and eigenvectors of the matrix <code class="docutils literal notranslate"><span class="pre">(SW^-1)</span> <span class="pre">*</span> <span class="pre">SB</span></code>. These eigenvalues represent the discriminative power of the corresponding eigenvectors. The eigenvectors are the directions that maximize the ratio of between-class variance to within-class variance.</p></li>
</ul>
</li>
<li><p><strong>Select Top Eigenvectors:</strong></p>
<ul class="simple">
<li><p>Sort the eigenvalues in descending order and select the top <code class="docutils literal notranslate"><span class="pre">k</span></code> eigenvectors corresponding to the largest eigenvalues. These eigenvectors become the new axes of the transformed feature space. The value of <code class="docutils literal notranslate"><span class="pre">k</span></code> depends on how many dimensions you want to reduce your data to.</p></li>
</ul>
</li>
<li><p><strong>Project Data:</strong></p>
<ul class="simple">
<li><p>Project the original data points onto the selected eigenvectors to obtain the reduced-dimensional representation. This can be done by multiplying the original feature matrix by the matrix of eigenvectors.</p></li>
</ul>
</li>
</ol>
<p>The advantages of LDA are as follows <span id="id9">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id119" title="Jia Li. Linear discriminant analysis. http://www.stat.ucla.edu/ ywu/research/documents/BOOKS/LinearDiscriminantAnalysis.pdf, 2023. [Online; accessed 01-August-2023].">Li, 2023</a>, <a class="reference internal" href="../References.html#id174" title="Javier A Romero, Paulina Putko, Mateusz Urbańczyk, Krzysztof Kazimierczuk, and Anna Zawadzka-Kazimierczuk. Linear discriminant analysis reveals hidden patterns in nmr chemical shifts of intrinsically disordered proteins. PLoS computational biology, 18(10):e1010258, 2022.">Romero <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id219" title="H. Zhao, Z. Lai, H. Leung, and X. Zhang. Feature Learning and Understanding: Algorithms and Applications. Information Fusion and Data Science. Springer International Publishing, 2020. ISBN 9783030407940. URL: https://books.google.ca/books?id=eBLbDwAAQBAJ.">Zhao <em>et al.</em>, 2020</a>]</span>:</p>
<ul class="simple">
<li><p>LDA explicitly considers class labels, making it suitable for classification tasks. Unlike some other dimensionality reduction techniques that focus solely on preserving variance or pairwise distances, LDA aims to find a lower-dimensional space that maximizes class separability <span id="id10">[<a class="reference internal" href="../References.html#id99" title="Hamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao Li, and Liang Zhao. Latent dirichlet allocation (lda) and topic modeling: models, applications, a survey. Multimedia Tools and Applications, 78:15169–15211, 2019.">Jelodar <em>et al.</em>, 2019</a>]</span>. This means that LDA can capture the differences between classes and make them more distinguishable in the transformed feature space.</p></li>
<li><p>It aims to maximize class separability, which can improve classification performance. By reducing the within-class variance and increasing the between-class variance, LDA can enhance the discriminative power of the features and make the decision boundaries more clear and accurate. This can lead to higher accuracy and lower error rates for classification models.</p></li>
<li><p>LDA can help mitigate the curse of dimensionality by reducing the dimensionality while preserving class information. The curse of dimensionality refers to the problem of high-dimensional data, which can cause computational difficulties, overfitting, and poor generalization. By projecting the data onto a lower-dimensional space, LDA can reduce the complexity and redundancy of the data, while retaining the essential information for classification. This can improve the efficiency and robustness of the models.</p></li>
</ul>
<p>Limitations of LDA <span id="id11">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id119" title="Jia Li. Linear discriminant analysis. http://www.stat.ucla.edu/ ywu/research/documents/BOOKS/LinearDiscriminantAnalysis.pdf, 2023. [Online; accessed 01-August-2023].">Li, 2023</a>, <a class="reference internal" href="../References.html#id174" title="Javier A Romero, Paulina Putko, Mateusz Urbańczyk, Krzysztof Kazimierczuk, and Anna Zawadzka-Kazimierczuk. Linear discriminant analysis reveals hidden patterns in nmr chemical shifts of intrinsically disordered proteins. PLoS computational biology, 18(10):e1010258, 2022.">Romero <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id219" title="H. Zhao, Z. Lai, H. Leung, and X. Zhang. Feature Learning and Understanding: Algorithms and Applications. Information Fusion and Data Science. Springer International Publishing, 2020. ISBN 9783030407940. URL: https://books.google.ca/books?id=eBLbDwAAQBAJ.">Zhao <em>et al.</em>, 2020</a>]</span>:</p>
<ul class="simple">
<li><p>LDA assumes that the classes are linearly separable and that the data is normally distributed within each class.</p></li>
<li><p>It may not work well when the classes are overlapping or when the assumptions of normality are not met.</p></li>
<li><p>LDA is supervised and requires labeled data for training.</p></li>
</ul>
<p>Use Cases <span id="id12">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id119" title="Jia Li. Linear discriminant analysis. http://www.stat.ucla.edu/ ywu/research/documents/BOOKS/LinearDiscriminantAnalysis.pdf, 2023. [Online; accessed 01-August-2023].">Li, 2023</a>, <a class="reference internal" href="../References.html#id174" title="Javier A Romero, Paulina Putko, Mateusz Urbańczyk, Krzysztof Kazimierczuk, and Anna Zawadzka-Kazimierczuk. Linear discriminant analysis reveals hidden patterns in nmr chemical shifts of intrinsically disordered proteins. PLoS computational biology, 18(10):e1010258, 2022.">Romero <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id219" title="H. Zhao, Z. Lai, H. Leung, and X. Zhang. Feature Learning and Understanding: Algorithms and Applications. Information Fusion and Data Science. Springer International Publishing, 2020. ISBN 9783030407940. URL: https://books.google.ca/books?id=eBLbDwAAQBAJ.">Zhao <em>et al.</em>, 2020</a>]</span>:</p>
<ul class="simple">
<li><p>Classification problems where dimensionality reduction is desired.</p></li>
<li><p>Face recognition, where LDA can be used to extract features that enhance differences between individuals.</p></li>
</ul>
<section id="using-lda-with-scikit-learn-sklearn">
<h3><span class="section-number">11.4.1.1. </span>Using LDA with scikit-learn (sklearn)<a class="headerlink" href="#using-lda-with-scikit-learn-sklearn" title="Permalink to this heading">#</a></h3>
<p>LDA can be implemented using libraries like scikit-learn in Python. The <code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code> class in scikit-learn provides an easy way to perform LDA for dimensionality reduction and classification <span id="id13">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Load the Iris dataset as an example</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span>

<span class="c1"># Create an instance of LinearDiscriminantAnalysis</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit the LDA model to the data and transform it</span>
<span class="n">X_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">))</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_lda</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X_lda</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                          <span class="n">hue</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                          <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Red&#39;</span><span class="p">,</span> <span class="s1">&#39;Blue&#39;</span><span class="p">,</span> <span class="s1">&#39;Green&#39;</span><span class="p">],</span>
                          <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Set labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;LDA Component 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;LDA Component 2&#39;</span><span class="p">,</span> 
       <span class="n">title</span><span class="o">=</span><span class="s1">&#39;LDA Projection of Iris Dataset&#39;</span><span class="p">)</span>

<span class="c1"># Set legend</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">scatter</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">target_names</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>

<span class="c1"># Display the plot with tight layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f0405311cdae2555ce23e7d96a1714b7442f46baa655e5e8b4ac050943b9b2d1.png" src="../_images/f0405311cdae2555ce23e7d96a1714b7442f46baa655e5e8b4ac050943b9b2d1.png" />
</div>
</div>
<p>In this example, we loaded the Iris dataset, which is a common dataset for classification tasks. We then created an instance of <code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code> with <code class="docutils literal notranslate"><span class="pre">n_components</span></code> set to 2, indicating that we want to reduce the dimensionality to 2 dimensions. We fit the LDA model to the data using the <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> method, which both fits the model and transforms the data to the reduced-dimensional space.</p>
<p>Keep in mind that LDA is a supervised technique, so you need to provide the class labels (<code class="docutils literal notranslate"><span class="pre">y</span></code>) when fitting the model. The resulting <code class="docutils literal notranslate"><span class="pre">X_lda</span></code> will contain the transformed data in the lower-dimensional space.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code> class in scikit-learn also provides other useful attributes and methods, such as <code class="docutils literal notranslate"><span class="pre">explained_variance_ratio_</span></code> to see the explained variance of each component, and <code class="docutils literal notranslate"><span class="pre">coef_</span></code> to access the linear coefficients of the original features that contribute to each component.</p>
<p>If you want to use LDA for classification, you can use the transformed data <code class="docutils literal notranslate"><span class="pre">X_lda</span></code> as input to a classifier, like logistic regression or a support vector machine, to perform classification tasks in the reduced-dimensional space.</p>
<p>Remember that LDA makes certain assumptions about the data distribution and class separability. It’s important to evaluate the performance of the reduced-dimensional representation on your specific task and dataset.</p>
</section>
</section>
<section id="quadratic-discriminant-analysis">
<h2><span class="section-number">11.4.2. </span>Quadratic Discriminant Analysis<a class="headerlink" href="#quadratic-discriminant-analysis" title="Permalink to this heading">#</a></h2>
<p>Quadratic Discriminant Analysis (QDA) represents a flexible advancement of Linear Discriminant Analysis (LDA), providing enhanced modeling capabilities for class distributions. While LDA assumes consistent covariance matrices across classes, QDA allows for distinct covariance matrices for individual classes. This unique characteristic empowers QDA to effectively capture intricate nonlinear associations between features and classes, thereby accommodating the modeling of complex data structures <span id="id14">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>]</span>.</p>
<p><strong>Primary Goal:</strong>
The core objective of Quadratic Discriminant Analysis is to identify a quadratic transformation within the original feature space that maximizes the posterior probability for each class, based on the available features. This objective is firmly rooted in Bayes’ theorem, which establishes a connection between the posterior class probability and the product of the class’s prior probability and the likelihood of observing the given features given that class. QDA operates under the assumption that the likelihood of features, given a specific class, adheres to a multivariate normal distribution. This distribution encompasses class-specific parameters such as mean and covariance matrix.</p>
<p>In practice, QDA derives distinct quadratic discriminant functions for each class. These functions encapsulate the logarithm of the posterior probability for the respective class, taking into account the provided features. The outcome of this process is the creation of decision boundaries expressed as quadratic curves, effectively shaping QDA as a quadratic classifier <span id="id15">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>]</span>.</p>
<section id="the-key-steps-of-quadratic-discriminant-analysis-qda">
<h3><span class="section-number">11.4.2.1. </span>The Key Steps of Quadratic Discriminant Analysis (QDA)<a class="headerlink" href="#the-key-steps-of-quadratic-discriminant-analysis-qda" title="Permalink to this heading">#</a></h3>
<p>Quadratic Discriminant Analysis (QDA) entails a series of pivotal steps for effective classification, as outlined below:</p>
<ol class="arabic simple">
<li><p><strong>Estimate Class-Specific Parameters:</strong> The initial step involves estimating essential parameters for each class within the original feature space. These parameters comprise the mean vector, covariance matrix, and prior probability for every class. To accomplish this, sample statistics derived from the training data are employed. These include the sample mean, sample covariance, and the proportion of samples belonging to each class <span id="id16">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Compute Quadratic Discriminant Functions:</strong> Quadratic discriminant functions are then computed for each class using the estimated parameters. These functions encapsulate the logarithm of the posterior probability of a given class, given the observed features. Mathematically, the discriminant function for class <span class="math notranslate nohighlight">\(i\)</span> can be expressed as <span id="id17">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-16dd6ded-d91b-46ab-b718-c24dbdb9d23d">
<span class="eqno">(11.23)<a class="headerlink" href="#equation-16dd6ded-d91b-46ab-b718-c24dbdb9d23d" title="Permalink to this equation">#</a></span>\[\begin{equation}
g_i(x) = -\frac{1}{2} \log |\Sigma_i| - \frac{1}{2} (x - \mu_i)^T \Sigma_i^{-1} (x - \mu_i) + \log \pi_i,
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> represents the feature vector of the data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_i\)</span> corresponds to the mean vector of class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma_i\)</span> denotes the covariance matrix of class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi_i\)</span> signifies the prior probability of class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Data Classification:</strong> To classify new data points, their feature values are inputted into the computed discriminant functions. The class associated with the highest resulting discriminant function value is selected as the classification choice. This selection is accomplished by comparing the values of <span class="math notranslate nohighlight">\(g_i(x)\)</span> for all classes and selecting the one with the maximum value <span id="id18">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>. This step effectively assigns the new data point to a specific class.</p></li>
</ol>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Unlike Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) does not serve as a dimensionality reduction technique. This aspect is mentioned here to provide a comprehensive understanding of LDA’s capabilities in contrast to QDA. While LDA can indeed be used to reduce dimensionality by projecting data onto a lower-dimensional subspace while preserving class separability, QDA’s primary focus lies in classification and modeling intricate relationships between features and classes.</p>
</div>
</section>
<section id="advantages-of-quadratic-discriminant-analysis-qda">
<h3><span class="section-number">11.4.2.2. </span>Advantages of Quadratic Discriminant Analysis (QDA)<a class="headerlink" href="#advantages-of-quadratic-discriminant-analysis-qda" title="Permalink to this heading">#</a></h3>
<p>The advantages of QDA are as follows:</p>
<ul class="simple">
<li><p><strong>Enhanced Boundary Modeling:</strong> QDA is capable of capturing intricate and nonlinear class boundaries that exceed the capabilities of LDA. QDA assumes that each class has its own covariance matrix, which allows it to model the curvature and orientation of the class distributions. This can result in more accurate and realistic classification models that can handle nonlinear and non-Gaussian data <span id="id19">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Diverse Distribution Handling:</strong> QDA adapts to varying class shapes and orientations, accommodating elliptical and skewed distributions. Unlike LDA, which assumes that the classes have the same covariance matrix, QDA allows each class to have different shapes and orientations. This can help to capture the diversity and heterogeneity of the data, especially when the classes are not well separated or have different sizes <span id="id20">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Improved Accuracy:</strong> When the assumptions of LDA are unmet, QDA may yield superior accuracy and lower error rates. LDA assumes that the classes are linearly separable and that the data is normally distributed within each class. However, these assumptions may not hold for some real-world data, such as hyperspectral or functional data. In such cases, QDA can outperform LDA by modeling the nonlinearities and heteroscedasticity of the data <span id="id21">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
</ul>
</section>
</section>
<section id="limitations-of-quadratic-discriminant-analysis-qda">
<h2><span class="section-number">11.4.3. </span>Limitations of Quadratic Discriminant Analysis (QDA)<a class="headerlink" href="#limitations-of-quadratic-discriminant-analysis-qda" title="Permalink to this heading">#</a></h2>
<p>However, it is important to be aware of the limitations associated with Quadratic Discriminant Analysis (QDA). These drawbacks include:</p>
<ol class="arabic simple">
<li><p><strong>Risk of Overfitting:</strong> QDA’s more intricate parameter estimation process can expose it to overfitting, rendering it less robust in the presence of outliers. Due to the increased number of parameters that need estimation compared to Linear Discriminant Analysis (LDA), QDA has a greater potential to capture noise and anomalies within the data instead of genuine patterns. This susceptibility to overfitting can lead to poor generalization capabilities, causing high variance in predictions for new data points. Additionally, QDA’s sensitivity to outliers can distort covariance matrices and impact the accuracy of discriminant functions <span id="id22">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Challenges with Small Sample Sizes:</strong> QDA’s performance may deteriorate when confronted with scenarios characterized by limited samples or a high feature-to-sample ratio. QDA relies on the sample statistics to estimate mean vectors and covariance matrices for each class. However, in cases where the sample size is inadequate or the number of features outweighs the number of samples, the resulting sample statistics might not accurately represent the underlying population parameters. This can lead to inaccurate parameter estimates, affecting both discriminant functions and classification accuracy <span id="id23">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Computational Complexity:</strong> QDA’s incorporation of quadratic terms in its discriminant functions increases its computational demands compared to LDA. The computations involved in QDA, such as matrix inversions, determinant calculations, and quadratic multiplications, are more intricate than those in LDA. Consequently, QDA’s computational requirements can be substantial and time-consuming, especially when dealing with high-dimensional data. Furthermore, QDA necessitates more storage space as it mandates the storage of a covariance matrix for each class, in contrast to LDA’s use of a single shared covariance matrix <span id="id24">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
</ol>
</section>
<section id="applications-of-quadratic-discriminant-analysis-qda">
<h2><span class="section-number">11.4.4. </span>Applications of Quadratic Discriminant Analysis (QDA)<a class="headerlink" href="#applications-of-quadratic-discriminant-analysis-qda" title="Permalink to this heading">#</a></h2>
<p>The versatility of Quadratic Discriminant Analysis (QDA) lends itself to various practical use cases, including:</p>
<ol class="arabic simple">
<li><p><strong>Complex Boundary Classification:</strong> QDA is particularly effective when dealing with classification tasks involving intricate, nonlinear class boundaries and diverse covariance matrices. By accommodating the curvature and orientation of class distributions, QDA can enhance the accuracy and realism of classification models. Additionally, QDA’s flexibility allows it to excel in scenarios where class separation is not clear-cut or where class sizes differ significantly <span id="id25">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Image Recognition:</strong> In the realm of image processing, QDA finds application in the categorization of pixels based on color and texture features. By modeling the multivariate normal distribution of pixel values for each class, QDA can effectively compute the posterior probabilities of class assignments given pixel features. QDA’s adaptability enables it to handle challenges like varying illumination and noise conditions in images, making it suitable for image recognition tasks <span id="id26">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Speech Recognition:</strong> QDA’s utility extends to phonetic class modeling in speech recognition endeavors, particularly in relation to acoustic features. By estimating mean and covariance functions for each phonetic class and computing discriminant functions through methods such as functional principal component analysis, QDA aids in phonetic classification. Its ability to account for non-stationary and noisy speech signals further enhances its suitability for speech recognition applications <span id="id27">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>.</p></li>
</ol>
</section>
<section id="differences-and-similarities-between-linear-discriminant-analysis-lda-and-quadratic-discriminant-analysis-qda">
<h2><span class="section-number">11.4.5. </span>Differences and Similarities between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)<a class="headerlink" href="#differences-and-similarities-between-linear-discriminant-analysis-lda-and-quadratic-discriminant-analysis-qda" title="Permalink to this heading">#</a></h2>
<p>Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are both techniques used for classification and dimensionality reduction. They share some common aspects while also exhibiting distinct characteristics <span id="id28">[<a class="reference internal" href="../References.html#id96" title="A.J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts in Statistics. Springer New York, 2009. ISBN 9780387781891. URL: https://books.google.ca/books?id=1CuznRORa3EC.">Izenman, 2009</a>, <a class="reference internal" href="../References.html#id121" title="Xiangyu Li and Hua Wang. On mean-optimal robust linear discriminant analysis. In 2022 IEEE International Conference on Data Mining (ICDM), 1047–1052. IEEE, 2022.">Li and Wang, 2022</a>, <a class="reference internal" href="../References.html#id201" title="R.A. Vannatta, K.N. LaVenia, P. Atkinson, S. Delamont, A. Cernat, J.W. Sakshaug, and R.A. Williams. Linear Discriminant Analysis. SAGE Publications Limited, 2020. ISBN 9781529749090. URL: https://books.google.ca/books?id=gAb7zQEACAAJ.">Vannatta <em>et al.</em>, 2020</a>]</span>:</p>
<p><strong>Similarities:</strong></p>
<ol class="arabic simple">
<li><p><strong>Bayesian Framework:</strong> Both LDA and QDA are grounded in a Bayesian framework, aiming to classify data points by maximizing the posterior probabilities of class membership given the observed features.</p></li>
<li><p><strong>Assumption of Normality:</strong> Both methods assume that the features within each class follow a multivariate normal distribution.</p></li>
<li><p><strong>Supervised Learning:</strong> Both LDA and QDA are supervised learning techniques that require labeled training data to build their classification models.</p></li>
</ol>
<p><strong>Differences:</strong></p>
<ol class="arabic simple">
<li><p><strong>Covariance Matrices:</strong></p>
<ul class="simple">
<li><p><strong>LDA:</strong> Assumes that all classes share a common covariance matrix. This assumption simplifies the model but may not be suitable when class distributions have different shapes and orientations.</p></li>
<li><p><strong>QDA:</strong> Allows each class to have its own distinct covariance matrix. This flexibility allows QDA to capture varying class distribution shapes and is well-suited for cases where classes have different covariance structures.</p></li>
</ul>
</li>
<li><p><strong>Boundary Modeling:</strong></p>
<ul class="simple">
<li><p><strong>LDA:</strong> Constructs linear decision boundaries to separate classes. It is most effective when class distributions are well-separated and linearly separable.</p></li>
<li><p><strong>QDA:</strong> Constructs quadratic decision boundaries, which can capture more complex and nonlinear relationships between features and classes. QDA is better equipped to handle intricate class boundaries.</p></li>
</ul>
</li>
<li><p><strong>Parameter Estimation:</strong></p>
<ul class="simple">
<li><p><strong>LDA:</strong> Involves estimating fewer parameters since it assumes a common covariance matrix. This can be advantageous when the dataset is small or when overfitting is a concern.</p></li>
<li><p><strong>QDA:</strong> Requires estimation of more parameters due to individual covariance matrices for each class. While this allows for better adaptation to diverse data patterns, it increases the risk of overfitting, especially when the sample size is limited.</p></li>
</ul>
</li>
<li><p><strong>Computational Complexity:</strong></p>
<ul class="simple">
<li><p><strong>LDA:</strong> Generally computationally simpler since it involves estimating and working with a single covariance matrix for all classes.</p></li>
<li><p><strong>QDA:</strong> Involves more complex computations due to separate covariance matrices for each class. The quadratic terms in the discriminant functions can lead to higher computational demands.</p></li>
</ul>
</li>
<li><p><strong>Use Cases:</strong></p>
<ul class="simple">
<li><p><strong>LDA:</strong> Suitable when classes have similar covariance structures and linear boundaries work well. Commonly used when the dataset is large and there is a desire to reduce dimensionality.</p></li>
<li><p><strong>QDA:</strong> Appropriate for cases with varying covariance structures and nonlinear class boundaries. Particularly effective when dealing with complex or non-Gaussian data distributions.</p></li>
</ul>
</li>
</ol>
<p>In summary, LDA and QDA share a foundation in Bayesian principles and the assumption of normality, yet they differ in terms of their handling of covariance matrices, decision boundary modeling, parameter estimation, and computational complexity. The choice between LDA and QDA should be guided by the characteristics of the data and the specific classification problem at hand.</p>
<section id="using-qda-with-scikit-learn-sklearn">
<h3><span class="section-number">11.4.5.1. </span>Using QDA with scikit-learn (sklearn)<a class="headerlink" href="#using-qda-with-scikit-learn-sklearn" title="Permalink to this heading">#</a></h3>
<p>Scikit-learn, a popular Python library, offers an accessible implementation of QDA. The <code class="docutils literal notranslate"><span class="pre">QuadraticDiscriminantAnalysis</span></code> class within scikit-learn facilitates QDA implementation for tasks such as dimensionality reduction and classification <span id="id29">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.”</p>
<p><font color='Blue'><b>Example:</b></font> The digits dataset comprises a collection of 8x8 pixel images depicting various numerical digits. Within the dataset, the <code class="docutils literal notranslate"><span class="pre">images</span></code> attribute holds 8x8 arrays representing grayscale values corresponding to each image. For illustrative purposes, we will leverage these arrays to visualize the initial four images. Notably, the ‘target’ attribute in the dataset retains information about the numerical digit portrayed by each image. This informative detail is seamlessly incorporated into the titles of the four plots showcased below.</p>
<p><font color='Blue'><b>Example:</b></font> Comparing Linear Discriminant Analysis and Quadratic Discriminant Analysis for Multi-label Classification</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">,</span> <span class="n">QuadraticDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Define color and marker specifications</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">,</span> <span class="s1">&#39;#B2FF66&#39;</span><span class="p">,</span> <span class="s1">&#39;#0096ff&#39;</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#fdceca&#39;</span><span class="p">,</span> <span class="s1">&#39;#ebdbfa&#39;</span><span class="p">,</span> <span class="s1">&#39;#e9ffd3&#39;</span><span class="p">,</span> <span class="s1">&#39;#c0def4&#39;</span><span class="p">])</span>
<span class="n">cmap_</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Generate synthetic data using the make_blobs function</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Split data into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="bp">cls</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="n">LinearDiscriminantAnalysis</span><span class="p">(),</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()],</span> <span class="s1">&#39;ab&#39;</span><span class="p">):</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span>
                                           <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    <span class="c1"># Scatter plot of data points</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                    <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[A-Z][a-z]*&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;()&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)))</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.69</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;F1 Score = </span><span class="si">{</span><span class="n">f1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                   <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
                   <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;Whitesmoke&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">))</span>  <span class="c1"># Add background color</span>

    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;LDA and QDA on the test set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fd0e47c434bd0595654de8be1a0436dd534ecee606530b191bc9d58218f8d95b.png" src="../_images/fd0e47c434bd0595654de8be1a0436dd534ecee606530b191bc9d58218f8d95b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">,</span> <span class="n">QuadraticDiscriminantAnalysis</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">LinearDiscriminantAnalysis</span><span class="p">(),</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()]</span>

<span class="c1"># Function to print a line of underscores for separation</span>
<span class="k">def</span> <span class="nf">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">60</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">31</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Sample data X and y are assumed to be defined here</span>

<span class="c1"># Models to be evaluated</span>
<span class="n">model_alphs</span> <span class="o">=</span> <span class="s1">&#39;ab&#39;</span>

<span class="c1"># Loop through each model</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">model_alphs</span><span class="p">):</span>
    
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[A-Z][a-z]*&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;()&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)))</span>
    
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">34</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>

    <span class="c1"># Initialize KFold cross-validator</span>
    <span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># The splitt would be 80-20!</span>

    <span class="c1"># Lists to store train and test scores for each fold</span>
    <span class="n">train_acc_scores</span><span class="p">,</span> <span class="n">test_acc_scores</span><span class="p">,</span> <span class="n">train_f1_scores</span><span class="p">,</span> <span class="n">test_f1_scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">train_class_proportions</span><span class="p">,</span> <span class="n">test_class_proportions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="c1"># DataFrames to store classification reports</span>
    <span class="n">reports_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">reports_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Perform Cross-Validation</span>
    <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Calculate class proportions for train and test sets</span>
        <span class="n">train_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>
        <span class="n">test_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

        <span class="c1"># train</span>
        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">train_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">))</span>
        <span class="n">train_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

        <span class="c1"># test</span>
        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>
        <span class="n">test_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="c1">#  Print the Train and Test Scores for each fold</span>
    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
        <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">:&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Class Proportions: </span><span class="si">{</span><span class="n">train_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Test Class Proportions: </span><span class="si">{</span><span class="n">test_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Accuracy Score = </span><span class="si">{</span><span class="n">train_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Accuracy Score = </span><span class="si">{</span><span class="n">test_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train F1 Score (weighted) = </span><span class="si">{</span><span class="n">train_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test F1 Score (weighted)= </span><span class="si">{</span><span class="n">test_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Accuracy Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Train Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Test Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;F1 Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Blue">(a) Linear Discriminant Analysis</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 1:</span>
	Train Class Proportions: [0.33375, 0.33375, 0.3325]*1600
	Test Class Proportions: [0.3325, 0.3325, 0.335]*400
	Train Accuracy Score = 0.9281, Test Accuracy Score = 0.9225
	Train F1 Score (weighted) = 0.9279, Test F1 Score (weighted)= 0.9228
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 2:</span>
	Train Class Proportions: [0.333125, 0.33375, 0.333125]*1600
	Test Class Proportions: [0.335, 0.3325, 0.3325]*400
	Train Accuracy Score = 0.9319, Test Accuracy Score = 0.9050
	Train F1 Score (weighted) = 0.9319, Test F1 Score (weighted)= 0.9045
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 3:</span>
	Train Class Proportions: [0.333125, 0.33375, 0.333125]*1600
	Test Class Proportions: [0.335, 0.3325, 0.3325]*400
	Train Accuracy Score = 0.9275, Test Accuracy Score = 0.9300
	Train F1 Score (weighted) = 0.9274, Test F1 Score (weighted)= 0.9302
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 4:</span>
	Train Class Proportions: [0.33375, 0.333125, 0.333125]*1600
	Test Class Proportions: [0.3325, 0.335, 0.3325]*400
	Train Accuracy Score = 0.9281, Test Accuracy Score = 0.9200
	Train F1 Score (weighted) = 0.9282, Test F1 Score (weighted)= 0.9194
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 5:</span>
	Train Class Proportions: [0.33375, 0.333125, 0.333125]*1600
	Test Class Proportions: [0.3325, 0.335, 0.3325]*400
	Train Accuracy Score = 0.9200, Test Accuracy Score = 0.9525
	Train F1 Score (weighted) = 0.9200, Test F1 Score (weighted)= 0.9523
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9271 ± 0.0039
	Mean Test Accuracy Score: 0.9260 ± 0.0155
<span class=" -Color -Color-Bold -Color-Bold-Red">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9271 ± 0.0039
	Mean F1 Accuracy Score (weighted): 0.9258 ± 0.0157
____________________________________________________________
================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Blue">(b) Quadratic Discriminant Analysis</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 1:</span>
	Train Class Proportions: [0.33375, 0.33375, 0.3325]*1600
	Test Class Proportions: [0.3325, 0.3325, 0.335]*400
	Train Accuracy Score = 0.9281, Test Accuracy Score = 0.9225
	Train F1 Score (weighted) = 0.9280, Test F1 Score (weighted)= 0.9228
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 2:</span>
	Train Class Proportions: [0.333125, 0.33375, 0.333125]*1600
	Test Class Proportions: [0.335, 0.3325, 0.3325]*400
	Train Accuracy Score = 0.9313, Test Accuracy Score = 0.9050
	Train F1 Score (weighted) = 0.9312, Test F1 Score (weighted)= 0.9045
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 3:</span>
	Train Class Proportions: [0.333125, 0.33375, 0.333125]*1600
	Test Class Proportions: [0.335, 0.3325, 0.3325]*400
	Train Accuracy Score = 0.9275, Test Accuracy Score = 0.9300
	Train F1 Score (weighted) = 0.9274, Test F1 Score (weighted)= 0.9302
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 4:</span>
	Train Class Proportions: [0.33375, 0.333125, 0.333125]*1600
	Test Class Proportions: [0.3325, 0.335, 0.3325]*400
	Train Accuracy Score = 0.9287, Test Accuracy Score = 0.9200
	Train F1 Score (weighted) = 0.9288, Test F1 Score (weighted)= 0.9194
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 5:</span>
	Train Class Proportions: [0.33375, 0.333125, 0.333125]*1600
	Test Class Proportions: [0.3325, 0.335, 0.3325]*400
	Train Accuracy Score = 0.9187, Test Accuracy Score = 0.9525
	Train F1 Score (weighted) = 0.9187, Test F1 Score (weighted)= 0.9523
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9269 ± 0.0043
	Mean Test Accuracy Score: 0.9260 ± 0.0155
<span class=" -Color -Color-Bold -Color-Bold-Red">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9268 ± 0.0042
	Mean F1 Accuracy Score (weighted): 0.9258 ± 0.0157
____________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>The results show the performance of the two models on five different folds of the data, which were obtained by using stratified k-fold cross-validation. This is a method that splits the data into k equal-sized parts, called folds, and preserves the proportion of the classes in each fold. For each fold, the model is trained on the remaining k-1 folds and tested on the current fold. The accuracy and F1 scores are calculated for both the training and test sets, and the mean and standard deviation of these scores are reported across the k folds.</p>
<p>The accuracy score is the proportion of correctly classified samples, and the F1 score is the harmonic mean of precision and recall, weighted by the number of samples in each class. Both scores range from 0 to 1, with higher values indicating better performance.</p>
<p>The results show that both models have similar performance, with mean test accuracy and F1 scores of 0.9260 and 0.9258, respectively. This means that both models can correctly classify about 92.6% of the test samples, on average. The standard deviations of the scores are also low, indicating that the models are stable and consistent across the folds.</p>
<p>The results also show that the performance of the models varies slightly depending on the fold. For example, the test accuracy and F1 scores are highest in fold 5 and lowest in fold 2 for both models. This suggests that some folds are easier or harder to classify than others, depending on the distribution and separation of the classes in the data.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_11"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C11S3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11.3. </span>t-Distributed Stochastic Neighbor Embedding (t-SNE)</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C11S5.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11.5. </span>Recursive Feature Elimination (RFE)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">11.4.1. Linear Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-lda-with-scikit-learn-sklearn">11.4.1.1. Using LDA with scikit-learn (sklearn)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-discriminant-analysis">11.4.2. Quadratic Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-key-steps-of-quadratic-discriminant-analysis-qda">11.4.2.1. The Key Steps of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-quadratic-discriminant-analysis-qda">11.4.2.2. Advantages of Quadratic Discriminant Analysis (QDA)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-quadratic-discriminant-analysis-qda">11.4.3. Limitations of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-quadratic-discriminant-analysis-qda">11.4.4. Applications of Quadratic Discriminant Analysis (QDA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-and-similarities-between-linear-discriminant-analysis-lda-and-quadratic-discriminant-analysis-qda">11.4.5. Differences and Similarities between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-qda-with-scikit-learn-sklearn">11.4.5.1. Using QDA with scikit-learn (sklearn)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>