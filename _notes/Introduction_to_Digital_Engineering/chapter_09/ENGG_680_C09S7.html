

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Support Vector Machines &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_09/ENGG_680_C09S7';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">10. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">10.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">10.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">11. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Support Vector Machines</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-principles-of-svm">Basic principles of SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-classifiers">Support Vector Classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formulating-the-maximal-margin-classifier">Formulating the Maximal Margin Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-with-non-linear-decision-boundaries">Classification with Non-Linear Decision Boundaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#augmented-feature-space">Augmented Feature Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-support-vector-machine-kernelized-enhancement">The Support Vector Machine: Kernelized Enhancement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-support-vector-classifier">Linear Support Vector Classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernelized-format">Kernelized Format</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearns-svc-and-svr">sklearn’s  SVC and SVR</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="support-vector-machines">
<h1>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">#</a></h1>
<p>Support Vector Machines (SVM) represent a versatile and potent paradigm in machine learning, adeptly handling both classification and regression tasks. The central tenet of SVM revolves around the discovery of an optimal hyperplane within a high-dimensional feature space, strategically demarcating data points belonging to disparate classes. This hyperplane’s selection hinges on the maximization of the margin, which denotes the spatial interval between the hyperplane and the nearest data points emanating from each distinct class. Thus, SVM operates as a finely calibrated equilibrium between achieving high-fidelity alignment with training data and fostering robust extrapolation to novel, previously unseen data instances <span id="id1">[<a class="reference internal" href="../References.html#id66" title="B. Scholkopf and A.J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. Adaptive Computation and Machine Learning series. MIT Press, 2018. ISBN 9780262536578. URL: https://books.google.ca/books?id=7r34DwAAQBAJ.">Scholkopf and Smola, 2018</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="section" id="basic-principles-of-svm">
<h2>Basic principles of SVM<a class="headerlink" href="#basic-principles-of-svm" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Hyperplane</strong>: A discernment boundary that partitions data instances of divergent classes, essentially serving as a segregation threshold.</p></li>
<li><p><strong>Support Vectors</strong>: Data instances in closest proximity to the hyperplane, wielding significant influence over its placement and orientation.</p></li>
<li><p><strong>Margin</strong>: The spatial expanse between the hyperplane and the closest support vectors, acting as a buffer zone against potential misclassification.</p></li>
<li><p><strong>Kernel Trick</strong>: An ingenious technique empowering SVM to navigate intricate non-linear data configurations by effectively remapping data points to an augmented dimensional space.</p></li>
<li><p><strong>C Parameter</strong>: Governs the compromise between maximizing the margin’s breadth and minimizing classification errors, thereby shaping SVM’s risk appetite.</p></li>
<li><p><strong>Kernel Functions</strong>: Distinct kernel functions (e.g., linear, polynomial, radial basis function) pivotal in shaping the curvature and structure of the resultant decision boundary.</p></li>
</ul>
</div>
<div class="section" id="support-vector-classifiers">
<h2>Support Vector Classifiers<a class="headerlink" href="#support-vector-classifiers" title="Permalink to this headline">#</a></h2>
<p>Support Vector Classifiers (SVCs) are a fundamental tool in binary classification problems, where the objective is to separate two classes, typically labeled as -1 and 1, by finding an optimal hyperplane. The mathematical representation of a hyperplane is as follows <span id="id2">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id63" title="K. Kuttler and I. Farah. A First Course in Linear Algebra. Lyryx Learning Incorporated, 2020. URL: https://books.google.ca/books?id=1jd8zgEACAAJ.">Kuttler and Farah, 2020</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f43c463b-d984-4f79-9471-0bdc9039e412">
<span class="eqno">()<a class="headerlink" href="#equation-f43c463b-d984-4f79-9471-0bdc9039e412" title="Permalink to this equation">#</a></span>\[\begin{equation} \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p = 0 \end{equation}\]</div>
<p>In this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta_0, \beta_1, \ldots, \beta_p \)</span> are coefficients that define the orientation and position of the hyperplane.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_1, x_2, \ldots, x_p \)</span> are the features of the data point.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font> Consider a two-dimensional space defined by the X-Y plane. A concrete example of a hyperplane within this context could be represented by the equation <span id="id3">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d3cb7299-f888-4f00-ad07-fcf7697a9656">
<span class="eqno">()<a class="headerlink" href="#equation-d3cb7299-f888-4f00-ad07-fcf7697a9656" title="Permalink to this equation">#</a></span>\[\begin{equation} 1 + 2X_1 + 3X_2 = 0\end{equation}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Create a figure and axis with a specific size</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Generate a range of values for X1</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Calculate corresponding values for X2 based on the hyperplane equation -(1 + 2*X1)/3</span>
<span class="n">X2</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">X1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>

<span class="c1"># Plot the hyperplane</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>

<span class="c1"># Set labels, limits, and aspect ratio for X and Y axes</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fill the region where 1 + 2X1 + 3X2 &gt; 0 with a light green color and annotate</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">X1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;LimeGreen&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$1 + 2X_1 + 3X_2 &gt; 0$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">)</span>

<span class="c1"># Fill the region where 1 + 2X1 + 3X2 &lt; 0 with an orange color and annotate</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">X1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$1 + 2X_1 + 3X_2 &lt; 0$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6ec5d4bb9004473185efe6ac745339397bd3223cf1ebec195047672ff338a838.png" src="../_images/6ec5d4bb9004473185efe6ac745339397bd3223cf1ebec195047672ff338a838.png" />
</div>
</div>
<p>By executing this code snippet, a vivid graphical representation of a hyperplane within a two-dimensional space emerges. The code generates a plot where the hyperplane, defined by the equation <span class="math notranslate nohighlight">\(1 + 2X_1 + 3X_2 = 0\)</span>, partitions the plane into regions where this inequality holds true and where it does not. This not only exemplifies the concept of a hyperplane in a 2D plane but also underscores the utility of programming tools in visualizing mathematical constructs.</p>
</div>
<div class="section" id="formulating-the-maximal-margin-classifier">
<h2>Formulating the Maximal Margin Classifier<a class="headerlink" href="#formulating-the-maximal-margin-classifier" title="Permalink to this headline">#</a></h2>
<p>In this section, we delve into the procedure for constructing the maximal margin hyperplane based on a set of <em>n</em> training observations <em><span class="math notranslate nohighlight">\(x_1, ..., x_n \in \mathbb{R}^p\)</span></em>, accompanied by corresponding class labels <em><span class="math notranslate nohighlight">\(y_1, ..., y_n \in \{-1, 1\}\)</span></em>. The core objective of a Support Vector Classifier (SVC) lies in the maximization of the margin (<em><span class="math notranslate nohighlight">\(M\)</span></em>), which symbolizes the perpendicular distance between the hyperplane and the nearest data points from each class. The essence of the optimization problem is to pinpoint the optimal coefficients <em><span class="math notranslate nohighlight">\(\beta_0, \beta_1, ..., \beta_p\)</span></em> that achieve the maximum <em><span class="math notranslate nohighlight">\(M\)</span></em>, under specific constraints for each training observation (<em><span class="math notranslate nohighlight">\(i\)</span></em>) <span id="id4">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>Mathematically, the optimization problem is formalized as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e544d1ec-1269-4ef3-b066-8f0f62ccb2d2">
<span class="eqno">()<a class="headerlink" href="#equation-e544d1ec-1269-4ef3-b066-8f0f62ccb2d2" title="Permalink to this equation">#</a></span>\[\begin{equation}
y_i (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}) \geq M (1 - \epsilon_i)
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><em><span class="math notranslate nohighlight">\(y_i\)</span></em> denotes the class label of the <em><span class="math notranslate nohighlight">\(i\)</span></em>-th observation, taking values -1 or 1.</p></li>
<li><p><em><span class="math notranslate nohighlight">\(x_{i1}\)</span></em>, <em><span class="math notranslate nohighlight">\(x_{i2}\)</span></em>, …, <em><span class="math notranslate nohighlight">\(x_{ip}\)</span></em> represent the features of the <em><span class="math notranslate nohighlight">\(i\)</span></em>-th observation.</p></li>
<li><p><em><span class="math notranslate nohighlight">\(\epsilon_i\)</span></em> acts as a slack variable that accommodates potential misclassification. It assumes a value of zero for accurately classified points and a positive value for misclassified points.</p></li>
</ul>
<p>These constraints collectively ensure that each observation resides on the appropriate side of the hyperplane, with a margin of at least <em><span class="math notranslate nohighlight">\(M\)</span></em>. Incorporating slack variables (<em><span class="math notranslate nohighlight">\(\epsilon_i\)</span></em>) introduces adaptability, allowing certain observations to exist within the margin or on the incorrect side of the hyperplane.</p>
<p>Another significant constraint, contributing to coefficient normalization, is captured by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9746f55d-e193-4747-863b-8dda2d4aaa40">
<span class="eqno">()<a class="headerlink" href="#equation-9746f55d-e193-4747-863b-8dda2d4aaa40" title="Permalink to this equation">#</a></span>\[\begin{equation}
\sum_{j=1}^{p} \beta_j^2 = 1
\end{equation}\]</div>
<p>This constraint mitigates the dominance of individual features in determining the classification outcome.</p>
<p>Furthermore, the regularization parameter <em><span class="math notranslate nohighlight">\(C\)</span></em> plays a pivotal role in balancing the trade-off between maximizing the margin <em><span class="math notranslate nohighlight">\(M\)</span></em> and minimizing misclassifications:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3c49c7ee-f70c-4e6a-81c7-92e327054115">
<span class="eqno">()<a class="headerlink" href="#equation-3c49c7ee-f70c-4e6a-81c7-92e327054115" title="Permalink to this equation">#</a></span>\[\begin{equation}
\sum_{i=1}^{n} \epsilon_i \leq C
\end{equation}\]</div>
<p>The optimization challenge centers on identifying coefficients <em><span class="math notranslate nohighlight">\(\beta_0, \beta_1, ..., \beta_p\)</span></em> and slack variables <em><span class="math notranslate nohighlight">\(\epsilon_i\)</span></em> that satisfy these constraints while concurrently maximizing the margin <em><span class="math notranslate nohighlight">\(M\)</span></em>. This formulation culminates in the establishment of a hyperplane that effectively segregates classes, accounting for the interplay between margin and misclassification.</p>
<p>Through the resolution of this optimization problem, a Support Vector Classifier determines the optimal hyperplane for class separation within the feature space. Notably, the observations positioned closest to this hyperplane, known as support vectors, wield substantial influence over the placement and margin of the hyperplane.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Sample data (features and class labels)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create a Support Vector Classifier</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>

<span class="c1"># Fit the model to the data</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Get the support vectors</span>
<span class="n">support_vectors</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Support Vectors:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">support_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="si">{</span><span class="n">support_vectors</span><span class="p">[:,</span><span class="w"> </span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Get the coefficients (beta values) and the intercept (beta_0)</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients (Beta values):</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept (Beta_0):</span><span class="se">\t\t\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>

<span class="c1"># Get the margin (M)</span>
<span class="n">margin</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Margin (M):</span><span class="se">\t\t\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">margin</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Get the slack variables (epsilon values)</span>
<span class="n">dual_coefs</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">dual_coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">epsilon_values</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dual_coefs</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Slack Variables (Epsilon values):</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">epsilon_values</span><span class="p">)</span>

<span class="c1"># Get the regularization parameter (C)</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">svc</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularization Parameter (C):</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Support Vectors:
		[5. 6.]
		[4. 5.]
Coefficients (Beta values):		 [[1. 1.]]
Intercept (Beta_0):			 [-10.]
Margin (M):				 +1.414
Slack Variables (Epsilon values):	 [2. 0.]
Regularization Parameter (C):		 0.5
</pre></div>
</div>
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Note that in the above code:</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">support_vectors</span></code> variable contains the support vectors that were identified by the Support Vector Classifier (SVC) during the fitting of the model. Support vectors are the data points from the training dataset that are closest to the decision boundary (hyperplane) and have the most influence on defining the decision boundary. These vectors “support” the position of the decision boundary and, as a result, are crucial in the classification process. Support vectors are important in SVM (Support Vector Machine) algorithms because they determine the margin, which is the distance between the decision boundary and the nearest data points. In the context of the code you’ve provided, <code class="docutils literal notranslate"><span class="pre">support_vectors</span></code> will hold the coordinates of these important data points in your feature space.</p></li>
<li><p>The variable <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> stores the coefficients associated with the features in the Support Vector Classifier (SVC) model. These coefficients represent the weights or importance of each feature in the decision boundary equation. For an SVC model with a linear kernel, the decision boundary is a hyperplane, and the coefficients represent the normal vector to this hyperplane. In other words, they define the orientation of the hyperplane in the feature space. In the context of your code, <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> contains the coefficients for each feature dimension, and you can use them to understand the relative importance of each feature in the classification decision. These coefficients are determined during the training of the SVC model and are essential for making predictions.</p></li>
<li><p>The variable <code class="docutils literal notranslate"><span class="pre">intercept</span></code> stores the intercept of the decision boundary in the Support Vector Classifier (SVC) model. The intercept represents the offset of the decision boundary from the origin in the feature space. For an SVC model with a linear kernel, the decision boundary is a hyperplane defined by the coefficients and the intercept. The intercept determines the position of this hyperplane along the feature space’s axis that is orthogonal to the hyperplane. In simpler terms, the intercept shifts the decision boundary back and forth along this axis. If we have a positive intercept, it means the decision boundary is shifted in one direction, and if you have a negative intercept, it’s shifted in the opposite direction. In the code, <code class="docutils literal notranslate"><span class="pre">intercept</span></code> contains the intercept value for the decision boundary and is determined during the training of the SVC model. It plays a crucial role in the classification process.</p></li>
<li><p>In the above code, the variable <code class="docutils literal notranslate"><span class="pre">margin</span></code> calculates the margin of the Support Vector Classifier (SVC) model. The margin represents the distance between the decision boundary (hyperplane) and the nearest support vectors. It is a measure of how well the model can separate the classes. The formula used to calculate <code class="docutils literal notranslate"><span class="pre">margin</span></code> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">margin</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>In the above code, the comments indicate that we are obtaining the slack variables (epsilon values) in the context of a Support Vector Classifier (SVC) model. Let’s break down what these lines of code are doing:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dual_coefs</span> <span class="pre">=</span> <span class="pre">svc.dual_coef_[0]</span></code>: In an SVM, the dual coefficients represent the Lagrange multipliers associated with the support vectors. These coefficients indicate the importance of each support vector in determining the location and orientation of the decision boundary. By accessing <code class="docutils literal notranslate"><span class="pre">svc.dual_coef_</span></code>, you are retrieving the dual coefficients for the support vectors. The <code class="docutils literal notranslate"><span class="pre">[0]</span></code> indexing suggests that you are extracting the dual coefficients for the first class (typically the positive class) of the binary classification problem.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epsilon_values</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">dual_coefs</span></code>: The variable <code class="docutils literal notranslate"><span class="pre">epsilon_values</span></code> is then calculated. Epsilon values, also known as slack variables, are introduced in soft-margin SVMs. They measure the degree to which a data point is allowed to be on the wrong side of the decision boundary while still satisfying the margin constraints. In this calculation, you are computing the epsilon values by subtracting the dual coefficients from 1. A value close to 1 indicates that the corresponding support vector is far from the margin, while a value close to 0 indicates that the support vector is very close to or on the wrong side of the margin.</p></li>
</ol>
<p>These values are useful for understanding the margin and classification properties of the SVM. A larger epsilon value indicates a greater margin violation by the corresponding support vector, and a smaller value indicates that the support vector is correctly classified or is very close to the margin.</p>
</li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#f7dfdf&#39;</span><span class="p">,</span> <span class="s1">&#39;#e3d3f2&#39;</span><span class="p">])</span>

<span class="c1"># Create a figure and axis</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                       <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span> <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                       <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

<span class="c1"># Define labels and markers for different classes</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">class_labels</span><span class="p">,</span> <span class="n">markers</span><span class="p">):</span>
    <span class="n">class_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">label</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">class_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span>
               <span class="n">ec</span><span class="o">=</span><span class="n">edge_colors</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">label</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">)</span>

<span class="c1"># Plot support vectors</span>
<span class="n">support_vectors</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">support_vectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">support_vectors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
           <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Support Vectors&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Support Vector Classifier&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ca4a9c9fb0b401820ce00ddab257762177853acc42458f98f0e894182e2522a4.png" src="../_images/ca4a9c9fb0b401820ce00ddab257762177853acc42458f98f0e894182e2522a4.png" />
</div>
</div>
<p>In the context of binary classification using a Support Vector Machine (SVM) with a linear kernel, it is possible to retrieve the coefficients associated with the decision boundary. These coefficients are indicative of the feature weights that define the orientation of the decision boundary, often referred to as a hyperplane. The equation representing the decision boundary takes the form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ca8e501b-30ee-441c-b2c0-1750fa8067ac">
<span class="eqno">()<a class="headerlink" href="#equation-ca8e501b-30ee-441c-b2c0-1750fa8067ac" title="Permalink to this equation">#</a></span>\[\begin{equation}w_1 \cdot x_1 + w_2 \cdot x_2 + b = 0\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span> represent the coefficients, <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> denote the features, and <span class="math notranslate nohighlight">\(b\)</span> stands for the intercept.</p>
<p>To access the coefficients <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span>, you can utilize <code class="docutils literal notranslate"><span class="pre">svc.coef_</span></code>, and to obtain the intercept <span class="math notranslate nohighlight">\(b\)</span>, you may employ <code class="docutils literal notranslate"><span class="pre">svc.intercept_</span></code> from your SVM model denoted as <code class="docutils literal notranslate"><span class="pre">svc</span></code>. The procedure for accessing these values is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hyperplane equation</span>
<span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">equation</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">w1</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1"> \cdot x_1 </span><span class="si">{</span><span class="n">w2</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1"> \cdot x_2 </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1"> = 0$&#39;</span>

<span class="c1"># Display the equation using LaTeX</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Latex</span><span class="p">,</span> <span class="n">display</span>
<span class="n">display</span><span class="p">(</span><span class="n">Latex</span><span class="p">(</span><span class="n">equation</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[+1.000 \cdot x_1 +1.000 \cdot x_2 -10.000 = 0\]</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#f7dfdf&#39;</span><span class="p">,</span> <span class="s1">&#39;#e3d3f2&#39;</span><span class="p">])</span>

<span class="c1"># Create a figure and axis</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Decision boundary display</span>
<span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span> <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

<span class="c1"># Define labels and markers for different classes</span>
<span class="n">class_info</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>

<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">marker</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="n">class_info</span><span class="p">:</span>
    <span class="n">class_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">label</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">class_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_colors</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span>
               <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">label</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">)</span>

<span class="c1"># Plot support vectors</span>
<span class="n">support_vectors</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">support_vectors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">support_vectors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span>
           <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Support Vectors&#39;</span><span class="p">)</span>

<span class="c1"># Decision boundary line</span>
<span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">line_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">line_y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">w1</span> <span class="o">/</span> <span class="n">w2</span><span class="p">)</span> <span class="o">*</span> <span class="n">line_x</span> <span class="o">-</span> <span class="p">(</span><span class="n">b</span> <span class="o">/</span> <span class="n">w2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line_x</span><span class="p">,</span> <span class="n">line_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Decision Boundary: </span><span class="si">{</span><span class="n">equation</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Plot settings</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Support Vector Classifier&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02067b6959b3b5d9114c623ff582f39d6f04acf267124c4b6e7cb65853d9bfef.png" src="../_images/02067b6959b3b5d9114c623ff582f39d6f04acf267124c4b6e7cb65853d9bfef.png" />
</div>
</div>
<div class="admonition-support-vector-classifier admonition">
<p class="admonition-title">Support Vector Classifier</p>
<p>The support vector classifier (SVC) is a robust classification method that categorizes test observations based on their positioning relative to a carefully chosen hyperplane. This hyperplane is strategically selected to efficiently segregate the majority of training observations into two distinct classes, even though there might be a minor number of misclassifications.</p>
<p>Consider a set of <span class="math notranslate nohighlight">\(n\)</span> training observations, <span class="math notranslate nohighlight">\(x_1, \ldots, x_n \in \mathbb{R}^p\)</span>, each associated with class labels <span class="math notranslate nohighlight">\(y_1, \ldots, y_n \in \{-1, 1\}\)</span>. The goal of the support vector classifier is to optimize the following mathematical formulation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-87f7425e-a29c-4e61-9085-12e9603ed792">
<span class="eqno">()<a class="headerlink" href="#equation-87f7425e-a29c-4e61-9085-12e9603ed792" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;\text{Maximize the margin } M \\
&amp;\text{subject to:} \\
&amp;\begin{cases}
\displaystyle{y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}) \geq M (1 - \epsilon_i)}\\
\displaystyle{\sum_{j=1}^{p} \beta_j^2 = 1}\\
\displaystyle{\epsilon_i \geq 0} \\
\displaystyle{\sum_{i=1}^{n} \epsilon_i \leq C}
\end{cases}
\end{align}\]</div>
<p>In this context:</p>
<ul class="simple">
<li><p>The primary objective is to maximize the margin (<span class="math notranslate nohighlight">\(M\)</span>), which signifies the separation between classes and enhances the classifier’s robustness.</p></li>
<li><p>The formulation guarantees that each training observation (<span class="math notranslate nohighlight">\(x_i\)</span>) lies on the correct side of its corresponding hyperplane. A slack variable (<span class="math notranslate nohighlight">\(\epsilon_i\)</span>) accommodates a controlled level of misclassification.</p></li>
<li><p>The constraint <span class="math notranslate nohighlight">\(\sum_{j=1}^{p} \beta_j^2 = 1\)</span> ensures that the chosen hyperplane remains properly scaled.</p></li>
<li><p>Slack variables (<span class="math notranslate nohighlight">\(\epsilon_i\)</span>) provide room for observations to fall within the margin or on the incorrect side of the hyperplane.</p></li>
<li><p>The parameter <span class="math notranslate nohighlight">\(C\)</span>, a nonnegative tuning parameter, constrains the cumulative magnitude of slack variables, thereby striking a balance between maximizing the margin and allowing for a certain level of misclassification.</p></li>
</ul>
<p>Through the harmonious interplay of these components, the support vector classifier effectively designs a hyperplane that optimally segregates classes while gracefully managing the intricacies inherent in real-world datasets. The selection of <span class="math notranslate nohighlight">\(C\)</span> and the subsequent determination of the optimal hyperplane contribute to the SVC’s ability to deliver accurate classification results, even in the presence of noise or outliers.</p>
</div>
</div>
<div class="section" id="classification-with-non-linear-decision-boundaries">
<h2>Classification with Non-Linear Decision Boundaries<a class="headerlink" href="#classification-with-non-linear-decision-boundaries" title="Permalink to this headline">#</a></h2>
<p>Support Vector Machines (SVMs) represent a significant leap beyond the conventional support vector classifier, enabling the handling of intricate non-linear decision boundaries in classification tasks. This transformative advancement stems from a novel approach that expands the feature space, providing a more comprehensive representation of the underlying data structure <span id="id5">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<div class="section" id="augmented-feature-space">
<h3>Augmented Feature Space<a class="headerlink" href="#augmented-feature-space" title="Permalink to this headline">#</a></h3>
<p>In contrast to the reliance on <em><span class="math notranslate nohighlight">\(p\)</span></em> traditional features:</p>
<div class="amsmath math notranslate nohighlight" id="equation-328b57c3-3341-485c-ba56-3dd8ecc27c93">
<span class="eqno">()<a class="headerlink" href="#equation-328b57c3-3341-485c-ba56-3dd8ecc27c93" title="Permalink to this equation">#</a></span>\[\begin{equation}
X_1, X_2, \ldots , X_p.
\end{equation}\]</div>
<p>SVMs introduce a revolutionary paradigm by extending the support vector classifier to function within an augmented feature space encompassing <em><span class="math notranslate nohighlight">\(2p\)</span></em> features:</p>
<div class="amsmath math notranslate nohighlight" id="equation-408342da-8a34-4c04-9445-e3f9f4767dfd">
<span class="eqno">()<a class="headerlink" href="#equation-408342da-8a34-4c04-9445-e3f9f4767dfd" title="Permalink to this equation">#</a></span>\[\begin{equation}
X_1, X_1^2, X_2, X_2^2, \ldots , X_p, X^2_p.
\end{equation}\]</div>
<p>This augmentation equips SVMs to adeptly address intricate classification complexities characterized by non-linear relationships <span id="id6">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
</div>
<div class="section" id="mathematical-formulation">
<h3>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">#</a></h3>
<p>The foundation of this advancement resides in the core optimization problem of SVMs, defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-929fd80b-0ec5-4118-8b87-4bb87c20cacf">
<span class="eqno">()<a class="headerlink" href="#equation-929fd80b-0ec5-4118-8b87-4bb87c20cacf" title="Permalink to this equation">#</a></span>\[\begin{align}
&amp;\text{Maximize the margin } M \\
&amp;\text{subject to:} \\
&amp;\begin{cases}
\displaystyle{y_i \left(\beta_0 + \sum_{j=1}^{p}\beta_{j1} x_{ij}+\sum_{j=1}^{p}\beta_{j2} x_{ij}^2 \right)
\geq M (1-\epsilon_i)}
\\
\displaystyle{\sum_{j=1}^{p} \sum_{k=1}^{2} \beta_{jk}^2 = 1}
\\
\displaystyle{\epsilon_i \geq 0}
\\
\displaystyle{\sum_{i=1}^{n} \epsilon_i \leq C.}
\end{cases}
\end{align}\]</div>
<p>Key Insights from this Formulation:</p>
<ul class="simple">
<li><p>The ultimate objective remains the maximization of the margin (<em><span class="math notranslate nohighlight">\(M\)</span></em>), embodying the distinction between classes.</p></li>
<li><p>The introduction of quadratic terms (<em><span class="math notranslate nohighlight">\(x_{ij}^2\)</span></em>) for each of the <em><span class="math notranslate nohighlight">\(p\)</span></em> original features enhances predictive capacity, enabling SVMs to capture intricate non-linear relationships inherent in the data.</p></li>
<li><p>Analogous to the conventional support vector classifier, the conditions ensure that training observations (<em><span class="math notranslate nohighlight">\(x_i\)</span></em>) consistently align with the correct side of their respective hyperplanes. The incorporation of slack variables (<em><span class="math notranslate nohighlight">\(\epsilon_i\)</span></em>) accommodates potential misclassifications.</p></li>
<li><p>A noteworthy addition lies in the utilization of coefficients (<em><span class="math notranslate nohighlight">\(\beta_{j1}\)</span></em> and <em><span class="math notranslate nohighlight">\(\beta_{j2}\)</span></em>) to capture both linear and quadratic facets of the transformed features. This versatility empowers SVMs to delineate non-linear decision boundaries.</p></li>
<li><p>The constraint <em><span class="math notranslate nohighlight">\(\sum_{j=1}^{p} \sum_{k=1}^{2} \beta_{jk}^2 = 1\)</span></em> ensures the appropriate scaling of the augmented hyperplane within the expanded feature space.</p></li>
<li><p>In alignment with its role in the original support vector classifier, the regularization parameter <em><span class="math notranslate nohighlight">\(C\)</span></em> retains its significance in balancing the optimization trade-off between maximizing the margin and permitting controlled misclassification.</p></li>
</ul>
<p><font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:</p>
<ul class="simple">
<li><p><strong>Number of Samples:</strong> 1000</p></li>
<li><p><strong>Number of Features:</strong> 2</p></li>
<li><p><strong>Number of Classes:</strong> 2</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as ‘Feature 1’ and ‘Feature 2’.</p></li>
</ul>
<p><strong>Outcome (Target Variable):</strong></p>
<ul class="simple">
<li><p>The dataset also includes a target variable called ‘Outcome.’ This variable assigns each data point to one of two distinct classes, identified as ‘Class 0’ and ‘Class 1’.</p></li>
</ul>
<p>The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Define colors and markers for data points</span>
<span class="n">colors</span><span class="p">,</span> <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f44336&quot;</span><span class="p">,</span> <span class="s2">&quot;#40a347&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>
<span class="n">cmap_</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
<span class="c1"># Titles for the subplots</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Linear Decision Boundaries&#39;</span><span class="p">,</span> <span class="s1">&#39;Non-Linear Decision Boundaries&#39;</span><span class="p">]</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Create SVM models and visualize decision boundaries</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">deg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
    <span class="c1"># Create SVM model with specific parameters</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Scatter plot of data points</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

    <span class="c1"># Display decision boundaries</span>
    <span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                                  <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                                  <span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_</span><span class="si">{1}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X_</span><span class="si">{2}</span><span class="s1">$&#39;</span><span class="p">)</span>
    
    <span class="c1"># Set the title for the subplot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Classification with </span><span class="si">{</span><span class="n">titles</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Add a title to the entire figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;SVM Poly Classifiers Comparison&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="c1"># Adjust layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0aa224ca2cdbd3d42dfd99533d4621cc6e1cec990f9dc7ef707a2918b2739446.png" src="../_images/0aa224ca2cdbd3d42dfd99533d4621cc6e1cec990f9dc7ef707a2918b2739446.png" />
</div>
</div>
</div>
</div>
<div class="section" id="the-support-vector-machine-kernelized-enhancement">
<h2>The Support Vector Machine: Kernelized Enhancement<a class="headerlink" href="#the-support-vector-machine-kernelized-enhancement" title="Permalink to this headline">#</a></h2>
<div class="section" id="linear-support-vector-classifier">
<h3>Linear Support Vector Classifier<a class="headerlink" href="#linear-support-vector-classifier" title="Permalink to this headline">#</a></h3>
<p>The linear support vector classifier can be expressed as follows <span id="id7">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a70b142d-99d1-4072-8aba-228196547d4d">
<span class="eqno">()<a class="headerlink" href="#equation-a70b142d-99d1-4072-8aba-228196547d4d" title="Permalink to this equation">#</a></span>\[\begin{align}
f(x) = \beta_0 + \sum_{i\in \mathcal{S}} \alpha_i\langle x, x_i\rangle,
\end{align}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x)\)</span> represents the decision function that outputs a classification score for the input feature vector <em><span class="math notranslate nohighlight">\(x\)</span></em>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept term.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_i\)</span> are the coefficients associated with the support vectors <em><span class="math notranslate nohighlight">\(x_i\)</span></em>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\langle x, x_i\rangle\)</span> denotes the inner product between the input vector <em><span class="math notranslate nohighlight">\(x\)</span></em> and the support vector <em><span class="math notranslate nohighlight">\(x_i\)</span></em>.</p></li>
<li><p>The set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> contains indices corresponding to the support vectors, which are the training samples that lie closest to the decision boundary.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Recall that the inner product of two <em>r</em>-vectors <em><span class="math notranslate nohighlight">\(a\)</span></em> and <em><span class="math notranslate nohighlight">\(b\)</span></em> is formally defined as <span class="math notranslate nohighlight">\(\langle a, b \rangle = \sum_{i=1}^{r} a_i b_i\)</span>, where the sum extends over <em><span class="math notranslate nohighlight">\(r\)</span></em> components <span id="id8">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id63" title="K. Kuttler and I. Farah. A First Course in Linear Algebra. Lyryx Learning Incorporated, 2020. URL: https://books.google.ca/books?id=1jd8zgEACAAJ.">Kuttler and Farah, 2020</a>]</span>.</p>
</div>
</div>
<div class="section" id="kernelized-format">
<h3>Kernelized Format<a class="headerlink" href="#kernelized-format" title="Permalink to this headline">#</a></h3>
<p>In a more generalized form, we replace the inner product with a function denoted as the <strong>kernel</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-80a34b61-f169-4d24-8dff-f163c090584b">
<span class="eqno">()<a class="headerlink" href="#equation-80a34b61-f169-4d24-8dff-f163c090584b" title="Permalink to this equation">#</a></span>\[\begin{align}
K(x_i, x_{i'}) = K(x_{i'}, x_i),
\end{align}\]</div>
<p>Here, <em><span class="math notranslate nohighlight">\(K\)</span></em> is a function that computes the similarity between two data points <em><span class="math notranslate nohighlight">\(x_i\)</span></em> and <em><span class="math notranslate nohighlight">\(x_{i'}\)</span></em>. This kernelized representation enables us to express <em><span class="math notranslate nohighlight">\(f(x)\)</span></em> as <span id="id9">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-108ef2d9-e828-46fb-9c1b-cfd3a2e4e285">
<span class="eqno">()<a class="headerlink" href="#equation-108ef2d9-e828-46fb-9c1b-cfd3a2e4e285" title="Permalink to this equation">#</a></span>\[\begin{align}
f(x) = \beta_0 + \sum_{i\in \mathcal{S}} \alpha_iK(x_i, x_{i'}).
\end{align}\]</div>
<p>Various types of kernels can be harnessed in SVMs to capture different types of relationships between data points. Here are some illustrative examples:</p>
<ul class="simple">
<li><p><strong>Linear Kernel:</strong> <span class="math notranslate nohighlight">\(\displaystyle{K(x_i, x_{i'}) = \sum_{j=1}^{p} x_{ij}x_{i'j}}\)</span></p></li>
<li><p><strong>Polynomial Kernel:</strong> <span class="math notranslate nohighlight">\(\displaystyle{K(x_i, x_{i'}) = \left(1+\sum_{j=1}^{p} x_{ij}x_{i'j}\right)^d}\)</span></p></li>
<li><p><strong>Radial Kernel (RBF):</strong> <span class="math notranslate nohighlight">\(\displaystyle{K(x_i, x_{i'}) = \exp\left(-\gamma \sum_{j=1}^{p} (x_{ij}-x_{i'j})^2\right)}\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span> are hyperparameters that influence the shape and flexibility of the decision boundary.</p>
<p>Scikit-learn’s SVM implementation provides flexibility in choosing the appropriate kernel and tuning hyperparameters to achieve the best classification performance for different types of data. The classifier aims to maximize the margin while ensuring accurate classification and controlled misclassification, guided by the chosen kernel’s ability to capture the data’s underlying structure.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>In the context of the provided text, <span class="math notranslate nohighlight">\(x'\)</span> is used as a placeholder to represent another data point. It’s not a specific variable; rather, it’s a general notation to indicate that the kernel function <span class="math notranslate nohighlight">\(K(x_i, x_{i'})\)</span> computes the similarity between the data point <span class="math notranslate nohighlight">\(x_i\)</span> and some other data point <span class="math notranslate nohighlight">\(x_{i'}\)</span>.</p>
<p>So, <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_{i'}\)</span> are two different data points, and <span class="math notranslate nohighlight">\(K(x_i, x_{i'})\)</span> measures the similarity between them using the chosen kernel function. The notation <span class="math notranslate nohighlight">\(x'\)</span> is used to emphasize that this kernel function can be applied to any pair of data points in the dataset, not just <span class="math notranslate nohighlight">\(x_i\)</span> and $x_{i’}). It’s a way to express the general concept of similarity between data points in the context of Support Vector Machines (SVM) and kernelized representations.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>The sklearn function also includes the following kernels:</p>
<ul class="simple">
<li><p><strong>Sigmoid Kernel (kernel=‘sigmoid’):</strong>
The sigmoid kernel maps data into a sigmoid-like space. It can be useful when dealing with data that might not be linearly separable in the original feature space. However, it’s less commonly used compared to other kernels, as it might be sensitive to hyperparameters.</p></li>
<li><p><strong>ecomputed Kernel (kernel=‘precomputed’):</strong>
Instead of using one of the built-in kernels, you can provide your own precomputed kernel matrix. This is useful when you’ve already calculated the similarity or distance between data points using a custom kernel and want to use it directly.</p></li>
</ul>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>The Support Vector Machine (SVM) algorithm was initially developed for binary classification tasks, where it aimed to separate data points into two distinct classes. However, in practice, many real-world problems involve multiple classes. Scikit-learn (sklearn), a widely used machine learning library, can handle multiclass classification with SVMs and other algorithms through techniques such as one-vs-one and one-vs-rest strategies <span id="id10">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>In the one-vs-one strategy, a separate binary classifier is trained for each pair of classes, effectively dividing the multiclass problem into a series of binary decisions. During classification, these binary classifiers vote, and the class with the most votes is the predicted class.</p>
<p>Conversely, the one-vs-rest strategy trains one binary classifier for each class, treating it as the positive class while considering all other classes as the negative class. The final prediction assigns the class associated with the binary classifier that outputs the highest confidence.</p>
<p>Sklearn’s ability to handle multiclass problems with SVMs is an extension of the original binary classification concept, enabling the algorithm to be applied to a wider range of practical applications. This extension enhances the versatility of SVMs and makes them more applicable to academic and real-world scenarios involving multiple classes <span id="id11">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
</div>
<p><font color='Blue'><b>Example:</b></font> In this example, we’re using SVM classifiers along with matplotlib to demonstrate how various SVM kernels behave on the <a class="reference external" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris dataset</a>. We aim to show their decision boundaries in a grid of four smaller plots (2x2).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Load data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Create SVM models</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)]</span>

<span class="c1"># Feature labels</span>
<span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">title</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Cm&#39;</span><span class="p">,</span><span class="s1">&#39;CM&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[:</span><span class="mi">2</span><span class="p">]]</span>

<span class="c1"># Fit models to data</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>

<span class="c1"># Titles for the plots</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LinearSVC (linear kernel)&quot;</span><span class="p">,</span>
          <span class="s2">&quot;SVC with RBF kernel&quot;</span><span class="p">,</span>
          <span class="s2">&quot;SVC with polynomial (degree 3) kernel&quot;</span><span class="p">,</span>
          <span class="s2">&quot;SVC with polynomial (degree 5) kernel&quot;</span><span class="p">]</span>

<span class="c1"># Define colors and markers</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f44336&quot;</span><span class="p">,</span> <span class="s2">&quot;#40a347&quot;</span><span class="p">,</span> <span class="s1">&#39;#0086ff&#39;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#cc180b&quot;</span><span class="p">,</span> <span class="s2">&quot;#16791d&quot;</span><span class="p">,</span> <span class="s1">&#39;#11548f&#39;</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="n">cmap_</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Create 2x2 grid for plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="c1"># Create a dictionary to map target names to numbers</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;Setosa&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;Versicolor&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;Virginica&#39;</span><span class="p">}</span>

<span class="c1"># Plot decision boundaries</span>
<span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_</span><span class="p">,</span>
                                                  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="n">ylabel</span><span class="p">)</span>
    
    <span class="c1"># Scatter plot of data points with target names</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">target_names</span><span class="p">[</span><span class="n">num</span><span class="p">])</span>
        
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Flower Type&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>

<span class="c1"># Add a title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;SVM Classifier Comparison&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/41cd0d6b69103bc2a8b60fe3268e43d7d1364344b3dcedb5b232fd1054eae144.png" src="../_images/41cd0d6b69103bc2a8b60fe3268e43d7d1364344b3dcedb5b232fd1054eae144.png" />
</div>
</div>
<!-- `````{admonition} ROC Curve and Key Concepts

The Receiver Operating Characteristic (ROC) curve is a graphical representation used to evaluate the performance of binary classification models across different classification thresholds. It illustrates the trade-off between the sensitivity (True Positive Rate, TPR) and the specificity (True Negative Rate, TNR) of the model. The ROC curve plots TPR against the False Positive Rate (FPR) for various threshold values {cite:p}`bloch2010progress`.

The key concepts are as follows {cite:p}`bloch2010progress`:

1. **Binary Classification Problem**: ROC is used in binary classification scenarios where the target variable has two classes, often denoted as positive and negative. Examples include medical diagnoses (disease or no disease) or spam classification (spam or not spam).

1. **True Positive Rate (Sensitivity)**: It's the ratio of correctly predicted positive instances to the total actual positive instances. It indicates how well the model identifies positive instances.

   \begin{equation}
   TPR = \frac{TP}{TP + FN}
   \end{equation}
   where $TP$ represents the number of true positive instances and $FN$ represents the number of false negative instances. Sensitivity indicates the model's ability to correctly identify positive instances.

1. **False Positive Rate**: It's the ratio of incorrectly predicted positive instances to the total actual negative instances. It measures how often the model makes false predictions for the negative class.
   \begin{equation}
   FPR = \frac{FP}{FP + TN}
   \end{equation}
   where $FP$ represents the number of false positive instances and $TN$ represents the number of true negative instances. FPR quantifies the rate at which the model incorrectly predicts positive instances for the negative class.

1. **True Negative Rate (Specificity)**: It's the ratio of correctly predicted negative instances to the total actual negative instances. It shows how well the model identifies negative instances.
   \begin{equation}
   TNR = \frac{TN}{TN + FP}
   \end{equation}
   Specificity measures the model's ability to correctly identify negative instances.

1. **False Negative Rate**: It's the ratio of incorrectly predicted negative instances to the total actual positive instances. It quantifies the rate at which the model misclassifies positive instances.
   \begin{equation}
   FNR = \frac{FN}{FN + TP}
   \end{equation}
   FNR represents the rate at which the model incorrectly predicts negative instances for the positive class.

1. **ROC Curve**: The ROC curve is a plot of sensitivity (true positive rate) against the false positive rate for various classification thresholds. Each point on the ROC curve corresponds to a particular threshold. The curve provides a visual representation of how well the model separates the two classes as the threshold changes.

1. **Area Under the Curve (AUC)**: The AUC measures the area under the ROC curve. It quantifies the overall performance of the model. An AUC value closer to 1 indicates a better-performing model, while an AUC of 0.5 suggests that the model's performance is no better than random guessing.

   - AUC = 1: Perfect classifier
   - AUC = 0.5: Random guessing
   - AUC > 0.5: Model performs better than random guessing
   - AUC < 0.5: Model performs worse than random guessing

````` --></div>
</div>
<div class="section" id="sklearns-svc-and-svr">
<h2>sklearn’s  SVC and SVR<a class="headerlink" href="#sklearns-svc-and-svr" title="Permalink to this headline">#</a></h2>
<p>Support Vector Machine (SVM), a prominent machine learning algorithm accessible within the scikit-learn (sklearn) library, exhibits versatility and finds utility in both classification and regression tasks <span id="id12">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>. Within the sklearn SVM module, two primary classes are available:</p>
<ul class="simple">
<li><p><strong>Support Vector Classification (SVC):</strong>
SVC specializes in addressing classification tasks. Its core aim is to identify the optimal hyperplane that effectively segregates data points into distinct classes, with the additional goal of maximizing the margin separating these classes. The definition of this hyperplane is influenced by a subset of data points referred to as support vectors. Furthermore, SVC can adeptly address both linear and non-linear classification challenges through the application of various kernel functions, including linear, polynomial, radial basis function (RBF), and sigmoid kernels <span id="id13">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>Support Vector Regression (SVR):</strong>
In contrast to SVC, SVR is tailored to regression tasks. Instead of seeking a hyperplane for class separation, SVR aims to determine a hyperplane that minimizes the error between predicted values and actual target values, all while considering a predefined margin of tolerance. Similar to SVC, SVR possesses the capability to handle non-linear regression tasks through the use of kernel functions <span id="id14">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
</ul>
<p>The choice of kernel and hyperparameters is a pivotal decision, contingent on the data’s characteristics and the specific problem at hand. Rigorous data preprocessing and comprehensive model evaluation are indispensable for achieving optimal performance within the context of academic or research endeavors.</p>
<p><font color='Blue'><b>Example:</b></font>
In this example, we focus on the Auto MPG dataset, which is sourced from the <a class="reference external" href="http://archive.ics.uci.edu/dataset/9/auto+mpg">UCI Machine Learning Repository</a>. The aim is to demonstrate the use of Support Vector Regression (SVR) with this dataset.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Feature</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MPG</p></td>
<td><p>Fuel efficiency in miles per gallon. Higher values indicate better fuel efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p>Cylinders</p></td>
<td><p>Number of engine cylinders, indicating engine capacity and power. Common values: 4, 6, and 8 cylinders.</p></td>
</tr>
<tr class="row-even"><td><p>Displacement</p></td>
<td><p>Engine volume in cubic inches or cubic centimeters, reflecting engine size and power. Higher values mean more power.</p></td>
</tr>
<tr class="row-odd"><td><p>Horsepower</p></td>
<td><p>Engine horsepower, measuring its ability to perform work. Higher values indicate a more powerful engine.</p></td>
</tr>
<tr class="row-even"><td><p>Weight</p></td>
<td><p>Vehicle mass in pounds or kilograms, influencing fuel efficiency. Lighter vehicles tend to have better MPG.</p></td>
</tr>
<tr class="row-odd"><td><p>Acceleration</p></td>
<td><p>Vehicle’s acceleration performance, usually measured in seconds to reach 60 mph (or 100 km/h) from a standstill.</p></td>
</tr>
<tr class="row-even"><td><p>Model Year</p></td>
<td><p>Year of vehicle manufacturing, useful for tracking technology and efficiency trends.</p></td>
</tr>
<tr class="row-odd"><td><p>Origin</p></td>
<td><p>Country or region of vehicle origin, often a categorical variable. Values: 1 (USA), 2 (Germany), 3 (Japan), and more.</p></td>
</tr>
<tr class="row-even"><td><p>Car Name</p></td>
<td><p>Name or model of the car, useful for identification and categorization of different car models.</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip</span>

<span class="c1"># Define column names based on the dataset&#39;s description</span>
<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">,</span> <span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span> <span class="s1">&#39;Displacement&#39;</span><span class="p">,</span> <span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Acceleration&#39;</span><span class="p">,</span> <span class="s1">&#39;Model_Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Origin&#39;</span><span class="p">,</span> <span class="s1">&#39;Car_Name&#39;</span><span class="p">]</span>

<span class="c1"># Read the dataset with column names, treating &#39;?&#39; as missing values, and remove rows with missing values</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;auto-mpg.data&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
                          <span class="n">na_values</span><span class="o">=</span><span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Remove the &#39;Car_Name&#39; column from the DataFrame</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Car_Name&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">auto_mpg_df</span><span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">])</span>
<span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;MPG&#39;</span> <span class="p">:</span> <span class="s1">&#39;ln(MPG)&#39;</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Display the resulting DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ln(MPG)</th>
      <th>Cylinders</th>
      <th>Displacement</th>
      <th>Horsepower</th>
      <th>Weight</th>
      <th>Acceleration</th>
      <th>Model_Year</th>
      <th>Origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.890372</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504.0</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.708050</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693.0</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.890372</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436.0</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.772589</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433.0</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.833213</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449.0</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>387</th>
      <td>3.295837</td>
      <td>4</td>
      <td>140.0</td>
      <td>86.0</td>
      <td>2790.0</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>388</th>
      <td>3.784190</td>
      <td>4</td>
      <td>97.0</td>
      <td>52.0</td>
      <td>2130.0</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
    </tr>
    <tr>
      <th>389</th>
      <td>3.465736</td>
      <td>4</td>
      <td>135.0</td>
      <td>84.0</td>
      <td>2295.0</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>390</th>
      <td>3.332205</td>
      <td>4</td>
      <td>120.0</td>
      <td>79.0</td>
      <td>2625.0</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>391</th>
      <td>3.433987</td>
      <td>4</td>
      <td>119.0</td>
      <td>82.0</td>
      <td>2720.0</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>392 rows × 8 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="c1"># Prepare the data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;ln(MPG)&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="p">[</span><span class="s1">&#39;ln(MPG)&#39;</span><span class="p">]</span>  <span class="c1"># Target variable</span>

<span class="c1"># Initialize KFold cross-validator</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Lists to store train and test scores for each fold</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To store training R-squared scores</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>   <span class="c1"># To store testing R-squared scores</span>

<span class="c1"># Perform Cross-Validation</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">svr</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>  <span class="c1"># Create a Support Vector Regression model with an RBF kernel</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>  <span class="c1"># Extract training and testing subsets by index</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">svr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># Train the SVR model</span>
    <span class="c1"># Evaluate the model</span>
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">svr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">train_r2_score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>  <span class="c1"># Calculate the R-squared score for training</span>
    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_r2_score</span><span class="p">)</span>  <span class="c1"># Store the training score</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">svr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_r2_score</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>  <span class="c1"># Calculate the R-squared score for testing</span>
    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_r2_score</span><span class="p">)</span>  <span class="c1"># Store the testing score</span>

<span class="c1"># Print the Train and Test Scores for each fold</span>
<span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Train R-squared Score = </span><span class="si">{</span><span class="n">train_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test R-squared Score = </span><span class="si">{</span><span class="n">test_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and print the average train and test scores</span>
<span class="n">average_train_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span>
<span class="n">average_test_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">50</span> <span class="o">*</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Train R-squared Score: </span><span class="si">{</span><span class="n">average_train_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Test R-squared Score: </span><span class="si">{</span><span class="n">average_test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">50</span> <span class="o">*</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fold 1: Train R-squared Score = 0.7809, Test R-squared Score = 0.7654
Fold 2: Train R-squared Score = 0.7775, Test R-squared Score = 0.7836
Fold 3: Train R-squared Score = 0.7888, Test R-squared Score = 0.7194
Fold 4: Train R-squared Score = 0.7787, Test R-squared Score = 0.7760
Fold 5: Train R-squared Score = 0.7668, Test R-squared Score = 0.8129
__________________________________________________
Mean Train R-squared Score: 0.7785 ± 0.0070
Mean Test R-squared Score: 0.7715 ± 0.0304
__________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Here’s an explanation of the results:</p>
<ol class="arabic simple">
<li><p><strong>Individual Folds (Fold 1 to Fold 5):</strong> Each of these segments provides the outcomes for one of the five divisions in your cross-validation. Within each fold, there are two R-squared scores:</p>
<ul class="simple">
<li><p><strong>Training R-squared Score:</strong> This figure (e.g., 0.7809 in Fold 1) gauges the SVR model’s ability to fit the training data within that specific fold. An R-squared score nearing 1 suggests that the model effectively captures the training data’s patterns.</p></li>
<li><p><strong>Testing R-squared Score:</strong> This value (e.g., 0.7654 in Fold 1) measures how well the SVR model applies its knowledge to unseen test data for that fold. It illustrates the model’s performance on data it hasn’t encountered during training.</p></li>
</ul>
</li>
<li><p><strong>Average Training R-squared Score (Mean Train R-squared Score):</strong> This line presents the overall R-squared score, averaged across all the training folds (e.g., 0.7785). It serves as a summary of the model’s general ability to fit the training data during the cross-validation process.</p></li>
<li><p><strong>Average Testing R-squared Score (Mean Test R-squared Score):</strong> This line showcases the comprehensive R-squared score, averaged across all the testing folds (e.g., 0.7715). It signifies how well the SVR model generalizes to fresh, unobserved data across the various folds.</p></li>
<li><p><strong>Variability Indicators (± 0.0070 for Mean Training R-squared Score and ± 0.0304 for Mean Testing R-squared Score):</strong> These values depict the standard deviation of the R-squared scores across the corresponding subsets (training or testing folds). The standard deviation offers insights into the extent of variation or diversity in the R-squared scores. In this context, it illustrates the degree to which the model’s performance differs across the distinct folds.</p></li>
</ol>
<p><font color='Blue'><b>Example:</b></font> Here, we have a code example that demonstrates the use of a Decision Tree Classifier to visualize decision boundaries on synthetic data. This synthetic dataset is created using scikit-learn’s <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function, specifically designed for generating artificial datasets for various machine learning experiments. This particular dataset has the following characteristics:</p>
<ul class="simple">
<li><p><strong>Number of Samples:</strong> 2000</p></li>
<li><p><strong>Number of Features:</strong> 2</p></li>
<li><p><strong>Number of Classes:</strong> 4</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>Within the dataset, there are 2000 data points, each characterized by a pair of feature values, denoted as ‘Feature 1’ and ‘Feature 2’.</p></li>
</ul>
<p><strong>Target Variable:</strong></p>
<ul class="simple">
<li><p>The dataset also includes a target variable named ‘Outcome.’ This variable assigns each data point to one of four distinct classes, labeled as ‘Class 0’, ‘Class 1’, ‘Class 2’, and ‘Class 3’.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">,</span> <span class="s1">&#39;#B2FF66&#39;</span><span class="p">,</span> <span class="s1">&#39;#0096ff&#39;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">,</span> <span class="s1">&#39;#6A993D&#39;</span><span class="p">,</span> <span class="s1">&#39;#2e658c&#39;</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span><span class="p">]</span>
<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>


<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
               <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Synthetic Dataset&#39;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/269f27f4e9dce7872a96d776e66440782c2b03ae9913afb923566addd97fa93b.png" src="../_images/269f27f4e9dce7872a96d776e66440782c2b03ae9913afb923566addd97fa93b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="k">def</span> <span class="nf">Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">60</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Initialize StratifiedKFold cross-validator</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># The splitt would be 80-20!</span>

<span class="c1"># Lists to store train and test scores for each fold</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Perform Cross-Validation</span>
<span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Calculate class proportions for train and test sets</span>
    <span class="n">train_class_proportions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)]</span>
    <span class="n">test_class_proportions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)]</span>
    
    <span class="c1"># train</span>
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
    
    <span class="c1"># test</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>
    
    <span class="c1"># Print class proportions for each fold</span>
    <span class="n">Line</span><span class="p">()</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fold </span><span class="si">{</span><span class="n">fold</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- Train Class Proportions: </span><span class="si">{</span><span class="n">train_class_proportions</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- Test Class Proportions: </span><span class="si">{</span><span class="n">test_class_proportions</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- Train R-Squared Score = </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- Test R-Squared Score = </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  
<span class="c1"># Calculate and print the average train and test scores</span>
<span class="n">average_train_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span>
<span class="n">average_test_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span>
<span class="n">Line</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Train R-Squared Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Test R-Squared Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">Line</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>____________________________________________________________
<span class=" -Color -Color-Bold">Fold 1</span>
- Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*1600
- Test Class Proportions: [0.25, 0.25, 0.25, 0.25]*400
- Train R-Squared Score = 0.9337
- Test R-Squared Score = 0.9200
____________________________________________________________
<span class=" -Color -Color-Bold">Fold 2</span>
- Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*1600
- Test Class Proportions: [0.25, 0.25, 0.25, 0.25]*400
- Train R-Squared Score = 0.9306
- Test R-Squared Score = 0.9375
____________________________________________________________
<span class=" -Color -Color-Bold">Fold 3</span>
- Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*1600
- Test Class Proportions: [0.25, 0.25, 0.25, 0.25]*400
- Train R-Squared Score = 0.9350
- Test R-Squared Score = 0.9300
____________________________________________________________
<span class=" -Color -Color-Bold">Fold 4</span>
- Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*1600
- Test Class Proportions: [0.25, 0.25, 0.25, 0.25]*400
- Train R-Squared Score = 0.9363
- Test R-Squared Score = 0.9275
____________________________________________________________
<span class=" -Color -Color-Bold">Fold 5</span>
- Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*1600
- Test Class Proportions: [0.25, 0.25, 0.25, 0.25]*400
- Train R-Squared Score = 0.9281
- Test R-Squared Score = 0.9475
____________________________________________________________
Mean Train R-Squared Score: 0.9327 ± 0.0030
Mean Test R-Squared Score: 0.9325 ± 0.0094
____________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Let’s break down the results:</p>
<ol class="arabic simple">
<li><p><strong>Fold 1, Fold 2, …, Fold 5</strong>: These represent different iterations or “folds” of a 5-fold cross-validation process. Each fold involves splitting the dataset into training and test sets, training a model on the training set, and evaluating it on the test set.</p></li>
<li><p><strong>Train Class Proportions</strong>: These values indicate the class distribution within the training data for each fold. In this case, it shows that for each fold, the training data is balanced, with each of the four clusters having an equal proportion (25%) of the data. The number (e.g., 1600) represents the size of the training dataset, which is 80% of the entire dataset (2000).</p></li>
<li><p><strong>Test Class Proportions</strong>: Similar to the “Train Class Proportions,” these values indicate the class distribution within the test data for each fold. Again, the test data is balanced, with each cluster having an equal proportion (25%) of the data. The number (e.g., 400) represents the size of the test dataset, which is 20% of the entire dataset (2000).</p></li>
<li><p><strong>Train R-Squared Score</strong>: This represents the coefficient of determination (R-squared) for the model’s predictions on the training data. R-squared is a measure of how well the model fits the training data. A value close to 1.0 indicates a good fit, and in this case, it’s around 0.93, which suggests that the model fits the training data well.</p></li>
<li><p><strong>Test R-Squared Score</strong>: Similar to the “Train R-Squared Score,” this represents the R-squared score for the model’s predictions on the test data. It’s an indicator of how well the model generalizes to unseen data. A high test R-squared score (e.g., 0.92) suggests good performance on the test set.</p></li>
<li><p><strong>Mean Train R-Squared Score</strong>: This is the average of the R-squared scores from all folds on the training data. It provides an overall measure of how well the model fits the training data across all folds. The mean train R-squared score is approximately 0.9327.</p></li>
<li><p><strong>Mean Test R-Squared Score</strong>: Similarly, this is the average of the R-squared scores from all folds on the test data. It provides an overall measure of how well the model generalizes to unseen data across all folds. The mean test R-squared score is approximately 0.9325.</p></li>
<li><p><strong>Standard Deviation (e.g., ± 0.0030, ± 0.0094)</strong>: These values represent the standard deviation of the R-squared scores. A higher standard deviation suggests more variability in the performance across folds, while a lower standard deviation indicates more consistent performance. In this case, it’s relatively small, indicating consistent performance.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-principles-of-svm">Basic principles of SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-classifiers">Support Vector Classifiers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formulating-the-maximal-margin-classifier">Formulating the Maximal Margin Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-with-non-linear-decision-boundaries">Classification with Non-Linear Decision Boundaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#augmented-feature-space">Augmented Feature Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-support-vector-machine-kernelized-enhancement">The Support Vector Machine: Kernelized Enhancement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-support-vector-classifier">Linear Support Vector Classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernelized-format">Kernelized Format</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearns-svc-and-svr">sklearn’s  SVC and SVR</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>