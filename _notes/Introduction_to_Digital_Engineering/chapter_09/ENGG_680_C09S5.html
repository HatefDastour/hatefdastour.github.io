

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>K-Nearest Neighbors (K-NN) &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_09/ENGG_680_C09S5';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">10. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">10.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">10.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">11. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>K-Nearest Neighbors (K-NN)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-where-knn-is-used">Scenarios where KNN is Used:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-knn">Advantages of KNN:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-knn">Limitations of KNN:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">Example: Synthetic Dataset</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="k-nearest-neighbors-k-nn">
<h1>K-Nearest Neighbors (K-NN)<a class="headerlink" href="#k-nearest-neighbors-k-nn" title="Permalink to this headline">#</a></h1>
<p>K-Nearest Neighbors (K-NN) is a straightforward and intuitive machine learning algorithm employed for classification and regression tasks. This approach belongs to the instance-based learning category, where predictions are made by considering the “k” nearest data points to the input point. Let’s delve into the mathematical foundations of the K-NN algorithm:</p>
<ol class="arabic">
<li><p><strong>Assumption: Similar Inputs have similar outputs:</strong>
The cornerstone of the K-NN algorithm is the assumption that data points with analogous features (inputs) tend to share similar labels (outputs). This notion underpins the K-NN approach to classify a new test input by examining the labels of its k most similar training inputs.</p></li>
<li><p><strong>Classification rule: For a test input x, assign the most common label amongst its k most similar training inputs:</strong>
When faced with a fresh test input (denoted as x), the K-NN algorithm identifies the k training inputs (neighbors) that closely resemble x using a specified distance metric. It then designates the label that emerges most frequently among these k neighbors as the anticipated label for the test input x.</p></li>
<li><p><strong>Formal definition of k-NN (in terms of nearest neighbors):</strong>
Let’s dissect the formal definition of k-NN <span id="id1">[<a class="reference internal" href="../References.html#id23" title="Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1):21–27, 1967.">Cover and Hart, 1967</a>, <a class="reference internal" href="../References.html#id24" title="Kilian Weinberger. Lecture 2: k-nearest neighbors. https://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote02_kNN.html, 2022. [Online; accessed 01-September-2023].">Weinberger, 2022</a>]</span>:</p>
<ul class="simple">
<li><p>Test point: x</p></li>
<li><p>The collection of k nearest neighbors of x is symbolized as <span class="math notranslate nohighlight">\(S_x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_x\)</span> constitutes a subset of the training data <span class="math notranslate nohighlight">\(D\)</span>, with <span class="math notranslate nohighlight">\(|S_x| = k\)</span></p></li>
<li><p>For every pair (x’, y’) in the training data <span class="math notranslate nohighlight">\(D\)</span> not within <span class="math notranslate nohighlight">\(S_x\)</span>, the distance from x to x’ exceeds or equals the distance from x to the farthest point in <span class="math notranslate nohighlight">\(S_x\)</span>:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-fcfe5a8d-cf26-454a-88b4-a8a1847a5cc7">
<span class="eqno">()<a class="headerlink" href="#equation-fcfe5a8d-cf26-454a-88b4-a8a1847a5cc7" title="Permalink to this equation">#</a></span>\[\begin{equation}
   \text{dist}(\mathbf{x},\mathbf{x}')\ge\max_{(\mathbf{x}'',y'')\in S_\mathbf{x}} \text{dist}(\mathbf{x},\mathbf{x}'')
   \end{equation}\]</div>
</li>
<li><p><strong>Distance Metric:</strong>
The initial step in the K-NN algorithm entails selecting a distance metric, which gauges the resemblance or distinction between data points.</p>
<ol class="arabic">
<li><p><strong>Euclidean Distance:</strong>
Euclidean distance corresponds to the linear distance between two points in a plane or hyperplane. It equates to measuring the shortest path between these points as if drawing a straight line. This metric provides insight into the extent of displacement between two states of an object.</p>
<p>Formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d17ca6c4-df46-4b60-9916-70c2771ccfae">
<span class="eqno">()<a class="headerlink" href="#equation-d17ca6c4-df46-4b60-9916-70c2771ccfae" title="Permalink to this equation">#</a></span>\[\begin{equation}
       d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
       \end{equation}\]</div>
</li>
<li><p><strong>Manhattan Distance:</strong>
The Manhattan distance is apt when total distance traveled by an object is of interest, irrespective of direction. It simulates calculating the distance covered when moving within a city grid pattern. It is computed by summing the absolute differences between coordinates of points in n-dimensional space.</p>
<p>Formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2838c08c-2023-473d-bb6f-970c5a02d9cd">
<span class="eqno">()<a class="headerlink" href="#equation-2838c08c-2023-473d-bb6f-970c5a02d9cd" title="Permalink to this equation">#</a></span>\[\begin{equation}
       d(x, y) = \sum_{i=1}^{n}|x_i - y_i|
       \end{equation}\]</div>
</li>
<li><p><strong>Minkowski Distance:</strong>
The Minkowski distance generalizes the Euclidean and Manhattan distances as special cases. The parameter “p” shapes the distance metric. With p = 1, it’s akin to the Manhattan distance, and p = 2 corresponds to the Euclidean distance.</p>
<p>Formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-626f681a-cf30-4b97-8939-67f3eb50117d">
<span class="eqno">()<a class="headerlink" href="#equation-626f681a-cf30-4b97-8939-67f3eb50117d" title="Permalink to this equation">#</a></span>\[\begin{equation}
       d(x, y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{\frac{1}{p}}
       \end{equation}\]</div>
</li>
</ol>
</li>
<li><p><strong>Training:</strong>
During training, the algorithm stores feature vectors along with their corresponding class labels from the training dataset.</p></li>
<li><p><strong>Prediction:</strong>
In making predictions for new data points, K-NN follows these steps:</p>
<p>a. <strong>Calculate Distances:</strong>
Compute the distance between the new data point and all training data points using the chosen distance metric.</p>
<p>b. <strong>Find K Neighbors:</strong>
Identify the “k” nearest neighbors based on the computed distances.</p>
<p>c. <strong>Majority Voting (Classification) or Average (Regression):</strong>
For classification tasks, the algorithm tallies the occurrences of each class among the “k” neighbors and designates the class with the highest count as the predicted class. For regression tasks, the algorithm computes the average of the target values of the “k” neighbors to predict the value.</p>
</li>
<li><p><strong>Choosing the Value of K:</strong>
Selecting the appropriate “k” is pivotal. A smaller “k” might yield noisy predictions, while a larger “k” could smooth decision boundaries. The optimal “k” value hinges on the dataset and the specific problem.</p></li>
<li><p><strong>Weighted K-NN (Optional):</strong>
Weighted K-NN can be employed to grant greater influence to closer neighbors than those farther away. This is accomplished by assigning weights to neighbors inversely proportional to their distances.</p></li>
<li><p><strong>Normalization and Scaling (Optional):</strong>
To prevent dominance by individual features in distance calculations, normalizing or scaling the features is recommended.</p></li>
</ol>
<p>It’s important to note that K-NN’s simplicity and interpretability come with potential limitations, such as reduced efficacy on high-dimensional or feature-irrelevant data due to the curse of dimensionality. Moreover, computation can become intensive for large datasets because of the need to calculate distances for each data point.</p>
<p>Explaining the Operation of the K-Nearest Neighbors Algorithm</p>
<p>The K-Nearest Neighbors (K-NN) algorithm is a method used to classify a new data point within a dataset containing various classes or categories. It operates by assessing the proximity and similarity of the new data point to the existing data entries. This process is executed in the following sequential steps:</p>
<ol class="arabic simple">
<li><p><strong>Determination of K-Value</strong>: In the first step, a value is assigned to K, which signifies the number of neighboring data points that will be considered for classification.</p></li>
<li><p><strong>Calculation of Distances</strong>: Subsequently, the algorithm computes the distances between the new data point and all the data entries present in the dataset. The specific method for calculating these distances is elaborated upon in subsequent sections. These distances are then arranged in ascending order.</p></li>
<li><p><strong>Identification of Nearest Neighbors</strong>: The next step involves the selection of the K nearest neighbors based on the previously computed distances. These neighbors represent the existing data points in the dataset that are most closely related to the new data point.</p></li>
<li><p><strong>Classification Assignment</strong>: Finally, the new data point is classified into a specific class or category based on the majority class among its K nearest neighbors. In essence, the class that is most prevalent among these neighbors determines the classification of the new data point.</p></li>
</ol>
<p>This method provides a clear and structured approach for understanding the functioning of the K-Nearest Neighbors algorithm.</p>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="../_images/KKN_Example.jpg"><img alt="../_images/KKN_Example.jpg" src="../_images/KKN_Example.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text">An example of using KKN.</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</div>
<ul class="simple">
<li><p>Panel (a) illustrates a dataset comprising three distinct classes, namely red, green, and blue.</p></li>
<li><p>Moving to Panel (b), a pivotal step is introduced: the assignment of a value to K, which signifies the quantity of neighboring data points to take into account before categorizing the new data entry. For the sake of this explanation, let us assume a value of K equal to 3.</p></li>
<li><p>Continuing in Panel (b), out of the three nearest neighbors depicted in the diagram above, the majority class identified is red. Consequently, the new data entry is designated to belong to the red class.</p></li>
</ul>
<p>We can define a simplified KKN classified as follow:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">KNNClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        K-Nearest Neighbors Classifier.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        k (int): Number of neighbors to consider for classification.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the KNN model on the training data.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        X (numpy.ndarray): Training feature data.</span>
<span class="sd">        y (numpy.ndarray): Training labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict labels for input data.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        X (numpy.ndarray): Input feature data.</span>

<span class="sd">        Returns:</span>
<span class="sd">        numpy.ndarray: Predicted labels for input data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict a single label for an input data point.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        x (numpy.ndarray): Input feature data point.</span>

<span class="sd">        Returns:</span>
<span class="sd">        int: Predicted label for the input data point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This is Euclidean Distance</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">]</span>
        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
        <span class="n">most_common</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">k_nearest_labels</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">most_common</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s explain the above code:</p>
<ol class="arabic">
<li><p><strong>Initialization:</strong>
The <code class="docutils literal notranslate"><span class="pre">KNNClassifier</span></code> class is initialized with a parameter <code class="docutils literal notranslate"><span class="pre">k</span></code>, which specifies the number of neighbors to consider for classification.</p></li>
<li><p><strong>Training (fit method):</strong>
In the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, the model is trained on a labeled dataset. The training feature data is stored in <code class="docutils literal notranslate"><span class="pre">self.X_train</span></code>, and the corresponding labels are stored in <code class="docutils literal notranslate"><span class="pre">self.y_train</span></code>.</p></li>
<li><p><strong>Prediction (predict method):</strong>
In the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method, predictions are made for a set of input feature data points (<code class="docutils literal notranslate"><span class="pre">X</span></code>). For each data point <code class="docutils literal notranslate"><span class="pre">x</span></code> in <code class="docutils literal notranslate"><span class="pre">X</span></code>, the <code class="docutils literal notranslate"><span class="pre">_predict</span></code> method is called to predict its label.</p></li>
<li><p><strong>Individual Prediction (_predict method):</strong>
The <code class="docutils literal notranslate"><span class="pre">_predict</span></code> method is where the core prediction happens for a single data point <code class="docutils literal notranslate"><span class="pre">x</span></code>. Here’s how it works:</p>
<p>a. <strong>Calculate Distances:</strong></p>
<ul class="simple">
<li><p>For the input data point <code class="docutils literal notranslate"><span class="pre">x</span></code>, the Euclidean distance is computed to all data points in the training set <code class="docutils literal notranslate"><span class="pre">self.X_train</span></code>. The distances are stored in the <code class="docutils literal notranslate"><span class="pre">distances</span></code> list, which contains the Euclidean distance between <code class="docutils literal notranslate"><span class="pre">x</span></code> and each point in the training set.</p></li>
</ul>
<p>b. <strong>Find Nearest Neighbors:</strong></p>
<ul class="simple">
<li><p>The indices of the <code class="docutils literal notranslate"><span class="pre">k</span></code> data points with the smallest distances are determined using <code class="docutils literal notranslate"><span class="pre">np.argsort(distances)[:self.k]</span></code>. These indices represent the <code class="docutils literal notranslate"><span class="pre">k</span></code> nearest neighbors.</p></li>
</ul>
<p>c. <strong>Count Labels:</strong></p>
<ul class="simple">
<li><p>The labels of these <code class="docutils literal notranslate"><span class="pre">k</span></code> nearest neighbors are collected from the <code class="docutils literal notranslate"><span class="pre">self.y_train</span></code> array.</p></li>
</ul>
<p>d. <strong>Majority Vote:</strong></p>
<ul class="simple">
<li><p>Finally, a majority vote is performed among the labels of the <code class="docutils literal notranslate"><span class="pre">k</span></code> nearest neighbors. The label that occurs most frequently is selected as the predicted label for the input data point <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Predict for All Data Points:</strong>
In the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method, this process is repeated for all data points in the input feature data <code class="docutils literal notranslate"><span class="pre">X</span></code>, and the predicted labels are stored in the <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> list.</p></li>
<li><p><strong>Return Predictions:</strong>
The predicted labels for all input data points are returned as a NumPy array.</p></li>
</ol>
<p>For practical implementation, the widely-used <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"><strong>KNeighborsClassifier</strong></a> class from scikit-learn is a powerful tool. This class simplifies the creation of a KNN classifier, its fitting to training data, and the making of predictions for new observations. It wraps the entire process described earlier and provides a range of options for customization, including distance metrics, weighting strategies, and more. Using this class, you can efficiently harness the capabilities of the K-Nearest Neighbors algorithm while enjoying the convenience of a well-designed interface.</p>
<p><font color='Blue'><b>Example</b></font>: The <strong>Iris Dataset</strong> serves as a quintessential example in explaining cross-validations. Crafted by the eminent British biologist and statistician Ronald A. Fisher in 1936, the dataset finds its roots in the realm of discriminant analysis. Named after the Iris flower species it encapsulates, this dataset stands as a foundational cornerstone in machine learning and statistics.</p>
<p><strong>Dataset Composition</strong></p>
<p>This dataset encompasses measurements of four distinct attributes in three diverse species of Iris flowers:</p>
<ol class="arabic simple">
<li><p>Sepal Length</p></li>
<li><p>Sepal Width</p></li>
<li><p>Petal Length</p></li>
<li><p>Petal Width</p></li>
</ol>
<p>The dataset encapsulates three distinct species of Iris flowers:</p>
<ol class="arabic simple">
<li><p>Iris setosa</p></li>
<li><p>Iris versicolor</p></li>
<li><p>Iris virginica</p></li>
</ol>
<p>Each Iris flower species is accompanied by precisely 50 samples, culminating in a total dataset size of 150 instances.</p>
<p>Before delving into that, let’s first examine the distribution of the dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Create a Pandas DataFrame</span>
<span class="n">iris_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">iris_df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>

<span class="c1"># Print the first few rows of the DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">iris_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Pairplot: Scatterplots for each pair of features</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">iris_df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># _ = g.map_lower(sns.kdeplot, levels=4, color=&quot;.2&quot;)</span>



<span class="c1"># Create a 2x2 grid of subplots with a specified figure size</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Flatten the 2x2 grid into a 1D array of subplots</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Get the column names of the iris DataFrame, excluding the &#39;species&#39; column</span>
<span class="n">Cols</span> <span class="o">=</span> <span class="n">iris_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># Iterate through the columns using enumerate</span>
<span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Cols</span><span class="p">):</span>
    <span class="c1"># Create a violin plot using Seaborn</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">iris_df</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
    
    <span class="c1"># Set the title for the subplot</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Violinplot of </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s2"> by species&quot;</span><span class="p">)</span>

<span class="c1"># Adjust the layout of the subplots for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/4daeecc11d413eb999f014f1be2f5a24ca0c98e29c18932f995f79314df57340.png" src="../_images/4daeecc11d413eb999f014f1be2f5a24ca0c98e29c18932f995f79314df57340.png" />
<img alt="../_images/2cf2825f769a02f59e9b02b6b3dfca708b70f79c07abf1ee130c524f3e7e1929.png" src="../_images/2cf2825f769a02f59e9b02b6b3dfca708b70f79c07abf1ee130c524f3e7e1929.png" />
</div>
</div>
<p>Classifying using our KKN classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">neighbors</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Select only the first two features for visualization</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">]</span>
<span class="n">feature_1</span><span class="p">,</span> <span class="n">feature_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                   <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">feature_1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">feature_2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Create color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;MistyRose&quot;</span><span class="p">,</span> <span class="s2">&quot;LightBlue&quot;</span><span class="p">,</span> <span class="s2">&quot;HoneyDew&quot;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Red&quot;</span><span class="p">,</span> <span class="s2">&quot;Blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Green&quot;</span><span class="p">]</span>

<span class="c1"># Different values of n_neighbors</span>
<span class="n">n_neighbors_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>

<span class="c1"># Create a 2x2 plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">n_neighbors_values</span><span class="p">):</span>
    <span class="c1"># Create a K-Nearest Neighbors classifier</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNNClassifier</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">feature_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">display_dbd</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="p">(</span><span class="n">xx0</span><span class="o">=</span><span class="n">feature_1</span><span class="p">,</span> <span class="n">xx1</span><span class="o">=</span><span class="n">feature_2</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">display_dbd</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

    <span class="c1"># Plot training points</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
                    <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

    <span class="c1"># Set the title</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="n">ylim</span><span class="p">,</span>
               <span class="n">xlabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
               <span class="n">ylabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;KKN classification (k = </span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="n">n_neighbors</span><span class="p">)</span>

<span class="c1"># Adjust the layout of the subplots for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9a57820c793116049eb3e6fe3f30534118556c4aa2fc9d2a002fd19c06103630.png" src="../_images/9a57820c793116049eb3e6fe3f30534118556c4aa2fc9d2a002fd19c06103630.png" />
</div>
</div>
<p>We can apply the sklearn’s k-Nearest Neighbors (k-NN) algorithm to the Iris dataset, experimenting with different numbers of neighbors: 3, 5, 8, 12, 15, and 20.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Different values of n_neighbors</span>
<span class="n">n_neighbors_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Create a 2x2 plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">n_neighbors_values</span><span class="p">):</span>
    <span class="c1"># Create a K-Nearest Neighbors classifier</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;distance&quot;</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                           <span class="n">ylabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>

    <span class="c1"># Plot training points</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
                    <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

    <span class="c1"># Set the title</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;KKN classification (k = </span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="n">n_neighbors</span><span class="p">)</span>
    

<span class="c1"># Adjust the layout of the subplots for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9a1b7d12d3ed8f5eea1b068fdff9a8f547c8e229f7d3b46412c0d1c4094d0e6d.png" src="../_images/9a1b7d12d3ed8f5eea1b068fdff9a8f547c8e229f7d3b46412c0d1c4094d0e6d.png" />
</div>
</div>
<div class="section" id="scenarios-where-knn-is-used">
<h2>Scenarios where KNN is Used:<a class="headerlink" href="#scenarios-where-knn-is-used" title="Permalink to this headline">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Small to Medium-Sized Datasets</strong>: KNN performs well on datasets of modest size, where the computational cost of calculating distances between data points remains manageable.</p></li>
<li><p><strong>Nonlinear Data</strong>: KNN excels in handling complex and nonlinear decision boundaries. It can capture intricate relationships between features that might not be captured by linear classifiers.</p></li>
<li><p><strong>Multi-Class Classification</strong>: KNN is naturally suited for multi-class classification tasks. It assigns a class label based on the majority class among the K nearest neighbors.</p></li>
<li><p><strong>Lazy Learning</strong>: KNN is considered a lazy learning algorithm. It doesn’t create an explicit model during training, which can be advantageous in situations where the underlying data distribution is unknown or subject to frequent changes.</p></li>
<li><p><strong>No Assumptions about Data Distribution</strong>: KNN is non-parametric and doesn’t make any assumptions about the distribution of the data. This makes it valuable when the data distribution is complex or unclear.</p></li>
<li><p><strong>Prototype-Based Learning</strong>: KNN can be viewed as a prototype-based learning method. It classifies new instances by comparing them to existing instances in the training set.</p></li>
<li><p><strong>Feature Selection and Exploration</strong>: KNN’s simplicity can aid in feature selection and exploration. It can help identify influential features by observing how the choice of neighbors impacts classification.</p></li>
<li><p><strong>Imbalanced Datasets</strong>: KNN can be adapted to handle imbalanced datasets by assigning different weights to neighbors or utilizing techniques to address class imbalances.</p></li>
<li><p><strong>Anomaly Detection</strong>: KNN can identify anomalies by flagging observations that are significantly dissimilar from their neighbors.</p></li>
<li><p><strong>Collaborative Filtering</strong>: In recommendation systems, KNN is used for collaborative filtering. It identifies similar users or items based on interaction patterns, enabling personalized recommendations.</p></li>
<li><p><strong>Initial Baseline Model</strong>: KNN serves as a starting point in model building. It provides a baseline performance that other, more complex algorithms can be compared against.</p></li>
</ol>
</div>
<div class="section" id="advantages-of-knn">
<h2>Advantages of KNN:<a class="headerlink" href="#advantages-of-knn" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Flexibility</strong>: KNN can handle various types of data and doesn’t impose strong assumptions about the data’s distribution.</p></li>
<li><p><strong>Simple Concept</strong>: The algorithm’s concept is easy to understand and implement, making it accessible to beginners.</p></li>
<li><p><strong>Intuitive Interpretation</strong>: The classifications made by KNN can often be interpreted intuitively by examining the nearest neighbors.</p></li>
</ul>
</div>
<div class="section" id="limitations-of-knn">
<h2>Limitations of KNN:<a class="headerlink" href="#limitations-of-knn" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Computational Cost</strong>: As the dataset grows, calculating distances between data points becomes computationally intensive, leading to longer processing times.</p></li>
<li><p><strong>Curse of Dimensionality</strong>: KNN’s performance degrades in high-dimensional spaces due to the curse of dimensionality. In such cases, data points tend to be equidistant, making nearest neighbors less meaningful.</p></li>
<li><p><strong>Sensitive to Noise</strong>: Outliers or noisy data points can significantly affect KNN’s predictions, leading to suboptimal results.</p></li>
<li><p><strong>Choosing K</strong>: Selecting the appropriate number of neighbors (K) can be challenging and can impact the algorithm’s performance.</p></li>
<li><p><strong>Scaling</strong>: Features with different scales can dominate the distance calculations, leading to biased results.</p></li>
</ul>
<p>In summary, KNN is valuable in scenarios where data isn’t too high-dimensional, and computational resources are sufficient for distance calculations. While it’s not a panacea, KNN serves as a benchmark algorithm, a quick baseline model, and a starting point for more advanced techniques. Understanding its strengths and limitations is crucial for effectively applying KNN in various real-world scenarios <span id="id2">[<a class="reference internal" href="../References.html#id20" title="P. Singh, D. Singh, V. Tiwari, and S. Misra. Machine Learning and Computational Intelligence Techniques for Data Engineering: Proceedings of the 4th International Conference MISP 2022, Volume 2. Lecture Notes in Electrical Engineering. Springer Nature Singapore, 2023. ISBN 9789819900473. URL: https://books.google.ca/books?id=5BW\_EAAAQBAJ.">Singh <em>et al.</em>, 2023</a>]</span>.</p>
</div>
<div class="section" id="example-synthetic-dataset">
<h2>Example: Synthetic Dataset<a class="headerlink" href="#example-synthetic-dataset" title="Permalink to this headline">#</a></h2>
<p><font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:</p>
<ul class="simple">
<li><p><strong>Number of Samples:</strong> 2000</p></li>
<li><p><strong>Number of Features:</strong> 2</p></li>
<li><p><strong>Number of Classes:</strong> 4</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>The dataset contains 2000 data points, each described by a pair of feature values. These features are represented as ‘Feature 1’ and ‘Feature 2’.</p></li>
</ul>
<p><strong>Outcome (Target Variable):</strong></p>
<ul class="simple">
<li><p>The dataset also includes a target variable called ‘Outcome.’ This variable assigns each data point to one of two distinct classes, identified as ‘Class 0’,  ‘Class 1’,  ‘Class 2’, and ‘Class 3’.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Create a DataFrame</span>
<span class="n">Data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Feature </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">Data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">,</span> <span class="s1">&#39;#B2FF66&#39;</span><span class="p">,</span> <span class="s1">&#39;#0096ff&#39;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">,</span><span class="s1">&#39;#6A993D&#39;</span><span class="p">,</span> <span class="s1">&#39;#2e658c&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#fdceca&#39;</span><span class="p">,</span> <span class="s1">&#39;#ebdbfa&#39;</span><span class="p">,</span> <span class="s1">&#39;#e9ffd3&#39;</span><span class="p">,</span> <span class="s1">&#39;#c0def4&#39;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge_color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span><span class="p">):</span>
    <span class="n">Data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Data</span><span class="o">.</span><span class="n">Outcome</span> <span class="o">==</span> <span class="n">outcome</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_color</span><span class="p">,</span>
                                           <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Outcome = </span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Synthetic Dataset&#39;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f759c4e715988e879a0967e9d7fc945858b8fb779cdb8c6ea203d078a7166c41.png" src="../_images/f759c4e715988e879a0967e9d7fc945858b8fb779cdb8c6ea203d078a7166c41.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span> <span class="n">colors</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/641977dab9449769de05196488645bdd400e7696c41664afc194e1e62753d60c.png" src="../_images/641977dab9449769de05196488645bdd400e7696c41664afc194e1e62753d60c.png" />
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>In the realm of machine learning and data analysis, a crucial step is the stratified train and test data split, typically carried out using the Scikit-Learn library in Python. This process aids in the evaluation of model performance by creating representative training and testing datasets.</p>
<p>To execute a stratified split using Scikit-Learn, you can employ the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from the <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code> module. Stratification ensures that the distribution of class labels in both the training and testing sets is similar to the original dataset, which is particularly useful when dealing with imbalanced datasets.</p>
<p>Here’s a code snippet illustrating this process:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># X represents your feature data, and y represents the corresponding labels</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>In this code, <code class="docutils literal notranslate"><span class="pre">X</span></code> refers to your feature matrix, <code class="docutils literal notranslate"><span class="pre">y</span></code> represents the target variable, and the <code class="docutils literal notranslate"><span class="pre">test_size</span></code> parameter specifies the proportion of data allocated to the test set. By setting the <code class="docutils literal notranslate"><span class="pre">stratify</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">y</span></code>, you ensure that the class distribution in the training and testing sets closely mirrors that of the original data. The <code class="docutils literal notranslate"><span class="pre">random_state</span></code> parameter is used to maintain reproducibility.</p>
<p>This stratified train and test split methodology is a fundamental practice in machine learning to prevent bias in model evaluation and validation.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Use train_test_split to split the DataFrame into train and test DataFrames</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">Data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="k">def</span> <span class="nf">plot_training_testing_pie_charts</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># Calculate the total number of observations in the training and testing datasets</span>
    <span class="n">train_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">test_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

    <span class="c1"># Create a figure with two subplots side by side</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">((</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">),</span> <span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Training&#39;</span><span class="p">,</span> <span class="s1">&#39;Testing&#39;</span><span class="p">)):</span>
        <span class="n">wedges</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">pie</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(),</span> <span class="n">labels</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                <span class="n">autopct</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%1.1f%%</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">startangle</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">shadow</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s1"> Data</span><span class="se">\n</span><span class="s1">(</span><span class="si">{</span><span class="n">train_total</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;Training&quot;</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="n">test_total</span><span class="si">}</span><span class="s1"> observations)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

        <span class="c1"># Add count values as annotations with a larger font size</span>
        <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">autotext</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span><span class="p">):</span>
            <span class="n">text</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
            <span class="n">autotext</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_training_testing_pie_charts</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e130fd5cf2b6a9f5b2511dd1693e77c07ca0a75eb0724c1d87a7594151c3a698.png" src="../_images/e130fd5cf2b6a9f5b2511dd1693e77c07ca0a75eb0724c1d87a7594151c3a698.png" />
</div>
</div>
<p>It is evident that the 1500 instances in the training set have been categorized into four groups. Similarly, the test set’s four categories have also been divided into their respective groups.</p>
<p>Training a model with <code class="docutils literal notranslate"><span class="pre">n_neighbors</span> <span class="pre">=</span> <span class="pre">5</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split &#39;train_data&#39; into X_train (features) and y_train (labels)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Assuming &#39;Outcome&#39; is the label column</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span>

<span class="c1"># Split &#39;test_data&#39; into X_test (features) and y_test (labels)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Assuming &#39;Outcome&#39; is the label column</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the necessary class from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Create a KNN classifier with 3 neighbors</span>
<span class="n">KKN</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit the KNN classifier to the training data</span>
<span class="n">KKN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_gen_cr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                             <span class="n">output_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Train Data:&#39;</span><span class="p">)</span>
<span class="n">_gen_cr</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test Data:&#39;</span><span class="p">)</span>
<span class="n">_gen_cr</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train Data:
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
</style>
<table id="T_76336">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_76336_level0_col0" class="col_heading level0 col0" >precision</th>
      <th id="T_76336_level0_col1" class="col_heading level0 col1" >recall</th>
      <th id="T_76336_level0_col2" class="col_heading level0 col2" >f1-score</th>
      <th id="T_76336_level0_col3" class="col_heading level0 col3" >support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_76336_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_76336_row0_col0" class="data row0 col0" >0.902</td>
      <td id="T_76336_row0_col1" class="data row0 col1" >0.912</td>
      <td id="T_76336_row0_col2" class="data row0 col2" >0.907</td>
      <td id="T_76336_row0_col3" class="data row0 col3" >375.000</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_76336_row1_col0" class="data row1 col0" >0.967</td>
      <td id="T_76336_row1_col1" class="data row1 col1" >0.952</td>
      <td id="T_76336_row1_col2" class="data row1 col2" >0.960</td>
      <td id="T_76336_row1_col3" class="data row1 col3" >375.000</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_76336_row2_col0" class="data row2 col0" >0.942</td>
      <td id="T_76336_row2_col1" class="data row2 col1" >0.947</td>
      <td id="T_76336_row2_col2" class="data row2 col2" >0.944</td>
      <td id="T_76336_row2_col3" class="data row2 col3" >375.000</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_76336_row3_col0" class="data row3 col0" >0.987</td>
      <td id="T_76336_row3_col1" class="data row3 col1" >0.987</td>
      <td id="T_76336_row3_col2" class="data row3 col2" >0.987</td>
      <td id="T_76336_row3_col3" class="data row3 col3" >375.000</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row4" class="row_heading level0 row4" >accuracy</th>
      <td id="T_76336_row4_col0" class="data row4 col0" >0.949</td>
      <td id="T_76336_row4_col1" class="data row4 col1" >0.949</td>
      <td id="T_76336_row4_col2" class="data row4 col2" >0.949</td>
      <td id="T_76336_row4_col3" class="data row4 col3" >0.949</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row5" class="row_heading level0 row5" >macro avg</th>
      <td id="T_76336_row5_col0" class="data row5 col0" >0.950</td>
      <td id="T_76336_row5_col1" class="data row5 col1" >0.949</td>
      <td id="T_76336_row5_col2" class="data row5 col2" >0.949</td>
      <td id="T_76336_row5_col3" class="data row5 col3" >1500.000</td>
    </tr>
    <tr>
      <th id="T_76336_level0_row6" class="row_heading level0 row6" >weighted avg</th>
      <td id="T_76336_row6_col0" class="data row6 col0" >0.950</td>
      <td id="T_76336_row6_col1" class="data row6 col1" >0.949</td>
      <td id="T_76336_row6_col2" class="data row6 col2" >0.949</td>
      <td id="T_76336_row6_col3" class="data row6 col3" >1500.000</td>
    </tr>
  </tbody>
</table>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Data:
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
</style>
<table id="T_46f06">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_46f06_level0_col0" class="col_heading level0 col0" >precision</th>
      <th id="T_46f06_level0_col1" class="col_heading level0 col1" >recall</th>
      <th id="T_46f06_level0_col2" class="col_heading level0 col2" >f1-score</th>
      <th id="T_46f06_level0_col3" class="col_heading level0 col3" >support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_46f06_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_46f06_row0_col0" class="data row0 col0" >0.858</td>
      <td id="T_46f06_row0_col1" class="data row0 col1" >0.872</td>
      <td id="T_46f06_row0_col2" class="data row0 col2" >0.865</td>
      <td id="T_46f06_row0_col3" class="data row0 col3" >125.000</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_46f06_row1_col0" class="data row1 col0" >0.967</td>
      <td id="T_46f06_row1_col1" class="data row1 col1" >0.944</td>
      <td id="T_46f06_row1_col2" class="data row1 col2" >0.955</td>
      <td id="T_46f06_row1_col3" class="data row1 col3" >125.000</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_46f06_row2_col0" class="data row2 col0" >0.889</td>
      <td id="T_46f06_row2_col1" class="data row2 col1" >0.896</td>
      <td id="T_46f06_row2_col2" class="data row2 col2" >0.892</td>
      <td id="T_46f06_row2_col3" class="data row2 col3" >125.000</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_46f06_row3_col0" class="data row3 col0" >0.992</td>
      <td id="T_46f06_row3_col1" class="data row3 col1" >0.992</td>
      <td id="T_46f06_row3_col2" class="data row3 col2" >0.992</td>
      <td id="T_46f06_row3_col3" class="data row3 col3" >125.000</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row4" class="row_heading level0 row4" >accuracy</th>
      <td id="T_46f06_row4_col0" class="data row4 col0" >0.926</td>
      <td id="T_46f06_row4_col1" class="data row4 col1" >0.926</td>
      <td id="T_46f06_row4_col2" class="data row4 col2" >0.926</td>
      <td id="T_46f06_row4_col3" class="data row4 col3" >0.926</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row5" class="row_heading level0 row5" >macro avg</th>
      <td id="T_46f06_row5_col0" class="data row5 col0" >0.927</td>
      <td id="T_46f06_row5_col1" class="data row5 col1" >0.926</td>
      <td id="T_46f06_row5_col2" class="data row5 col2" >0.926</td>
      <td id="T_46f06_row5_col3" class="data row5 col3" >500.000</td>
    </tr>
    <tr>
      <th id="T_46f06_level0_row6" class="row_heading level0 row6" >weighted avg</th>
      <td id="T_46f06_row6_col0" class="data row6 col0" >0.927</td>
      <td id="T_46f06_row6_col1" class="data row6 col1" >0.926</td>
      <td id="T_46f06_row6_col2" class="data row6 col2" >0.926</td>
      <td id="T_46f06_row6_col3" class="data row6 col3" >500.000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>The results can be visualized as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Create a loop for train and test sets</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train Set&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test Set&#39;</span><span class="p">)]):</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_set</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge_color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span><span class="p">):</span>
        <span class="n">Data</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">Data</span><span class="o">.</span><span class="n">Outcome</span> <span class="o">==</span> <span class="n">outcome</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">Data</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">X_set</span><span class="o">.</span><span class="n">index</span><span class="p">))]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_color</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Outcome&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s1"> - KNN (neighbors = 5)&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ab53ca81e504fa77c805f15bb1fd5fdaff2a107bb7de812b87cda95b9097a912.png" src="../_images/ab53ca81e504fa77c805f15bb1fd5fdaff2a107bb7de812b87cda95b9097a912.png" />
</div>
</div>
<p>Inaccurate Predictions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Create a loop for train and test sets</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train Set&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test Set&#39;</span><span class="p">)]):</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_set</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge_color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span><span class="p">):</span>
        <span class="n">Data</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">Data</span><span class="o">.</span><span class="n">Outcome</span> <span class="o">==</span> <span class="n">outcome</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">Data</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">X_set</span><span class="o">.</span><span class="n">index</span><span class="p">))]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_color</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Outcome = </span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
    <span class="c1"># Plot data points where y_set and log_reg(X_set) differ in color</span>
    <span class="n">diff_points</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">!=</span> <span class="n">KKN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_set</span><span class="p">)]</span>  <span class="c1"># Filter points where predictions differ</span>
    <span class="n">diff_points</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                     <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;Yellow&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
                     <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Inaccurate Predictions&quot;</span><span class="p">)</span>
    
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s1"> - KNN (neighbors = 5)&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="c1"># Remove the legend for each panel</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend</span><span class="p">()</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

<span class="c1"># Create a single legend for both subplots at the top</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/176596e9534de0f7f516b860b57b46a1562fe82f76e3c46b32d1c23bec6b5a67.png" src="../_images/176596e9534de0f7f516b860b57b46a1562fe82f76e3c46b32d1c23bec6b5a67.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="k">def</span> <span class="nf">plot_cm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Create a figure and axes for displaying confusion matrices side by side</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    
    <span class="n">datasets</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test&#39;</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dataset_name</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        <span class="c1"># Compute confusion matrix for the dataset predictions</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="n">cm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Confusion Matrices (Normalized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Confusion Matrices&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
        <span class="c1"># Create a ConfusionMatrixDisplay and plot it on the respective axis</span>
        <span class="n">cm_display</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>\
                        <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                              <span class="n">im_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span> <span class="k">if</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s1">&#39;Train&#39;</span> <span class="k">else</span> <span class="s1">&#39;Blues&#39;</span><span class="p">),</span>
                              <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s1"> Data&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Adjust the layout for better spacing</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_cm</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plot_cm</span><span class="p">(</span><span class="n">KKN</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f22d286adae98872ad7d231eb60d2e3e44cb3de31fdf4afcce0b36eab70d1616.png" src="../_images/f22d286adae98872ad7d231eb60d2e3e44cb3de31fdf4afcce0b36eab70d1616.png" />
<img alt="../_images/893035e15c7825baaf1bd00e8d9c82b1bbe37e79323df3b5fd345f9fafa143bb.png" src="../_images/893035e15c7825baaf1bd00e8d9c82b1bbe37e79323df3b5fd345f9fafa143bb.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-where-knn-is-used">Scenarios where KNN is Used:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-knn">Advantages of KNN:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-knn">Limitations of KNN:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">Example: Synthetic Dataset</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>