
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9.2. An Introduction to Linear Regression &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_09/ENGG680_C09S02';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.3. Multiple Linear Regression" href="ENGG680_C09S03.html" />
    <link rel="prev" title="9.1. Prologue: Statistical Metrics and Evaluation" href="ENGG680_C09S01.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C09.html">9. An Introduction to Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S04.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ENGG680_C10.html">10. Tree-Based Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S02.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S03.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ENGG680_C12.html">12. Introduction to Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>An Introduction to Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-beta-coefficients-in-linear-regression-using-least-squares">9.2.1. Estimating Beta Coefficients in Linear Regression using Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-modeling-a-single-variable-relationship">9.2.2. Simple Linear Regression: Modeling a Single Variable Relationship</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-intercept-and-the-slope">9.2.2.1. Finding  the intercept and the slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-intercept-and-the-slope-vector-format">9.2.2.2. Finding  the intercept and the slope (Vector Format)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hat-matrix-optional-content">9.2.2.3. The Hat matrix (Optional Content)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-boston-house-price-data">9.2.3. Example: Boston House-Price Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data">9.2.3.1. Loading the Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-design-matrix">9.2.3.2. Creating the Design Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-linear-regression-model">9.2.3.3. Fitting the Linear Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-hat-matrix-diagonal-values-leverages-optional-content">9.2.3.4. Calculating Hat Matrix Diagonal Values (Leverages) -  (Optional Content)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-through-statsmodels-api">9.2.3.5. Modeling through statsmodels api</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-through-sklearn-api">9.2.4. Linear Regression through sklearn API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-long-term-trends">9.2.5. Analyzing Long-Term Trends</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="an-introduction-to-linear-regression">
<h1><span class="section-number">9.2. </span>An Introduction to Linear Regression<a class="headerlink" href="#an-introduction-to-linear-regression" title="Link to this heading">#</a></h1>
<p>Linear regression is a foundational statistical technique that’s used to establish a mathematical relationship between a dependent variable (often referred to as the response variable) and one or more independent variables (referred to as predictors). It operates under the assumption that this relationship can be approximated by a straight line.</p>
<p>The core formula for linear regression is expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-79e16e08-08dc-449a-8720-f19e5be8fdeb">
<span class="eqno">(9.36)<a class="headerlink" href="#equation-79e16e08-08dc-449a-8720-f19e5be8fdeb" title="Permalink to this equation">#</a></span>\[\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \varepsilon,
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> represents the dependent variable (response).</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span> are the independent variables (predictors).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> are the coefficients (regression coefficients or model parameters).</p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\)</span> denotes the error term, accounting for unexplained variability.</p></li>
</ul>
<p>The objective of linear regression is to determine the coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> in a manner that optimally fits the observed data. The prevalent approach to estimating these coefficients is the least-squares method, which minimizes the sum of squared residuals <span id="id1">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a2149d4b-40f4-485c-93d1-e6115d0ec4a4">
<span class="eqno">(9.37)<a class="headerlink" href="#equation-a2149d4b-40f4-485c-93d1-e6115d0ec4a4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{RSS} = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2,
\end{equation}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the observed value of the dependent variable.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value based on the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
</ul>
<p>The coefficient estimation process involves using a training dataset with known values of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>. The aim is to find the <span class="math notranslate nohighlight">\(\beta\)</span> values that minimize the RSS. This is typically achieved using numerical optimization techniques.</p>
<p>After the coefficients are estimated, the linear regression model can be applied to make predictions for new data points. Given the values of the independent variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span>, the predicted value of the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> is calculated using this equation <span id="id2">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-21803773-f63b-43e6-bed6-de811a818ce4">
<span class="eqno">(9.38)<a class="headerlink" href="#equation-21803773-f63b-43e6-bed6-de811a818ce4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2 + \ldots + \hat{\beta_p} X_p,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}\)</span> denote estimated value for unknown coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</p>
<p>Linear regression accommodates both simple linear regression (with a single predictor) and multiple linear regression (involving multiple predictors). It’s extensively employed across diverse fields, including statistics, economics, social sciences, and machine learning, for tasks like prediction, forecasting, and understanding the relationships between variables.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>In this context, the hat symbol (ˆ) is employed to signify the estimated value for an unknown parameter or coefficient, or to represent the predicted value of the response.</p>
</div>
<section id="estimating-beta-coefficients-in-linear-regression-using-least-squares">
<h2><span class="section-number">9.2.1. </span>Estimating Beta Coefficients in Linear Regression using Least Squares<a class="headerlink" href="#estimating-beta-coefficients-in-linear-regression-using-least-squares" title="Link to this heading">#</a></h2>
<p>The estimation of beta coefficients in linear regression, also known as the model parameters or regression coefficients, is a key step in fitting the regression model to the data. The most commonly used method for estimating these coefficients is the least squares approach. The goal is to find the values of the coefficients that minimize the sum of squared residuals (RSS), which measures the discrepancy between the observed values and the values predicted by the model.</p>
<p>Here’s a step-by-step overview of how beta coefficients are estimated using the least squares method:</p>
<ol class="arabic">
<li><p><strong>Formulate the Objective</strong>: The linear regression model is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f46a1c37-1ee9-474b-b54d-1b683c37af6d">
<span class="eqno">(9.39)<a class="headerlink" href="#equation-f46a1c37-1ee9-474b-b54d-1b683c37af6d" title="Permalink to this equation">#</a></span>\[\begin{equation}
   Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \varepsilon
   \end{equation}\]</div>
<p>The goal is to find the values of <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> that minimize the RSS:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8cc97d99-26c9-4929-a1da-208892eec4a4">
<span class="eqno">(9.40)<a class="headerlink" href="#equation-8cc97d99-26c9-4929-a1da-208892eec4a4" title="Permalink to this equation">#</a></span>\[\begin{equation}
   \text{RSS} = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2
   \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the observed value of the dependent variable.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value of the dependent variable based on the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
</ul>
</li>
<li><p><strong>Differentiate the RSS</strong>: Take the partial derivatives of the RSS with respect to each coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span> (for <span class="math notranslate nohighlight">\(j=0\)</span> to <span class="math notranslate nohighlight">\(p\)</span>) and set them equal to zero. This step finds the values of the coefficients that minimize the RSS.</p></li>
<li><p><strong>Solve the Equations</strong>: The equations obtained from differentiation are a set of linear equations in terms of the coefficients. You can solve these equations to obtain the estimated values of <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</p></li>
<li><p><strong>Interpretation</strong>: Once the coefficients are estimated, you can interpret their values. Each coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span> represents the change in the mean response for a one-unit change in the corresponding predictor <span class="math notranslate nohighlight">\(X_j\)</span>, while holding all other predictors constant.</p></li>
<li><p><strong>Assumptions</strong>: It’s important to note that linear regression makes certain assumptions about the data, such as linearity, independence of errors, constant variance of errors (homoscedasticity), and normality of errors. These assumptions should be checked to ensure the validity of the regression results.</p></li>
<li><p><strong>Implementation</strong>: In practice, numerical optimization techniques are often used to solve the equations and estimate the coefficients, especially when dealing with complex models or large datasets. Software packages like Python’s <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> <span id="id3">[<a class="reference internal" href="../References.html#id160" title="F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>, <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> <span id="id4">[<a class="reference internal" href="../References.html#id187" title="Skipper Seabold and Josef Perktold. Statsmodels: econometric and statistical modeling with python. In 9th Python in Science Conference. 2010.">Seabold and Perktold, 2010</a>]</span>, and others provide functions for performing linear regression and estimating the coefficients.</p></li>
</ol>
<p>The resulting estimated coefficients represent the best-fitting linear relationship between the dependent variable and the independent variables in the least squares sense.</p>
<div class="admonition-key-assumptions-for-linear-regression-validity-and-consequences-of-violations admonition">
<p class="admonition-title">Key Assumptions for Linear Regression Validity and Consequences of Violations:</p>
<p>Linear regression rests upon a foundational set of assumptions that are pivotal for ensuring the model’s reliability and the accuracy of its estimates. These critical assumptions, as delineated in <span id="id5">[<a class="reference internal" href="../References.html#id73" title="J. Frost. Regression Analysis: An Intuitive Guide for Using and Interpreting Linear Models. Statistics By Jim Publishing, 2020. ISBN 9781735431185. URL: https://books.google.ca/books?id=1UPzzQEACAAJ.">Frost, 2020</a>, <a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id173" title="P. Roback and J. Legler. Beyond Multiple Linear Regression: Applied Generalized Linear Models And Multilevel Models in R. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2021. ISBN 9781439885406. URL: https://books.google.ca/books?id=pYAUEAAAQBAJ.">Roback and Legler, 2021</a>]</span>, encompass the following:</p>
<ol class="arabic simple">
<li><p><strong>Linearity</strong>: The relationship between the dependent variable and the independent variables must adhere to a linear pattern. This assumption is vital to accurately capture the inherent relationships within the data. If violated, the model might misrepresent the true relationships, leading to inaccurate predictions and biased coefficient estimates.</p></li>
<li><p><strong>Independence</strong>: The error terms (<span class="math notranslate nohighlight">\(\varepsilon\)</span>) must be independent of one another. This assumption guards against correlated errors, which could result in biased or inefficient estimates. Violations could cause the model to overstate its precision, leading to unreliable inferences.</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of the error terms should remain consistent across all levels of the predictors. In simpler terms, the spread of residuals should exhibit uniformity, reflecting consistent variability throughout the dataset. Breach of this assumption could introduce heteroscedasticity, wherein the variability of errors differs across levels of predictors. This might undermine the accuracy of statistical tests and lead to imprecise confidence intervals.</p></li>
<li><p><strong>Normality</strong>: The error terms should conform to a normal distribution with a mean of zero. This assumption is instrumental for valid statistical inference and hypothesis testing associated with linear regression. Deviations from normality might not have severe consequences if the sample size is sufficiently large. However, in smaller samples, violations could compromise the accuracy of statistical tests.</p></li>
<li><p><strong>No Perfect Multicollinearity</strong>: The independent variables should not exhibit perfect multicollinearity, implying they should not be perfectly correlated with one another. Substantial multicollinearity can render the interpretation of individual predictor effects challenging and inflate the standard errors of coefficient estimates. This can make the model sensitive to small changes in data and lead to instability in the estimates.</p></li>
</ol>
<p>Ensuring the fulfillment of these assumptions is paramount, as deviations can undermine the reliability and precision of the linear regression model’s results. It is imperative to scrutinize these assumptions before drawing conclusions from the model’s outcomes. Various diagnostic techniques, such as residual plots and statistical tests, can be employed to evaluate the soundness of these assumptions. If violations are identified, corrective measures like transformation of variables or employing more sophisticated modeling techniques might be necessary to mitigate their impact and enhance the validity of the model’s inferences.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Definition - Multicollinearity</p>
<p>Multicollinearity refers to a situation in which two or more independent variables in a regression model are highly correlated with each other. In other words, multicollinearity occurs when there is a strong linear relationship between two or more predictor variables.</p>
</div>
</section>
<section id="simple-linear-regression-modeling-a-single-variable-relationship">
<h2><span class="section-number">9.2.2. </span>Simple Linear Regression: Modeling a Single Variable Relationship<a class="headerlink" href="#simple-linear-regression-modeling-a-single-variable-relationship" title="Link to this heading">#</a></h2>
<p>Simple linear regression is a fundamental statistical technique that focuses on modeling the relationship between two variables: a dependent variable (often referred to as the response variable) and a single independent variable (commonly known as the predictor). This approach operates under the assumption of a linear relationship, implying that the connection between these variables can be approximated by a straight line.</p>
<p>The core equation for simple linear regression, as described in <span id="id6">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>, is expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-129bfe51-54c6-44a1-aa70-0b2232de6b1e">
<span class="eqno">(9.41)<a class="headerlink" href="#equation-129bfe51-54c6-44a1-aa70-0b2232de6b1e" title="Permalink to this equation">#</a></span>\[\begin{equation}
Y = \beta_0 + \beta_1 X + \varepsilon,
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> represents the dependent variable (response).</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the independent variable (predictor).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> denotes the intercept, signifying the value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X\)</span> equals zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> represents the slope, indicating the change in <span class="math notranslate nohighlight">\(Y\)</span> for a one-unit change in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\)</span> is the error term, accounting for the unexplained variability within the model.</p></li>
</ul>
<p>The objective of simple linear regression is to estimate the coefficients <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> in a manner that the line best fits the observed data. The widely-used method for estimating these coefficients is the least-squares approach, which minimizes the sum of squared residuals:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3ad49e0b-6605-4f9e-83ae-8dcf04efbc8a">
<span class="eqno">(9.42)<a class="headerlink" href="#equation-3ad49e0b-6605-4f9e-83ae-8dcf04efbc8a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{RSS} = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2,
\end{equation}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the observed value of the dependent variable (<span class="math notranslate nohighlight">\(Y\)</span>) for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value of the dependent variable (<span class="math notranslate nohighlight">\(Y\)</span>) for the <span class="math notranslate nohighlight">\(i\)</span>th data point based on the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
</ul>
<figure class="align-center" id="id21">
<a class="reference internal image-reference" href="../_images/LinearReg.png"><img alt="../_images/LinearReg.png" src="../_images/LinearReg.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9.4 </span><span class="caption-text">The aggregate of the dashed green lines is elucidated as <span class="math notranslate nohighlight">\(\text{RSS} = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2\)</span>.</span><a class="headerlink" href="#id21" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Once the coefficients <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are estimated, the simple linear regression model can be represented as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e63a81d3-9659-4d6f-b353-4220408d81cb">
<span class="eqno">(9.43)<a class="headerlink" href="#equation-e63a81d3-9659-4d6f-b353-4220408d81cb" title="Permalink to this equation">#</a></span>\[\begin{equation}
\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> denote estimated value for unknown coefficients <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>This line serves as the best linear fit to the data and is utilized for making predictions regarding new data points. Given the value of the independent variable (<span class="math notranslate nohighlight">\(X\)</span>) for a new data point, the equation enables us to calculate the predicted value of the dependent variable (<span class="math notranslate nohighlight">\(Y\)</span>).</p>
<p>Simple linear regression finds broad applications across diverse fields, including economics, finance, social sciences, and engineering. It is particularly effective when a clear linear relationship exists between the two variables, enabling predictions and understanding the impact of the independent variable on the dependent variable, as highlighted in <span id="id7">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>Nevertheless, interpreting the results of a simple linear regression analysis demands careful consideration. Factors such as goodness of fit (e.g., R-squared), statistical significance of the coefficients, and the validity of assumptions (e.g., linearity, independence of errors, normality of errors) should be taken into account to draw meaningful conclusions from the model. Furthermore, it’s essential to recognize that simple linear regression may not be suitable if the relationship between variables is nonlinear or if multiple predictors influence the dependent variable. In such cases, other approaches, such as multiple linear regression or more complex regression models, may be more appropriate, as discussed in <span id="id8">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<!--
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Generate random data for demonstration purposes
np.random.seed(42)
X = np.array([1, 2, 4, 5, 6, 7, 8, 10, 11])
n = len(X)
y = 2 + 2*X + 5*np.random.randn(1, n)

X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

# Create and fit the Linear Regression model
reg = LinearRegression()
_ = reg.fit(X, y)
y_hat = reg.predict(X)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data points and the linear regression line
X_new = np.linspace(X.min()- 0.5, X.max()+0.5, 100).reshape(-1, 1)
ax.plot(X_new, reg.predict(X_new), "r-", label="Linear Regression", zorder = 0)
ax.scatter(X.ravel(), y.ravel(), c="b", label=r"$y$", s = 40, zorder = 1)
ax.scatter(X.ravel(), y_hat.ravel(), c="k", label=r"$\hat{y}$", s = 30, marker='*', zorder = 2)
ax.legend()
# # Connect y_predict and y with lines
for i in range(len(X)):
    ax.vlines(x=X.ravel()[i], ymin=y_hat.ravel()[i], ymax=y.ravel()[i],
              colors='g', linestyles='dashed', label='Vertical Line')
# Set labels and title
ax.set_xlabel(r"$X$")
ax.set_ylabel(r"$y$")
ax.set_title("Linear Regression Example")

plt.tight_layout()
# Add legend

fig.savefig('LinearReg.png', dpi = 300)
 --><section id="finding-the-intercept-and-the-slope">
<h3><span class="section-number">9.2.2.1. </span>Finding  the intercept and the slope<a class="headerlink" href="#finding-the-intercept-and-the-slope" title="Link to this heading">#</a></h3>
<p>Finding the values of the coefficients <span class="math notranslate nohighlight">\(\beta_0\)</span> (the intercept) and <span class="math notranslate nohighlight">\(\beta_1\)</span> (the slope) in simple linear regression involves minimizing the sum of squared residuals (RSS). The RSS represents the difference between the observed values (<span class="math notranslate nohighlight">\(y_i\)</span>) and the predicted values (<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>) based on the linear regression model for each data point. Mathematically, the objective is to find <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize the following expression:</p>
<div class="amsmath math notranslate nohighlight" id="equation-37de7933-26e3-450c-9201-f31d79922096">
<span class="eqno">(9.44)<a class="headerlink" href="#equation-37de7933-26e3-450c-9201-f31d79922096" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{RSS} = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2 \end{equation}\]</div>
<p>Here’s a step-by-step mathematical derivation for finding <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Define the Linear Regression Model</strong>: Start with the basic linear regression equation:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-d57d8d78-6354-4f6a-bcbf-cb3e173c4a26">
<span class="eqno">(9.45)<a class="headerlink" href="#equation-d57d8d78-6354-4f6a-bcbf-cb3e173c4a26" title="Permalink to this equation">#</a></span>\[\begin{equation} Y = \beta_0 + \beta_1 X + \varepsilon \end{equation}\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Calculate the Predicted Values</strong>: The predicted value of <span class="math notranslate nohighlight">\(Y\)</span> (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) for each data point is given by:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-88a5c43d-6976-4c3e-9113-828e080898b9">
<span class="eqno">(9.46)<a class="headerlink" href="#equation-88a5c43d-6976-4c3e-9113-828e080898b9" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{y}_i = \beta_0 + \beta_1 x_i \end{equation}\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Define the Residuals</strong>: The residuals (<span class="math notranslate nohighlight">\(e_i\)</span>) are the differences between the observed values and the predicted values:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-40085d66-2680-497d-9dd3-c5a24574d4ff">
<span class="eqno">(9.47)<a class="headerlink" href="#equation-40085d66-2680-497d-9dd3-c5a24574d4ff" title="Permalink to this equation">#</a></span>\[\begin{equation} e_i = y_i - \hat{y}_i \end{equation}\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Formulate the Objective Function</strong>: The goal is to minimize the sum of squared residuals:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-2dca5a30-48ef-418f-8ec9-b8b0f278a0b6">
<span class="eqno">(9.48)<a class="headerlink" href="#equation-2dca5a30-48ef-418f-8ec9-b8b0f278a0b6" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{RSS} = \sum_{i=0}^{n} e_i^2 \end{equation}\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Find the Partial Derivatives</strong>: Compute the partial derivatives of the RSS with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-c118a41c-4f17-472d-a1fb-2987070bcff7">
<span class="eqno">(9.49)<a class="headerlink" href="#equation-c118a41c-4f17-472d-a1fb-2987070bcff7" title="Permalink to this equation">#</a></span>\[\begin{equation} \frac{\partial}{\partial \beta_0}\text{RSS} = -2 \sum_{i=0}^{n} (y_i - \beta_0 - \beta_1 x_i) \end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-ee61d454-d6ec-4d7e-b640-f0f53b9c39e1">
<span class="eqno">(9.50)<a class="headerlink" href="#equation-ee61d454-d6ec-4d7e-b640-f0f53b9c39e1" title="Permalink to this equation">#</a></span>\[\begin{equation} \frac{\partial}{\partial \beta_1}\text{RSS} = -2 \sum_{i=0}^{n} x_i (y_i - \beta_0 - \beta_1 x_i) \end{equation}\]</div>
<ol class="arabic simple" start="6">
<li><p><strong>Set the Derivatives to Zero and Solve</strong>: Set the partial derivatives to zero and solve the resulting system of equations for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-c430ce04-50d2-4458-bebb-b93edc2200ac">
<span class="eqno">(9.51)<a class="headerlink" href="#equation-c430ce04-50d2-4458-bebb-b93edc2200ac" title="Permalink to this equation">#</a></span>\[\begin{align}
\begin{cases}
\displaystyle{\sum_{i=0}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0},
\\
\displaystyle{\sum_{i=0}^{n} x_i (y_i - \beta_0 - \beta_1 x_i) = 0}.
\end{cases}
\end{align}\]</div>
<p>it follows that</p>
<div class="amsmath math notranslate nohighlight" id="equation-41b35528-e266-4768-8f0c-27fcd393a5a5">
<span class="eqno">(9.52)<a class="headerlink" href="#equation-41b35528-e266-4768-8f0c-27fcd393a5a5" title="Permalink to this equation">#</a></span>\[\begin{align}\begin{cases}
\displaystyle{\sum_{j = 0}^{n} y_{j}} &amp; = \displaystyle{\beta_{0}\, \sum_{j = 0}^{n} 1 + \beta_{1}\, \sum_{j = 0}^{n} x_{j}},
\\
\displaystyle{\sum_{j = 0}^{n} x_{j}\,y_{j}} &amp;= \displaystyle{\beta_{0}\,\sum_{j = 0}^{n}x_{j} + \beta_{1}\,\sum_{j = 0}^{n}x_{j}^{2}}.
\end{cases}\end{align}\]</div>
<p>The above linear system can be expressed in the following matrix form,</p>
<div class="amsmath math notranslate nohighlight" id="equation-b58100d7-c225-4dc1-87e2-217abb6f1d0c">
<span class="eqno">(9.53)<a class="headerlink" href="#equation-b58100d7-c225-4dc1-87e2-217abb6f1d0c" title="Permalink to this equation">#</a></span>\[\begin{align}\begin{bmatrix}n+1 &amp; \sum_{j = 0}^{n} x_{j}\\ \sum_{j = 0}^{n} x_{j} &amp; \sum_{j = 0}^{n} x_{j}^{2}\end{bmatrix}
\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}
= \begin{bmatrix}\sum_{j = 0}^{n}y_{j}\\ \sum_{j = 0}^{n}x_{j}y_{j}\end{bmatrix}.\end{align}\]</div>
<p>Once the optimal values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are found, the simple linear regression model is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3a19561b-748d-4e41-91e8-479818fd84fc">
<span class="eqno">(9.54)<a class="headerlink" href="#equation-3a19561b-748d-4e41-91e8-479818fd84fc" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{Y} = \hat{\beta_0} + \hat{\beta_1} X \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> is the estimated intercept.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is the estimated slope.</p></li>
</ul>
<p>This model represents the best-fit line that minimizes the sum of squared residuals and can be used for making predictions and understanding the relationship between the variables.</p>
</section>
<section id="finding-the-intercept-and-the-slope-vector-format">
<h3><span class="section-number">9.2.2.2. </span>Finding  the intercept and the slope (Vector Format)<a class="headerlink" href="#finding-the-intercept-and-the-slope-vector-format" title="Link to this heading">#</a></h3>
<p>Given:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( X \)</span>: The matrix of predictor variables (including an additional column of ones for the intercept).</p></li>
<li><p><span class="math notranslate nohighlight">\( y \)</span>: The vector of response variable values.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta \)</span>: The vector of coefficients (<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>).</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>Define the Linear Regression Model</strong>:</p></li>
</ol>
<p>The linear regression model can be represented in matrix form as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5bf11708-f200-4d6d-a905-4b0166e89b57">
<span class="eqno">(9.55)<a class="headerlink" href="#equation-5bf11708-f200-4d6d-a905-4b0166e89b57" title="Permalink to this equation">#</a></span>\[\begin{equation} y = X \beta + \varepsilon \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> is the vector of observed response values.</p></li>
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the matrix of predictor variables (including an additional column of ones for the intercept).</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta \)</span> is the vector of coefficients (<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\( \varepsilon \)</span> is the vector of error terms.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Calculate the Predicted Values</strong>:</p></li>
</ol>
<p>The predicted values of <span class="math notranslate nohighlight">\( y \)</span> (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) can be calculated as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5197541d-05a3-42da-a7eb-defd5f15a378">
<span class="eqno">(9.56)<a class="headerlink" href="#equation-5197541d-05a3-42da-a7eb-defd5f15a378" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{y} = X \beta \end{equation}\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Define the Residuals</strong>:</p></li>
</ol>
<p>The residuals can be defined as the difference between the observed values <span class="math notranslate nohighlight">\( y \)</span> and the predicted values <span class="math notranslate nohighlight">\( \hat{y} \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-24355cff-4c78-4c92-b3fa-bb31a535aa83">
<span class="eqno">(9.57)<a class="headerlink" href="#equation-24355cff-4c78-4c92-b3fa-bb31a535aa83" title="Permalink to this equation">#</a></span>\[\begin{equation} e = y - \hat{y} \end{equation}\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Formulate the Objective Function (Sum of Squared Residuals)</strong>:</p></li>
</ol>
<p>The objective is to minimize the sum of squared residuals:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f69d57e5-5e2c-49f2-9e82-bc0597af9057">
<span class="eqno">(9.58)<a class="headerlink" href="#equation-f69d57e5-5e2c-49f2-9e82-bc0597af9057" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{RSS} = e^T e = (y - \hat{y})^T (y - \hat{y}) \end{equation}\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Find the Optimal Coefficients</strong>:</p></li>
</ol>
<p>The optimal coefficients <span class="math notranslate nohighlight">\( \beta \)</span>, which we show it with <span class="math notranslate nohighlight">\( \hat{\beta} \)</span>, can be found by minimizing the RSS:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a94f7dd3-1a24-44d5-b5a1-2401b1e656a4">
<span class="eqno">(9.59)<a class="headerlink" href="#equation-a94f7dd3-1a24-44d5-b5a1-2401b1e656a4" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{\beta} = (X^T X)^{-1} X^T y \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( X^T \)</span> is the transpose of the matrix <span class="math notranslate nohighlight">\( X \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( (X^T X)^{-1} \)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\( X^T X \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( X^T y \)</span> is the matrix-vector multiplication between <span class="math notranslate nohighlight">\( X^T \)</span> and <span class="math notranslate nohighlight">\( y \)</span>.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p><strong>Use the Optimal Coefficients for Prediction</strong>:</p></li>
</ol>
<p>Once you have the optimal coefficients <span class="math notranslate nohighlight">\( \hat{\beta} \)</span>, you can use them to predict new values of <span class="math notranslate nohighlight">\( y \)</span> based on new values of <span class="math notranslate nohighlight">\( X \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f05efb49-4afa-4de2-a905-5911f82cb510">
<span class="eqno">(9.60)<a class="headerlink" href="#equation-f05efb49-4afa-4de2-a905-5911f82cb510" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{y}_{\text{new}} = X_{\text{new}} \hat{\beta} \end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\( X_{\text{new}} \)</span> is the matrix of new predictor variables.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Let’s delve into the mathematical details of how we obtain the coefficient vector <span class="math notranslate nohighlight">\( \hat{\beta} \)</span> in linear regression. We’ll use matrix algebra to explain this process step by step.</p>
<p>Starting from the objective function to minimize the sum of squared residuals (RSS):</p>
<div class="amsmath math notranslate nohighlight" id="equation-be5ac6a4-c9ad-4449-bc0b-1d3fdfa8cb01">
<span class="eqno">(9.61)<a class="headerlink" href="#equation-be5ac6a4-c9ad-4449-bc0b-1d3fdfa8cb01" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{RSS} (\beta )  = e^T e = (y - \hat{y})^T (y - \hat{y}) \end{equation}\]</div>
<p>We can expand this expression and rewrite it as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f52ccdbb-fd98-49f4-8802-856b641e0b15">
<span class="eqno">(9.62)<a class="headerlink" href="#equation-f52ccdbb-fd98-49f4-8802-856b641e0b15" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{RSS} (\beta )  = (y - X\beta)^T (y - X\beta) = (y^T - \beta^T X^T)(y - X\beta)
\end{equation}\]</div>
<p>Now, let’s differentiate RSS with respect to <span class="math notranslate nohighlight">\( \beta \)</span> to find the minimum (i.e., where the gradient is zero). We use calculus to find the minimum point:</p>
<div class="amsmath math notranslate nohighlight" id="equation-fd4d5b62-d7e6-4f41-bd65-ba6b9fe8cf03">
<span class="eqno">(9.63)<a class="headerlink" href="#equation-fd4d5b62-d7e6-4f41-bd65-ba6b9fe8cf03" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{d}{d\beta} \text{RSS}(\beta) &amp;= \frac{d}{d\beta}[(y - X\beta)^T (y - X\beta)]
\notag \\
&amp;= \frac{d}{d\beta}[(y^T - \beta^TX^T)(y - X\beta)] \quad \text{(Using the transpose property)}
\notag \\
&amp;= \frac{d}{d\beta}(y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta) \quad \text{(Expanding the expression)}
\notag \\
&amp;= \frac{d}{d\beta}(y^Ty) - \frac{d}{d\beta}(y^TX\beta) - \frac{d}{d\beta}(\beta^TX^Ty) + \frac{d}{d\beta}(\beta^TX^TX\beta)
\notag \\
&amp;= 0 - X^Ty - X^Ty + 2X^TX\beta \quad \text{(Taking derivatives)}
\notag \\
&amp;= -2X^Ty + 2X^TX\beta = -2X^T(y - X\beta).
\end{align}\]</div>
<p>Therefore,</p>
<div class="amsmath math notranslate nohighlight" id="equation-9e6e354a-3430-4c1e-8649-f5868ed2aea5">
<span class="eqno">(9.64)<a class="headerlink" href="#equation-9e6e354a-3430-4c1e-8649-f5868ed2aea5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{d}{d\beta} \text{RSS}(\beta)
= -2X^T(y - X\beta)
\end{equation}\]</div>
<p>Setting this derivative equal to zero:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e4c328e4-b8ef-4e5e-b3ed-b70abfe4e7c9">
<span class="eqno">(9.65)<a class="headerlink" href="#equation-e4c328e4-b8ef-4e5e-b3ed-b70abfe4e7c9" title="Permalink to this equation">#</a></span>\[\begin{equation} -2X^T(y - X\hat{\beta}) = 0 \end{equation}\]</div>
<p>Now, we want to isolate <span class="math notranslate nohighlight">\( \hat{\beta} \)</span>, so we’ll solve for it:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7e84c04c-5511-41b3-8a7d-e700f7103e17">
<span class="eqno">(9.66)<a class="headerlink" href="#equation-7e84c04c-5511-41b3-8a7d-e700f7103e17" title="Permalink to this equation">#</a></span>\[\begin{equation} X^T(y - X\hat{\beta}) = 0 \end{equation}\]</div>
<p>Now, we’ll distribute <span class="math notranslate nohighlight">\( X^T \)</span> and simplify:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a8fd67b4-42c6-4720-aa68-99278a1e55c3">
<span class="eqno">(9.67)<a class="headerlink" href="#equation-a8fd67b4-42c6-4720-aa68-99278a1e55c3" title="Permalink to this equation">#</a></span>\[\begin{equation} X^Ty - X^TX\hat{\beta} = 0 \end{equation}\]</div>
<p>To solve for <span class="math notranslate nohighlight">\( \hat{\beta} \)</span>, we can isolate it on one side:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ae98b7e7-e68f-4c3f-bec4-44be345b42e1">
<span class="eqno">(9.68)<a class="headerlink" href="#equation-ae98b7e7-e68f-4c3f-bec4-44be345b42e1" title="Permalink to this equation">#</a></span>\[\begin{equation} X^TX\hat{\beta} = X^Ty \end{equation}\]</div>
<p>Now, to find <span class="math notranslate nohighlight">\( \hat{\beta} \)</span>, we need to multiply both sides by the inverse of <span class="math notranslate nohighlight">\( X^TX \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-82359c7b-57ea-4de7-8d22-3651c5f019dc">
<span class="eqno">(9.69)<a class="headerlink" href="#equation-82359c7b-57ea-4de7-8d22-3651c5f019dc" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{\beta} = (X^TX)^{-1}X^Ty \end{equation}\]</div>
<p>So, <span class="math notranslate nohighlight">\( \hat{\beta} \)</span> is obtained by multiplying the inverse of the matrix <span class="math notranslate nohighlight">\( X^TX \)</span> with the product of <span class="math notranslate nohighlight">\( X^T \)</span> and <span class="math notranslate nohighlight">\( y \)</span>.</p>
</div>
</section>
<section id="the-hat-matrix-optional-content">
<h3><span class="section-number">9.2.2.3. </span>The Hat matrix (Optional Content)<a class="headerlink" href="#the-hat-matrix-optional-content" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Definition</strong>: The hat matrix, a fundamental tool in regression analysis and analysis of variance, serves as the bridge between observed values and estimations derived through the least squares method. In the context of linear regression represented in matrix form, if we denote <span class="math notranslate nohighlight">\( \hat{y} \)</span> as the vector comprising estimations calculated from the least squares parameters and <span class="math notranslate nohighlight">\( y \)</span> as a vector containing observations pertaining to the dependent variable, the relationship can be succinctly expressed as <span class="math notranslate nohighlight">\( \hat{y} = Hy \)</span>, where <span class="math notranslate nohighlight">\( H \)</span> is the hat matrix responsible for transforming <span class="math notranslate nohighlight">\( y \)</span> into <span class="math notranslate nohighlight">\( \hat{y}\)</span> <span id="id9">[<a class="reference internal" href="../References.html#id57" title="Y. Dodge. The Concise Encyclopedia of Statistics. The Concise Encyclopedia of Statistics. Springer New York, 2008. ISBN 9780387317427. URL: https://books.google.ca/books?id=k2zklGOBRDwC.">Dodge, 2008</a>]</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-31862c1b-093b-45a5-ad83-1c6c746c93c4">
<span class="eqno">(9.70)<a class="headerlink" href="#equation-31862c1b-093b-45a5-ad83-1c6c746c93c4" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{y} = X \beta = \underbrace{X(X^TX)^{-1}X^T}_{\text{Consider this part as }H}y \end{equation}\]</div>
<div class="tip admonition">
<p class="admonition-title">Definition - Hat matrix</p>
<p>The Hat matrix, often denoted as <span class="math notranslate nohighlight">\(H\)</span>, is a key concept in linear regression analysis. It plays a crucial role in understanding the influence of each observation on the fitted values of the regression model. The Hat matrix helps identify potential outliers, high-leverage points, and observations that have a significant impact on the model’s estimates <span id="id10">[<a class="reference internal" href="../References.html#id27" title="Carla Cardinali. Observation Influence Diagnostic of a Data Assimilation System, pages 89–110. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. doi:10.1007/978-3-642-35088-7_4.">Cardinali, 2013</a>, <a class="reference internal" href="../References.html#id142" title="Milan Meloun and Jiří Militký. Statistical Data Analysis: A Practical Guide. Woodhead Publishing India Series. Woodhead Pub. India Pvt Limited, 2011. ISBN 9780857091093. URL: https://books.google.ca/books?id=FgtEYgEACAAJ.">Meloun and Militký, 2011</a>]</span>.</p>
</div>
<p>The Hat matrix is a projection matrix that maps the response variable values <span class="math notranslate nohighlight">\(y\)</span> to the predicted values <span class="math notranslate nohighlight">\(\hat{y}\)</span> obtained from the linear regression model. It is defined as <span id="id11">[<a class="reference internal" href="../References.html#id27" title="Carla Cardinali. Observation Influence Diagnostic of a Data Assimilation System, pages 89–110. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. doi:10.1007/978-3-642-35088-7_4.">Cardinali, 2013</a>, <a class="reference internal" href="../References.html#id142" title="Milan Meloun and Jiří Militký. Statistical Data Analysis: A Practical Guide. Woodhead Publishing India Series. Woodhead Pub. India Pvt Limited, 2011. ISBN 9780857091093. URL: https://books.google.ca/books?id=FgtEYgEACAAJ.">Meloun and Militký, 2011</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-34e0962e-b78b-40b1-b316-bdc739eb71f3">
<span class="eqno">(9.71)<a class="headerlink" href="#equation-34e0962e-b78b-40b1-b316-bdc739eb71f3" title="Permalink to this equation">#</a></span>\[\begin{equation} H = X(X^TX)^{-1}X^T \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the design matrix containing predictor variables (including a constant term for the intercept).</p></li>
<li><p><span class="math notranslate nohighlight">\(X^T\)</span> is the transpose of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X^TX\)</span> is the matrix product of the transpose of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\(X^TX\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Diagonal Elements</strong>:
The diagonal elements of the Hat matrix, denoted as <span class="math notranslate nohighlight">\(h_{ii}\)</span>, represent the leverage of each observation <span class="math notranslate nohighlight">\(i\)</span>. Leverage quantifies how much influence an observation has on the fitted values. Each <span class="math notranslate nohighlight">\(h_{ii}\)</span> value is calculated as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4f7bd692-266d-4d8c-8587-1fc48da6417e">
<span class="eqno">(9.72)<a class="headerlink" href="#equation-4f7bd692-266d-4d8c-8587-1fc48da6417e" title="Permalink to this equation">#</a></span>\[\begin{equation} h_{ii} = (H)_{ii} = x_{i}^T (X^TX)^{-1}x_{i} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th row of the design matrix <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\(X^TX\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>The diagonal elements of the Hat matrix range from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>. They indicate how much an observation’s predictor variables differ from the average values.</p></li>
<li><p>The average of the diagonal elements is  <span id="id12">[<a class="reference internal" href="../References.html#id57" title="Y. Dodge. The Concise Encyclopedia of Statistics. The Concise Encyclopedia of Statistics. Springer New York, 2008. ISBN 9780387317427. URL: https://books.google.ca/books?id=k2zklGOBRDwC.">Dodge, 2008</a>]</span></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum _{i=1}^{n}h_{ii} &amp;= \operatorname {Tr} (\mathbf {H} )=\operatorname {Tr} \left(\mathbf {X} \left(\mathbf {X} ^{\top }\mathbf {X} \right)^{-1}\mathbf {X} ^{\top }\right)=\operatorname {Tr} \left(\mathbf {X} ^{\top }\mathbf {X} \left(\mathbf {X} ^{\top }\mathbf {X} \right)^{-1}\right)=\operatorname {Tr} (\mathbf {I} _{p})=p,\\
    \bar {h} &amp;= \dfrac {1}{n}\sum _{i=1}^{n}h_{ii}=\dfrac {p}{n},
    \end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> refers to the number of predictor variables (also known as features or independent variables) in your regression model and <span class="math notranslate nohighlight">\(n\)</span> refers to the number of observations or data points in your dataset. This average represents the expected leverage of an observation.</p>
</li>
<li><p><strong>Interpretation</strong>:
High leverage points are observations with <span class="math notranslate nohighlight">\(h_{ii}\)</span> values close to 1, indicating that their predictor variable values are far from the mean. These points can significantly affect the fitted values and regression coefficients.</p></li>
<li><p><strong>Influence Measures</strong>:</p>
<ul class="simple">
<li><p>Leverage values help identify high-leverage points that may be potential outliers or influential observations.</p></li>
<li><p>Combined with the residuals, leverage values are used to calculate influence measures like Cook’s distance, which considers both leverage and residual information to assess the impact of an observation on the regression results.</p></li>
</ul>
</li>
<li><p><strong>Outliers and Model Assumptions</strong>:</p>
<ul class="simple">
<li><p>Observations with high leverage values might not necessarily be outliers but could indicate extreme predictor variable values.</p></li>
<li><p>When checking assumptions of linear regression, examining high-leverage points can help detect deviations from linearity, normality, and constant variance assumptions.</p></li>
<li><p>Some statisticians consider <span class="math notranslate nohighlight">\(x_{i}\)</span> as an outlier if <span class="math notranslate nohighlight">\({\displaystyle h_{ii}&gt;2{\dfrac {p}{n}}}\)</span>.</p></li>
<li><p>Some other statisticians consider <span class="math notranslate nohighlight">\(x_{i}\)</span> as an outlier if <span class="math notranslate nohighlight">\({\displaystyle h_{ii}&gt;3{\dfrac {p}{n}}}\)</span> <span id="id13">[<a class="reference internal" href="../References.html#id27" title="Carla Cardinali. Observation Influence Diagnostic of a Data Assimilation System, pages 89–110. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. doi:10.1007/978-3-642-35088-7_4.">Cardinali, 2013</a>]</span>.</p></li>
</ul>
</li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Definition - Cook’s distance</p>
<p>Cook’s distance is a statistical measure that serves as a tool to identify outliers and influential observations in the context of regression analysis. It quantifies how much the fitted values of the response variable change when a specific observation is removed from the dataset. Cook’s distance is particularly helpful for detecting outliers in the predictor variables (X values) and assessing the impact of individual observations on the fitted response values. Observations with Cook’s distance exceeding three times the mean Cook’s distance might indicate potential outliers <span id="id14">[<a class="reference internal" href="../References.html#id27" title="Carla Cardinali. Observation Influence Diagnostic of a Data Assimilation System, pages 89–110. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. doi:10.1007/978-3-642-35088-7_4.">Cardinali, 2013</a>, <a class="reference internal" href="../References.html#id142" title="Milan Meloun and Jiří Militký. Statistical Data Analysis: A Practical Guide. Woodhead Publishing India Series. Woodhead Pub. India Pvt Limited, 2011. ISBN 9780857091093. URL: https://books.google.ca/books?id=FgtEYgEACAAJ.">Meloun and Militký, 2011</a>, <a class="reference internal" href="../References.html#id45" title="MATLAB Developers. Cook’s distance. https://se.mathworks.com/help/stats/cooks-distance.html, 2023. [Online; accessed 01-August-2023].">MATLAB Developers, 2023</a>]</span>.</p>
</div>
<p><strong>Definition</strong></p>
<p>Each element of Cook’s distance (<span class="math notranslate nohighlight">\(D\)</span>) signifies the normalized alteration in the fitted response values due to the exclusion of a particular observation. The Cook’s distance for observation <span class="math notranslate nohighlight">\(i\)</span> is defined as <span id="id15">[<a class="reference internal" href="../References.html#id34" title="R. Dennis Cook. Detection of influential observation in linear regression. Technometrics, 42(1):65–68, 2000. doi:10.1080/00401706.2000.10485981.">Cook, 2000</a>, <a class="reference internal" href="../References.html#id45" title="MATLAB Developers. Cook’s distance. https://se.mathworks.com/help/stats/cooks-distance.html, 2023. [Online; accessed 01-August-2023].">MATLAB Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0ab73f4a-f4d2-4e01-a3ac-b65da33a7f4f">
<span class="eqno">(9.73)<a class="headerlink" href="#equation-0ab73f4a-f4d2-4e01-a3ac-b65da33a7f4f" title="Permalink to this equation">#</a></span>\[\begin{equation} D_i = \frac{1}{p \cdot \text{MSE}} \sum_{j=1}^{n} \left( \hat{y}_j - \hat{y}_{j(i)} \right)^2 \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the total number of observations.</p></li>
<li><p><span class="math notranslate nohighlight">\( p \)</span> is the number of coefficients in the regression model.</p></li>
<li><p><span class="math notranslate nohighlight">\( \text{MSE} \)</span> represents the mean squared error of the regression model.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_j \)</span> is the fitted response value for the <span class="math notranslate nohighlight">\(j\)</span>th observation.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_{j(i)} \)</span> is the fitted response value for the <span class="math notranslate nohighlight">\(j\)</span>th observation when observation <span class="math notranslate nohighlight">\(i\)</span> is omitted.</p></li>
</ul>
<p><strong>Algebraic Equivalent</strong></p>
<p>Cook’s distance can also be expressed algebraically as follows  <span id="id16">[<a class="reference internal" href="../References.html#id34" title="R. Dennis Cook. Detection of influential observation in linear regression. Technometrics, 42(1):65–68, 2000. doi:10.1080/00401706.2000.10485981.">Cook, 2000</a>, <a class="reference internal" href="../References.html#id114" title="M. Kutner, C. Nachtsheim, J. Neter, and W. Li. Applied Linear Statistical Models with Student CD. McGraw-Hill Companies,Incorporated, 2004. ISBN 9780073108742. URL: https://books.google.ca/books?id=0Qq-swEACAAJ.">Kutner <em>et al.</em>, 2004</a>, <a class="reference internal" href="../References.html#id45" title="MATLAB Developers. Cook’s distance. https://se.mathworks.com/help/stats/cooks-distance.html, 2023. [Online; accessed 01-August-2023].">MATLAB Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5879a49-bfef-4f4e-85f4-a83466434422">
<span class="eqno">(9.74)<a class="headerlink" href="#equation-f5879a49-bfef-4f4e-85f4-a83466434422" title="Permalink to this equation">#</a></span>\[\begin{equation} D_i = \frac{r_i^2}{p \cdot \text{MSE}} \cdot \frac{h_{ii}}{(1 - h_{ii})^2} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( r_i \)</span> is the residual for the <span class="math notranslate nohighlight">\(i\)</span>th observation.</p></li>
<li><p><span class="math notranslate nohighlight">\( h_{ii} \)</span> is the leverage value for the <span class="math notranslate nohighlight">\(i\)</span>th observation.</p></li>
</ul>
<p>In both forms, Cook’s distance reflects the combined effect of the squared differences in fitted response values after removing the <span class="math notranslate nohighlight">\(i\)</span>th observation, normalized by the model’s complexity (<span class="math notranslate nohighlight">\(p\)</span>) and the mean squared error. Additionally, the leverage value <span class="math notranslate nohighlight">\(h_{ii}\)</span> accounts for the influence of the <span class="math notranslate nohighlight">\(i\)</span>th observation’s predictor values.</p>
<p>High values of Cook’s distance indicate that the <span class="math notranslate nohighlight">\(i\)</span>-th observation has a substantial impact on the regression results and should be investigated further for potential influence or outliers. The leverage term (<span class="math notranslate nohighlight">\(h_{ii}\)</span>) provides insight into how much the <span class="math notranslate nohighlight">\(i\)</span>-th observation’s predictor values deviate from the average predictor values, contributing to its potential influence.</p>
<p>Interpreting Cook’s distance:</p>
<ul class="simple">
<li><p>A larger Cook’s distance indicates that the corresponding data point has a significant impact on the regression model’s coefficients.</p></li>
<li><p>A small Cook’s distance suggests that the data point has less influence on the model.</p></li>
</ul>
<p>The rule of thumb suggests that if the Cook’s distance for a specific data point exceeds 4/n, that data point is considered influential <span id="id17">[<a class="reference internal" href="../References.html#id176" title="RPubs by RStudio. Cook’s distance. https://rpubs.com/DragonflyStats/Cooks-Distance, 2023. [Online; accessed 01-August-2023].">RPubs by RStudio, 2023</a>]</span>. In general,</p>
<div class="amsmath math notranslate nohighlight" id="equation-3f4c0413-59d7-4429-bff0-785aece509a4">
<span class="eqno">(9.75)<a class="headerlink" href="#equation-3f4c0413-59d7-4429-bff0-785aece509a4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{4}{(n-k-1)}
\end{equation}\]</div>
<p>In other words, if the influence of a data point is relatively large, meaning it significantly affects the regression model’s coefficients and predictions, it might warrant closer examination.</p>
<p>However, it’s important to note that this rule of thumb is just a guideline, and the actual threshold value can vary depending on the context and the specific analysis. Additionally, the decision to consider a data point as influential should be made in conjunction with other diagnostic tools, domain knowledge, and the goals of the analysis.</p>
<p>To compute these values, you typically use the matrix algebra operations available in numerical libraries, as shown in your initial code. Here’s a step-by-step breakdown of the calculation for a single observation <span class="math notranslate nohighlight">\( i \)</span>:</p>
<ol class="arabic simple">
<li><p>Retrieve the <span class="math notranslate nohighlight">\( i \)</span>-th row of the design matrix <span class="math notranslate nohighlight">\( X \)</span>, denoted as <span class="math notranslate nohighlight">\( x_{i} \)</span>.</p></li>
<li><p>Compute the matrix product <span class="math notranslate nohighlight">\( X^TX \)</span>.</p></li>
<li><p>Compute the inverse of <span class="math notranslate nohighlight">\( X^TX \)</span>.</p></li>
<li><p>Calculate the product <span class="math notranslate nohighlight">\( x_{i}^T (X^TX)^{-1}x_{i} \)</span> to obtain the leverage <span class="math notranslate nohighlight">\( h_{ii} \)</span>.</p></li>
</ol>
<p>These leverage values provide information about how influential each observation is on the fitted values and regression coefficients. High leverage points could potentially be outliers or observations with extreme predictor values that have a substantial impact on the model.</p>
</section>
</section>
<section id="example-boston-house-price-data">
<h2><span class="section-number">9.2.3. </span>Example: Boston House-Price Data<a class="headerlink" href="#example-boston-house-price-data" title="Link to this heading">#</a></h2>
<p><font color='Blue'><b>Example</b></font>. To illustrate these concepts, we will employ the Boston Housing dataset. Widely utilized in machine learning and data science, Scikit-learn offers a range of tools for analysis. The Boston Housing dataset is a staple in regression analysis and can be accessed from the link: <a class="reference external" href="http://lib.stat.cmu.edu/datasets/boston">http://lib.stat.cmu.edu/datasets/boston</a>. This dataset contains information about various features related to housing prices in different neighborhoods in Boston.</p>
<p>For our purposes, we will focus exclusively on three variables: <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, and <code class="docutils literal notranslate"><span class="pre">MEDV</span></code>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Variable</p></th>
<th class="head text-center"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>LSTAT</p></td>
<td class="text-center"><p>% lower status of the population</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>MEDV</p></td>
<td class="text-center"><p>Median value of owner-occupied homes in $1000’s</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">_url</span> <span class="o">=</span> <span class="s2">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span>
<span class="n">columns</span> <span class="o">=</span> <span class="mi">12</span> <span class="o">*</span><span class="p">[</span><span class="s1">&#39;_&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">,</span> <span class="s1">&#39;MEDV&#39;</span><span class="p">]</span>

<span class="n">Boston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="o">=</span> <span class="n">_url</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span>
                 <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1">#Flatten all the values into a single long list and remove the nulls</span>
<span class="n">values_w_nulls</span> <span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">all_values</span> <span class="o">=</span> <span class="n">values_w_nulls</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">values_w_nulls</span><span class="p">)]</span>

<span class="c1">#Reshape the values to have 14 columns and make a new df out of them</span>
<span class="n">Boston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">all_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)),</span>
                      <span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span><span class="p">)</span>
<span class="n">Boston</span> <span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;_&#39;</span><span class="p">])</span>
<span class="n">display</span><span class="p">(</span><span class="n">Boston</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 2 columns</p>
</div></div></div>
</div>
<p>To start, we will fit a simple linear regression model using the <code class="docutils literal notranslate"><span class="pre">sm.OLS()</span></code> function from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library. In this model, our response variable will be <code class="docutils literal notranslate"><span class="pre">medv</span></code>, and <code class="docutils literal notranslate"><span class="pre">lstat</span></code> will be the single predictor.</p>
<section id="loading-the-data">
<h3><span class="section-number">9.2.3.1. </span>Loading the Data<a class="headerlink" href="#loading-the-data" title="Link to this heading">#</a></h3>
<p>You have the Boston Housing dataset loaded, which contains variables like <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code> (predictor) and <code class="docutils literal notranslate"><span class="pre">MEDV</span></code> (response).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select predictor and response variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[</span><span class="s1">&#39;MEDV&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-the-design-matrix">
<h3><span class="section-number">9.2.3.2. </span>Creating the Design Matrix<a class="headerlink" href="#creating-the-design-matrix" title="Link to this heading">#</a></h3>
<p>After selecting the <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code> variable as the predictor, you add a constant term to create the design matrix <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-28cf16a2-af2e-4ca2-b548-5e57edc71ec1">
<span class="eqno">(9.76)<a class="headerlink" href="#equation-28cf16a2-af2e-4ca2-b548-5e57edc71ec1" title="Permalink to this equation">#</a></span>\[\begin{equation} X = \begin{bmatrix}
1 &amp; \text{LSTAT}_1 \\
1 &amp; \text{LSTAT}_2 \\
\vdots &amp; \vdots \\
1 &amp; \text{LSTAT}_n \\
\end{bmatrix} \end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\( n \)</span> is the number of observations in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add a column of ones to the predictor matrix for the intercept term</span>
<span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.   4.98]
 [1.   9.14]
 [1.   4.03]
 ...
 [1.   5.64]
 [1.   6.48]
 [1.   7.88]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-the-linear-regression-model">
<h3><span class="section-number">9.2.3.3. </span>Fitting the Linear Regression Model<a class="headerlink" href="#fitting-the-linear-regression-model" title="Link to this heading">#</a></h3>
<p>We fit the simple linear regression model: <span class="math notranslate nohighlight">\( y = \beta_0 + \beta_1 \cdot \text{LSTAT} \)</span>, where <span class="math notranslate nohighlight">\( y \)</span> is the response variable <code class="docutils literal notranslate"><span class="pre">MEDV</span></code> and <span class="math notranslate nohighlight">\(\text{LSTAT}\)</span> is the predictor variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">my_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform linear regression and return model coefficients, predicted values, and residuals.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X (numpy.ndarray): Input feature matrix with shape (n, p).</span>
<span class="sd">    y (numpy.ndarray): Target vector with shape (n,).</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: Intercept of the linear regression model.</span>
<span class="sd">    numpy.ndarray: Array of slope coefficients for each feature.</span>
<span class="sd">    numpy.ndarray: Array of predicted values.</span>
<span class="sd">    numpy.ndarray: Array of residuals.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Calculate the coefficients using the normal equation</span>
    <span class="n">coeff_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Extract coefficients</span>
    <span class="n">intercept</span><span class="p">,</span> <span class="n">slope</span> <span class="o">=</span> <span class="n">coeff_matrix</span>

    <span class="c1"># Calculate the predicted values</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">coeff_matrix</span>

    <span class="c1"># Calculate the residuals</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>

    <span class="k">return</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">residuals</span>

<span class="n">intercept</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">residuals</span> <span class="o">=</span> <span class="n">my_linear_regression</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">sharey</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="c1"># regplot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span> <span class="s1">&#39;SkyBlue&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Data&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;LSTAT&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;MEDV&#39;</span><span class="p">,</span>
              <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Linear Regression&#39;</span><span class="p">)</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="n">slope</span> <span class="o">*</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Line&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># residuals</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;DarkRed&#39;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
              <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Fitted value&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Residual&#39;</span><span class="p">,</span>
              <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Residuals for Linear Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># Print the coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Intercept: </span><span class="si">{</span><span class="n">intercept</span><span class="si">:</span><span class="s1">.8f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Slope: </span><span class="si">{</span><span class="n">slope</span><span class="si">:</span><span class="s1">.8f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept: 34.55384088
Slope: -0.95004935
</pre></div>
</div>
<img alt="../_images/9c7d1b0f2282183ea12716eb77631c4a0772f44d736845e254236ec0e6a738fd.png" src="../_images/9c7d1b0f2282183ea12716eb77631c4a0772f44d736845e254236ec0e6a738fd.png" />
</div>
</div>
<p><strong>Left Plot</strong>: The plot shows that there is a negative linear relationship between LSTAT and MEDV, which means that as the percentage of lower status of the population increases, the median value of owner-occupied homes decreases. The plot also shows that the linear regression model fits the data reasonably well, as most of the points are close to the fitted line. However, there are some outliers and some curvature in the data, which indicate that the linear model may not capture all the features of the data. The plot can help us understand how LSTAT affects MEDV, and how well the linear regression model can predict MEDV based on LSTAT.</p>
<p><strong>Right Plot</strong>: This plot shows the relationship between the fitted values and the residuals of a linear regression model. The fitted values are the predicted values of the response variable based on the predictor variable, and the residuals are the differences between the observed values and the predicted values. A residual plot can help you check the assumptions of a linear regression model, such as linearity, homoskedasticity, and normality of errors.</p>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>A plot of fitted values vs. residuals is a fundamental diagnostic tool in regression analysis. It visually represents the relationship between the predicted values (fitted values) and the corresponding residuals. This plot serves to assess the assumptions and model performance in regression analysis.</p>
<p>In this plot, the x-axis typically displays the fitted values obtained from the regression model, while the y-axis represents the residuals, which are the differences between the observed data points and the corresponding predicted values. By examining this plot, researchers can discern several key aspects <span id="id18">[<a class="reference internal" href="../References.html#id140" title="G. McPherson. Applying and Interpreting Statistics: A Comprehensive Guide. Springer Texts in Statistics. Springer, 2001. ISBN 9780387951102. URL: https://books.google.ca/books?id=FezZhTX9OgQC.">McPherson, 2001</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Linearity</strong>: A random scatter of residuals around the horizontal axis indicates that the assumption of linearity is met. If a pattern is discernible (e.g., a curve or funnel shape), it suggests potential issues with linearity.</p></li>
<li><p><strong>Homoscedasticity</strong>: The spread of residuals should remain roughly constant across the range of fitted values. A fan-shaped or widening pattern could indicate heteroscedasticity, where the variance of errors is not constant.</p></li>
<li><p><strong>Independence of Errors</strong>: Ideally, there should be no discernible pattern or correlation among the residuals in the plot. Any systematic trends or clusters in the residuals may suggest omitted variables or autocorrelation.</p></li>
<li><p><strong>Outliers and Influential Points</strong>: Outliers, which are data points with unusually large residuals, can be identified in this plot. Additionally, influential observations that significantly impact the model can be detected.</p></li>
</ol>
<p>A plot of fitted values vs. residuals aids in assessing the validity of regression assumptions and the overall quality of the model fit. Researchers use this diagnostic tool to identify potential issues, guide model refinement, and ensure that the model’s predictions are reliable.</p>
</div>
</section>
<section id="calculating-hat-matrix-diagonal-values-leverages-optional-content">
<h3><span class="section-number">9.2.3.4. </span>Calculating Hat Matrix Diagonal Values (Leverages) -  (Optional Content)<a class="headerlink" href="#calculating-hat-matrix-diagonal-values-leverages-optional-content" title="Link to this heading">#</a></h3>
<p>The Hat matrix <span class="math notranslate nohighlight">\( H \)</span> is calculated as <span class="math notranslate nohighlight">\( H = X(X^TX)^{-1}X^T \)</span>. However, to compute leverage, you need the diagonal elements of <span class="math notranslate nohighlight">\( H \)</span>, which is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8a8d41bc-cf60-4d4e-a1b5-169e82aa74bd">
<span class="eqno">(9.77)<a class="headerlink" href="#equation-8a8d41bc-cf60-4d4e-a1b5-169e82aa74bd" title="Permalink to this equation">#</a></span>\[\begin{equation} h_{ii} = x_{i}^T (X^TX)^{-1}x_{i} \end{equation}\]</div>
<p>Here’s how you compute <span class="math notranslate nohighlight">\( h_{ii} \)</span> for each observation:</p>
<ul class="simple">
<li><p>For the <span class="math notranslate nohighlight">\( i \)</span>-th observation, you take the <span class="math notranslate nohighlight">\( i \)</span>-th row of the design matrix <span class="math notranslate nohighlight">\( X \)</span>, denoted as <span class="math notranslate nohighlight">\( x_{i} \)</span>.</p></li>
<li><p>You calculate <span class="math notranslate nohighlight">\( (X^TX)^{-1} \)</span> as well.</p></li>
<li><p>Then, you perform the multiplication <span class="math notranslate nohighlight">\( x_{i}^T (X^TX)^{-1}x_{i} \)</span> to get the leverage <span class="math notranslate nohighlight">\( h_{ii} \)</span> for that observation.</p></li>
</ul>
<p>This process is repeated for each observation, resulting in an array of leverage values.</p>
<p>In mathematical terms, for each observation <span class="math notranslate nohighlight">\( i \)</span>, you compute the leverage <span class="math notranslate nohighlight">\( h_{ii} \)</span> using:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2d960680-2c53-44ab-9502-d316c3aa96b2">
<span class="eqno">(9.78)<a class="headerlink" href="#equation-2d960680-2c53-44ab-9502-d316c3aa96b2" title="Permalink to this equation">#</a></span>\[\begin{equation} h_{ii} = x_{i}^T (X^TX)^{-1}x_{i} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( x_{i} \)</span> is the <span class="math notranslate nohighlight">\( i \)</span>-th row of the design matrix <span class="math notranslate nohighlight">\( X \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( (X^TX)^{-1} \)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\( X^TX \)</span>.</p></li>
</ul>
<p>In code, this is implemented as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leverage</span> <span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">XTX_inverse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
    <span class="n">leverage_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">XTX_inverse</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">leverage_values</span>
</pre></div>
</div>
</div>
</div>
<p>Now, the <code class="docutils literal notranslate"><span class="pre">leverage_values</span></code> array contains the leverage values (Hat matrix diagonal values) for each observation. These values indicate how much influence each observation has on the fitted values of the regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">leverage_values</span> <span class="o">=</span> <span class="n">leverage</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">stemlines</span><span class="p">,</span> <span class="n">baseline</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">leverage_values</span><span class="p">,</span>
                                          <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Leverage&#39;</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Leverage vs. Index&#39;</span><span class="p">,</span>
           <span class="n">yscale</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span><span class="p">,</span>  <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Remove x-axis ticks</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$2/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$3/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># Fig setting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f7b0ddae2b45839b06e1c6abfbb0c1873e01094b4edd1e168fa44763e961d88f.png" src="../_images/f7b0ddae2b45839b06e1c6abfbb0c1873e01094b4edd1e168fa44763e961d88f.png" />
</div>
</div>
<p>Let’s break down the interpretation based on some key observations:</p>
<ol class="arabic">
<li><p><strong>Low Leverage Values (Close to 0)</strong>:</p>
<ul class="simple">
<li><p>Observations with low leverage values indicate that their predictor variable values are similar to the mean values of the predictor variables in the dataset.</p></li>
<li><p>These observations are not exerting a strong influence on the model’s predictions.</p></li>
</ul>
</li>
<li><p><strong>Moderate Leverage Values</strong>:</p>
<ul class="simple">
<li><p>Observations with moderate leverage values have some deviation from the mean predictor variable values.</p></li>
<li><p>These observations contribute somewhat to the model’s predictions, but their influence is not extreme.</p></li>
</ul>
</li>
<li><p><strong>High Leverage Values (Close to 1)</strong>:</p>
<ul class="simple">
<li><p>Observations with high leverage values indicate that their predictor variable values are significantly different from the mean.</p></li>
<li><p>These observations have a strong impact on the model’s predictions, and their values could be driving the fitted line of the regression model.</p></li>
</ul>
</li>
<li><p><strong>Identifying High-Leverage Points</strong>:</p>
<ol class="arabic simple">
<li><p><strong><span class="math notranslate nohighlight">\(2p/n\)</span> as a Threshold</strong>:</p>
<ul class="simple">
<li><p>Some statisticians use <span class="math notranslate nohighlight">\(2p/n\)</span> as a guideline to identify observations with high leverage. If the leverage value (<span class="math notranslate nohighlight">\(h_{ii}\)</span>) of an observation exceeds <span class="math notranslate nohighlight">\(2p/n\)</span>, it is considered as potentially influential.</p></li>
<li><p>This guideline suggests that observations with leverage values that are at least twice the average leverage are worth examining closely, as they might have a strong impact on the model’s fit.</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(3p/n\)</span> as a Threshold</strong>:</p>
<ul class="simple">
<li><p>Some other statisticians use an even stricter threshold of <span class="math notranslate nohighlight">\(3p/n\)</span>. If the leverage value (<span class="math notranslate nohighlight">\(h_{ii}\)</span>) of an observation exceeds <span class="math notranslate nohighlight">\(3p/n\)</span>, it is flagged as potentially influential.</p></li>
<li><p>This threshold implies that an observation’s leverage needs to be significantly higher than the average to be considered influential.</p></li>
</ul>
</li>
</ol>
<p>Choosing between these thresholds can depend on the specific context of your analysis, the characteristics of your data, and the goals of your regression modeling. It’s important to note that these guidelines are not strict rules but rather heuristics to help identify observations that might be influential. Further investigation is usually necessary to determine whether these observations are indeed influential or if they should be treated as outliers.</p>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">calculate_cooks_distance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate Cook&#39;s distance for each observation.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X (numpy.ndarray): Input feature matrix with shape (n, p).</span>
<span class="sd">    y (numpy.ndarray): Target vector with shape (n,).</span>

<span class="sd">    Returns:</span>
<span class="sd">    numpy.ndarray: Array containing Cook&#39;s distance values for each observation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Calculate the coefficients using the normal equation</span>
    <span class="n">coeff_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Calculate the predicted values</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">coeff_matrix</span>

    <span class="c1"># Calculate the residuals</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>

    <span class="c1"># Calculate leverage values using the Moore-Penrose pseudo-inverse</span>
    <span class="n">XTX_inverse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">leverage_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">XTX_inverse</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="c1"># Get the shape of the input data</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calculate Mean Squared Error (MSE)</span>
    <span class="n">MSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Cook&#39;s distance calculation</span>
    <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">MSE</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">leverage_values</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">leverage_values</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">D</span>

<span class="c1"># Calculate Cook&#39;s distance</span>
<span class="n">cook_distance</span> <span class="o">=</span> <span class="n">calculate_cooks_distance</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X_with_intercept</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">markerline</span><span class="p">,</span> <span class="n">stemlines</span><span class="p">,</span> <span class="n">baseline</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">cook_distance</span><span class="p">,</span>
                                          <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;OrangeRed&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Cook&#39;s Distance&quot;&quot;&quot;</span><span class="p">,</span>
           <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Cook&#39;s Distance vs. Index&quot;&quot;&quot;</span><span class="p">,</span>
           <span class="n">yscale</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span><span class="p">,</span>  <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Remove x-axis ticks</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">4</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$4/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># Fig setting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/df1fee438c4eba73c1c94dadf2270f6a0d427404d77a6b86d86cff8e565cfa1d.png" src="../_images/df1fee438c4eba73c1c94dadf2270f6a0d427404d77a6b86d86cff8e565cfa1d.png" />
</div>
</div>
</section>
<section id="modeling-through-statsmodels-api">
<h3><span class="section-number">9.2.3.5. </span>Modeling through statsmodels api<a class="headerlink" href="#modeling-through-statsmodels-api" title="Link to this heading">#</a></h3>
<p>Now, let’s summerize everything and do the above through <a class="reference external" href="https://www.statsmodels.org/stable/index.html">statsmodels API</a> <span id="id19">[<a class="reference internal" href="../References.html#id187" title="Skipper Seabold and Josef Perktold. Statsmodels: econometric and statistical modeling with python. In 9th Python in Science Conference. 2010.">Seabold and Perktold, 2010</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Create the model matrix manually</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">]</span>  <span class="c1"># Predictor variable (lstat)</span>
<span class="c1"># Response variable (medv)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[</span><span class="s1">&#39;MEDV&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Add an intercept term</span>
<span class="n">display</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Fit the simple linear regression model</span>
<span class="n">Results</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>LSTAT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>9.14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>4.03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>5.33</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>1.0</td>
      <td>9.67</td>
    </tr>
    <tr>
      <th>502</th>
      <td>1.0</td>
      <td>9.08</td>
    </tr>
    <tr>
      <th>503</th>
      <td>1.0</td>
      <td>5.64</td>
    </tr>
    <tr>
      <th>504</th>
      <td>1.0</td>
      <td>6.48</td>
    </tr>
    <tr>
      <th>505</th>
      <td>1.0</td>
      <td>7.88</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 2 columns</p>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.544
Model:                            OLS   Adj. R-squared:                  0.543
Method:                 Least Squares   F-statistic:                     601.6
Date:                Wed, 08 Nov 2023   Prob (F-statistic):           5.08e-88
Time:                        14:31:08   Log-Likelihood:                -1641.5
No. Observations:                 506   AIC:                             3287.
Df Residuals:                     504   BIC:                             3295.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         34.5538      0.563     61.415      0.000      33.448      35.659
LSTAT         -0.9500      0.039    -24.528      0.000      -1.026      -0.874
==============================================================================
Omnibus:                      137.043   Durbin-Watson:                   0.892
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373
Skew:                           1.453   Prob(JB):                     5.36e-64
Kurtosis:                       5.319   Cond. No.                         29.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The output is from the summary of a fitted linear regression model, which provides valuable information about the model’s coefficients, their standard errors, t-values, p-values, and confidence intervals. Let’s break down each component:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">const</span></code> and <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>: These are the variable names in the model. <code class="docutils literal notranslate"><span class="pre">const</span></code> represents the intercept (constant term), and <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code> represents the predictor variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">coef</span></code>: This column displays the estimated coefficients of the linear regression model. For <code class="docutils literal notranslate"><span class="pre">const</span></code>, the estimated coefficient is 34.5538, and for <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, it is -0.9500.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">err</span></code>: This column represents the standard errors of the coefficient estimates. It quantifies the uncertainty in the estimated coefficients. For <code class="docutils literal notranslate"><span class="pre">const</span></code>, the standard error is 0.563, and for <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, it is 0.039.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">t</span></code>: The t-values are obtained by dividing the coefficient estimates by their standard errors. The t-value measures how many standard errors the coefficient estimate is away from zero. For <code class="docutils literal notranslate"><span class="pre">const</span></code>, the t-value is 61.415, and for <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, it is -24.528.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P&gt;|t|</span></code>: This column provides the p-values associated with the t-values. The p-value represents the probability of observing a t-value as extreme as the one calculated, assuming the null hypothesis that the coefficient is equal to zero. Lower p-values indicate stronger evidence against the null hypothesis. In this case, both coefficients have extremely low p-values (close to 0), indicating that they are statistically significant.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">[0.025</span> <span class="pre">0.975]</span></code>: These are the lower and upper bounds of the 95% confidence intervals for the coefficients. The confidence intervals provide a range of plausible values for the true population coefficients. For <code class="docutils literal notranslate"><span class="pre">const</span></code>, the confidence interval is [33.448, 35.659], and for <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, it is [-1.026, -0.874].</p></li>
</ol>
<p>The coefficient estimates indicate how the response variable (<code class="docutils literal notranslate"><span class="pre">medv</span></code>) is expected to change for a one-unit increase in the predictor variable (<code class="docutils literal notranslate"><span class="pre">lstat</span></code>). The low p-values and confidence intervals not containing zero suggest that both the intercept and <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code> coefficient are statistically significant and have a significant impact on the response variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># OLS Reression results</span>
<span class="k">def</span> <span class="nf">Reg_Result</span><span class="p">(</span><span class="n">Inp</span><span class="p">):</span>
    <span class="n">Temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_html</span><span class="p">(</span><span class="n">Inp</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">as_html</span><span class="p">(),</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Temp</span><span class="o">.</span><span class="n">style</span>\
    <span class="o">.</span><span class="n">format</span><span class="p">({</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;P&gt;|t|&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;std err&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">})</span>\
    <span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Lime&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">set_properties</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;std err&#39;</span><span class="p">],</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;background-color&#39;</span><span class="p">:</span> <span class="s1">&#39;DimGray&#39;</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;White&#39;</span><span class="p">}))</span>

<span class="n">Reg_Result</span><span class="p">(</span><span class="n">Results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_7fa0b_row0_col0 {
  width: 10em;
  background: linear-gradient(90deg, transparent 2.7%, Lime 2.7%, Lime 100.0%, transparent 100.0%);
}
#T_7fa0b_row0_col1, #T_7fa0b_row1_col1 {
  background-color: DimGray;
  color: White;
}
#T_7fa0b_row1_col0 {
  width: 10em;
  background: linear-gradient(90deg, Lime 2.7%, transparent 2.7%);
}
</style>
<table id="T_7fa0b">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_7fa0b_level0_col0" class="col_heading level0 col0" >coef</th>
      <th id="T_7fa0b_level0_col1" class="col_heading level0 col1" >std err</th>
      <th id="T_7fa0b_level0_col2" class="col_heading level0 col2" >t</th>
      <th id="T_7fa0b_level0_col3" class="col_heading level0 col3" >P>|t|</th>
      <th id="T_7fa0b_level0_col4" class="col_heading level0 col4" >[0.025</th>
      <th id="T_7fa0b_level0_col5" class="col_heading level0 col5" >0.975]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_7fa0b_level0_row0" class="row_heading level0 row0" >const</th>
      <td id="T_7fa0b_row0_col0" class="data row0 col0" >3.4554e+01</td>
      <td id="T_7fa0b_row0_col1" class="data row0 col1" >5.6300e-01</td>
      <td id="T_7fa0b_row0_col2" class="data row0 col2" >61.415000</td>
      <td id="T_7fa0b_row0_col3" class="data row0 col3" >0.0000e+00</td>
      <td id="T_7fa0b_row0_col4" class="data row0 col4" >33.448000</td>
      <td id="T_7fa0b_row0_col5" class="data row0 col5" >35.659000</td>
    </tr>
    <tr>
      <th id="T_7fa0b_level0_row1" class="row_heading level0 row1" >LSTAT</th>
      <td id="T_7fa0b_row1_col0" class="data row1 col0" >-9.5000e-01</td>
      <td id="T_7fa0b_row1_col1" class="data row1 col1" >3.9000e-02</td>
      <td id="T_7fa0b_row1_col2" class="data row1 col2" >-24.528000</td>
      <td id="T_7fa0b_row1_col3" class="data row1 col3" >0.0000e+00</td>
      <td id="T_7fa0b_row1_col4" class="data row1 col4" >-1.026000</td>
      <td id="T_7fa0b_row1_col5" class="data row1 col5" >-0.874000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_regplot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    y  = beta_1 *x + beta_0</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">b0</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;const&#39;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">]</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="n">b1</span> <span class="o">*</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="n">sharex</span>  <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># regplot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">LSTAT</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">MEDV</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span> <span class="s1">&#39;SkyBlue&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Data&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;LSTAT&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;MEDV&#39;</span><span class="p">,</span>
              <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Linear Regression&#39;</span><span class="p">)</span>
<span class="n">_regplot</span><span class="p">(</span><span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">results</span> <span class="o">=</span> <span class="n">Results</span><span class="p">,</span>
         <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Line&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># residuals</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span> <span class="n">Results</span><span class="o">.</span><span class="n">resid</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;DarkRed&#39;</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
              <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Fitted value&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Residual&#39;</span><span class="p">,</span>
              <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Residuals for Linear Regression&#39;</span><span class="p">)</span>
<span class="c1"># leverage (Optional Content)</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_gridspec</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">infl</span> <span class="o">=</span> <span class="n">Results</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">stemlines</span><span class="p">,</span> <span class="n">baseline</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">infl</span><span class="o">.</span><span class="n">hat_matrix_diag</span><span class="p">,</span>
                                             <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Leverage&#39;</span><span class="p">,</span>
            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Leverage vs. Index&#39;</span><span class="p">,</span>
            <span class="n">yscale</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span><span class="p">,</span>  <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Remove x-axis ticks</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$2/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$3/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Cook&#39;s Distance (Optional Content)</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">get_gridspec</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">ax4</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">stemlines</span><span class="p">,</span> <span class="n">baseline</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">infl</span><span class="o">.</span><span class="n">cooks_distance</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                             <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;OrangeRed&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Cook&#39;s Distance&quot;&quot;&quot;</span><span class="p">,</span>
            <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Cook&#39;s Distance vs. Index&quot;&quot;&quot;</span><span class="p">,</span>
            <span class="n">yscale</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span><span class="p">,</span>  <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Remove x-axis ticks</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$2/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$3/n$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax4</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># Fig setting</span>

<span class="c1"># Fig setting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0ca1c5c02b9a1b38dcacd8614da943368300352239c24a9549fe7b42809795a2.png" src="../_images/0ca1c5c02b9a1b38dcacd8614da943368300352239c24a9549fe7b42809795a2.png" />
</div>
</div>
</section>
</section>
<section id="linear-regression-through-sklearn-api">
<h2><span class="section-number">9.2.4. </span>Linear Regression through sklearn API<a class="headerlink" href="#linear-regression-through-sklearn-api" title="Link to this heading">#</a></h2>
<p>One can also execute a straightforward linear regression analysis by employing the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> model available in the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library <span id="id20">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>. The following delineates the procedure:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># Prepare the predictor and response variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[[</span><span class="s1">&#39;LSTAT&#39;</span><span class="p">]]</span>  <span class="c1"># Predictor variable (lstat)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">[</span><span class="s1">&#39;MEDV&#39;</span><span class="p">]</span>  <span class="c1"># Response variable (medv)</span>

<span class="c1"># Create and fit the Linear Regression model</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the model coefficients and intercept</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Intercept (const): </span><span class="si">{</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficient (LSTAT): </span><span class="si">{</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intercept (const): 34.55384087938311
Coefficient (LSTAT): -0.9500493537579909
</pre></div>
</div>
</div>
</div>
<p>In this code, we use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> to perform the simple linear regression. We first read the dataset as before, then prepare the predictor variable <code class="docutils literal notranslate"><span class="pre">X</span></code> (lstat) and the response variable <code class="docutils literal notranslate"><span class="pre">y</span></code> (medv). We create an instance of the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> model and fit it to the data using <code class="docutils literal notranslate"><span class="pre">model.fit(X,</span> <span class="pre">y)</span></code>.</p>
<p>After fitting the model, we can access the model’s coefficients using <code class="docutils literal notranslate"><span class="pre">model.coef_</span></code>, which gives us the coefficient for the predictor variable <code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>, and <code class="docutils literal notranslate"><span class="pre">model.intercept_</span></code>, which gives us the intercept (constant term). The coefficients obtained here should match the ones we got from the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> library earlier.</p>
<p><font color='Blue'><b>Example:</b></font>
The dataset used in this example comprises monthly mean temperature data for ‘CALGARY INT’L A’. Our objective is to identify linear trendlines for each of the 12 months, spanning the period from 1881 to 2012.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">Link</span> <span class="o">=</span> <span class="s1">&#39;https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&amp;stationID=2205&amp;Year=2000&amp;Month=1&amp;Day=1&amp;time=&amp;timeframe=3&amp;submit=Download+Data&#39;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">Link</span><span class="p">,</span> <span class="n">usecols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">,</span> <span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Month&#39;</span> <span class="p">,</span> <span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">:</span><span class="s1">&#39;Date&#39;</span><span class="p">})</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Year</th>
      <th>Month</th>
      <th>Mean Temp (°C)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1881-01-01</td>
      <td>1881</td>
      <td>1</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1881-02-01</td>
      <td>1881</td>
      <td>2</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1881-03-01</td>
      <td>1881</td>
      <td>3</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1881-04-01</td>
      <td>1881</td>
      <td>4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1881-05-01</td>
      <td>1881</td>
      <td>5</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1574</th>
      <td>2012-03-01</td>
      <td>2012</td>
      <td>3</td>
      <td>0.7</td>
    </tr>
    <tr>
      <th>1575</th>
      <td>2012-04-01</td>
      <td>2012</td>
      <td>4</td>
      <td>5.2</td>
    </tr>
    <tr>
      <th>1576</th>
      <td>2012-05-01</td>
      <td>2012</td>
      <td>5</td>
      <td>9.6</td>
    </tr>
    <tr>
      <th>1577</th>
      <td>2012-06-01</td>
      <td>2012</td>
      <td>6</td>
      <td>13.7</td>
    </tr>
    <tr>
      <th>1578</th>
      <td>2012-07-01</td>
      <td>2012</td>
      <td>7</td>
      <td>18.3</td>
    </tr>
  </tbody>
</table>
<p>1579 rows × 4 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">calendar</span>

<span class="k">def</span> <span class="nf">_regplot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">df_sub</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Plots the regression line on the given axis.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - ax: matplotlib axis</span>
<span class="sd">        The axis to plot on.</span>
<span class="sd">    - df_sub: pandas DataFrame</span>
<span class="sd">        Subset of data for a specific month.</span>
<span class="sd">    - X: numpy array</span>
<span class="sd">        Independent variable (index values).</span>
<span class="sd">    - reg: regression model</span>
<span class="sd">        The regression model used for prediction.</span>
<span class="sd">    - *args, **kwargs: additional plot arguments</span>
<span class="sd">        Additional arguments for the plot function.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_sub</span><span class="o">.</span><span class="n">Date</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Slope = </span><span class="si">{</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.3</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Create and fit the Linear Regression model</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="c1"># Subset data for a specific month</span>
    <span class="n">df_sub</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Month</span> <span class="o">==</span> <span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Scatter plot for Mean Max Temp</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">],</span> <span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">],</span>
               <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;Blue&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">)</span>

    <span class="c1"># Set axis label as the month name</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">calendar</span><span class="o">.</span><span class="n">month_name</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Remove rows with missing values</span>
    <span class="n">df_sub</span> <span class="o">=</span> <span class="n">df_sub</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

    <span class="c1"># Prepare data for linear regression</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df_sub</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create and fit a Linear Regression model</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Plot the fitted line</span>
    <span class="n">_regplot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">df_sub</span><span class="o">=</span><span class="n">df_sub</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Line&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Set four y-ticks</span>
    <span class="n">yticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">yticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">yticks</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Monthly Mean Temperature (°C) Trends at CALGARY INT&#39;L A: Linear Analysis&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/08d3d2a8d1872d9fb8b6de48beb7c792355bd1ef1c895d329a7b98d057a70e73.png" src="../_images/08d3d2a8d1872d9fb8b6de48beb7c792355bd1ef1c895d329a7b98d057a70e73.png" />
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Please note that all the slopes shown in the figure above have been provided with three significant decimal places.</p>
</div>
</section>
<section id="analyzing-long-term-trends">
<h2><span class="section-number">9.2.5. </span>Analyzing Long-Term Trends<a class="headerlink" href="#analyzing-long-term-trends" title="Link to this heading">#</a></h2>
<p>When analyzing trends in data, it’s important to consider the significance of the observed trendlines, and p-values play a crucial role in this context. A p-value is a statistical measure that helps determine the significance of a trend or relationship observed in a dataset. In the context of the provided trendlines for mean monthly temperatures at Calgary International Airport, it would be advisable to conduct a statistical test to evaluate the significance of these trends.</p>
<p>To assess the significance of these trendlines and whether they are statistically meaningful, you would typically perform a linear regression analysis and calculate p-values for each trendline. Here’s a brief note on the significance of these trendlines and p-values:</p>
<p><strong>Note on P-Values and Significance of Trendlines:</strong></p>
<p>In the analysis of the mean monthly temperature trends at Calgary International Airport, we have calculated the slopes of linear trendlines to understand how temperatures are changing over time. However, to determine whether these observed trends are statistically significant, we need to consider p-values.</p>
<ul class="simple">
<li><p><strong>P-Values</strong>: A p-value measures the probability of obtaining the observed trendlines (or more extreme ones) if there were no actual trends in the data. Lower p-values indicate stronger evidence against the null hypothesis, suggesting that the observed trend is not due to random chance.</p></li>
<li><p><strong>Significance</strong>: To assess the significance of these trendlines, we typically perform linear regression analysis. In this analysis, each month’s temperature data is regressed against time (the independent variable). The p-value associated with each trendline quantifies the likelihood of observing the reported trend under the assumption that there is no true linear relationship between time and temperature.</p></li>
<li><p><strong>Interpretation</strong>: A low p-value (typically, below a significance level like 0.05) suggests that the trend is statistically significant. In other words, it provides evidence that the temperature changes observed for that month are not just random fluctuations.</p></li>
<li><p><strong>Caution</strong>: It’s important to be cautious when interpreting p-values. While a low p-value indicates statistical significance, it does not by itself establish the practical or real-world significance of the trend. Additionally, trends can be affected by many factors, and further analysis is needed to understand the underlying causes of temperature changes.</p></li>
</ul>
<p>Several Python packages can calculate p-values for linear regression. Here are some commonly used ones:</p>
<ul class="simple">
<li><p>StatsModels</p></li>
<li><p>SciPy</p></li>
<li><p>PyStatsModels</p></li>
<li><p>etc.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">calendar</span>

<span class="n">slope_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">month_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">p_val_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">13</span><span class="p">):</span>
    <span class="n">df_sub</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Month</span> <span class="o">==</span> <span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df_sub</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df_sub</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">slope_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slope</span><span class="p">)</span>
    <span class="n">month_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calendar</span><span class="o">.</span><span class="n">month_name</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>
    <span class="n">p_val_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">df_sub</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">p_value</span>

<span class="n">df_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Slope&#39;</span><span class="p">:</span><span class="n">slope_list</span><span class="p">,</span> <span class="s1">&#39;p-value&#39;</span><span class="p">:</span><span class="n">p_val_list</span><span class="p">},</span> <span class="n">index</span> <span class="o">=</span> <span class="n">month_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following table  represents the slopes of linear trendlines for the mean monthly temperature at Calgary International Airport weather stations from January 1881 to July 2012. Additionally, p-values are provided to assess the significance of these trends.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Slope</th>
      <th>p-value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>January</th>
      <td>0.028849</td>
      <td>0.019986</td>
    </tr>
    <tr>
      <th>February</th>
      <td>0.036869</td>
      <td>0.000755</td>
    </tr>
    <tr>
      <th>March</th>
      <td>0.018809</td>
      <td>0.025253</td>
    </tr>
    <tr>
      <th>April</th>
      <td>-0.003040</td>
      <td>0.601981</td>
    </tr>
    <tr>
      <th>May</th>
      <td>-0.000235</td>
      <td>0.948623</td>
    </tr>
    <tr>
      <th>June</th>
      <td>0.002036</td>
      <td>0.517881</td>
    </tr>
    <tr>
      <th>July</th>
      <td>0.004418</td>
      <td>0.142400</td>
    </tr>
    <tr>
      <th>August</th>
      <td>0.005177</td>
      <td>0.151559</td>
    </tr>
    <tr>
      <th>September</th>
      <td>0.008547</td>
      <td>0.067413</td>
    </tr>
    <tr>
      <th>October</th>
      <td>-0.001246</td>
      <td>0.796791</td>
    </tr>
    <tr>
      <th>November</th>
      <td>0.003791</td>
      <td>0.684526</td>
    </tr>
    <tr>
      <th>December</th>
      <td>0.000463</td>
      <td>0.962145</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Each slope corresponds to a specific month and represents the rate of change in temperature over time, expressed in degrees Celsius per year. Let’s break down the information, explain the slope, and discuss the significance at different confidence levels.</p>
<ol class="arabic simple">
<li><p><strong>January (0.028849):</strong> The positive slope indicates an increasing trend in mean temperatures in January over the years. At a 90% confidence level, this trend is significant, as the p-value (0.019986) is less than 0.1.</p></li>
<li><p><strong>February (0.036869):</strong> This positive slope suggests a significant increase in mean temperatures in February. The very low p-value (0.000755) indicates significance even at a 99% confidence level.</p></li>
<li><p><strong>March (0.018809):</strong> The positive slope implies a gradual rise in March temperatures. At a 90% confidence level, this trend is significant (p-value: 0.025253).</p></li>
<li><p><strong>April (-0.003040):</strong> The negative slope suggests a slight decrease in mean temperatures in April, although it is not statistically significant at any of the specified confidence levels. The p-value is relatively high (0.601981).</p></li>
<li><p><strong>May (-0.000235):</strong> Similarly, the small negative slope in May temperatures is not statistically significant at any confidence level (p-value: 0.948623).</p></li>
<li><p><strong>June (0.002036):</strong> The positive slope indicates a minor increase in June temperatures, but it’s not statistically significant at any confidence level (p-value: 0.517881).</p></li>
<li><p><strong>July (0.004418):</strong> This positive slope suggests a small but significant increase in July temperatures at a 90% confidence level, as the p-value is 0.142400.</p></li>
<li><p><strong>August (0.005177):</strong> Like July, August also shows a slight but statistically significant increase in temperatures at a 90% confidence level (p-value: 0.151559).</p></li>
<li><p><strong>September (0.008547):</strong> The positive slope for September indicates a more significant increase in temperatures. This trend is significant at a 90% confidence level (p-value: 0.067413).</p></li>
<li><p><strong>October (-0.001246):</strong> The small negative slope in October temperatures is not statistically significant at any of the confidence levels (p-value: 0.796791).</p></li>
<li><p><strong>November (0.003791):</strong> This positive slope suggests a slight increase in November temperatures, but it is not statistically significant at any confidence level (p-value: 0.684526).</p></li>
<li><p><strong>December (0.000463):</strong> The positive slope for December indicates a very slight increase in temperatures, but it’s not statistically significant at any confidence level (p-value: 0.962145).</p></li>
</ol>
<p>These slopes provide insights into the long-term temperature trends for each month at the Calgary International Airport weather stations. Positive slopes indicate warming trends, negative slopes indicate cooling trends, and the magnitude of the slope reflects the rate of change.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C09S01.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9.1. </span>Prologue: Statistical Metrics and Evaluation</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C09S03.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.3. </span>Multiple Linear Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-beta-coefficients-in-linear-regression-using-least-squares">9.2.1. Estimating Beta Coefficients in Linear Regression using Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-modeling-a-single-variable-relationship">9.2.2. Simple Linear Regression: Modeling a Single Variable Relationship</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-intercept-and-the-slope">9.2.2.1. Finding  the intercept and the slope</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-intercept-and-the-slope-vector-format">9.2.2.2. Finding  the intercept and the slope (Vector Format)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hat-matrix-optional-content">9.2.2.3. The Hat matrix (Optional Content)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-boston-house-price-data">9.2.3. Example: Boston House-Price Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data">9.2.3.1. Loading the Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-design-matrix">9.2.3.2. Creating the Design Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-linear-regression-model">9.2.3.3. Fitting the Linear Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-hat-matrix-diagonal-values-leverages-optional-content">9.2.3.4. Calculating Hat Matrix Diagonal Values (Leverages) -  (Optional Content)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-through-statsmodels-api">9.2.3.5. Modeling through statsmodels api</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-through-sklearn-api">9.2.4. Linear Regression through sklearn API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-long-term-trends">9.2.5. Analyzing Long-Term Trends</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>