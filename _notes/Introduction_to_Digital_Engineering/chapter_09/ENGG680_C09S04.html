
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9.4. Logistic Regression &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_09/ENGG680_C09S04';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.5. K-Nearest Neighbors (K-NN)" href="ENGG680_C09S05.html" />
    <link rel="prev" title="9.3. Multiple Linear Regression" href="ENGG680_C09S03.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C09.html">9. An Introduction to Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S02.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ENGG680_C10.html">10. Tree-Based Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S02.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S03.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ENGG680_C12.html">12. Introduction to Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-linear-regression">9.4.1. Why Not Linear Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-predicting-a-binary-response-variable">9.4.2. Logistic Regression: Predicting a Binary Response Variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-model">9.4.2.1. Logistic Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-probabilities">9.4.2.2. Estimating Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-logistic-function">9.4.2.3. Sigmoid (Logistic) Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-coefficients">9.4.2.4. Finding the Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translating-theory-into-python">9.4.2.5. Translating Theory into Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-logistic-regression">9.4.3. Simple Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-logistic-regression">9.4.4. Multiple Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confounding-optional-content">9.4.5. Confounding (Optional Content)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">9.4.6. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classifier">9.4.7. Multiclass Classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-approach">9.4.7.1. One-vs.-Rest Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">9.4.7.2. Multinomial Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-logistic-regression">9.4.7.3. Sklearn Logistic Regression</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="logistic-regression">
<h1><span class="section-number">9.4. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h1>
<section id="why-not-linear-regression">
<h2><span class="section-number">9.4.1. </span>Why Not Linear Regression?<a class="headerlink" href="#why-not-linear-regression" title="Link to this heading">#</a></h2>
<p>Linear regression is a fundamental statistical technique that is widely used for modeling the relationship between a dependent variable and one or more independent variables. It has its strengths and weaknesses, and there are situations where it may not be suitable <span id="id1">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Nonlinear Relationships:</strong> Linear regression assumes a linear relationship between the independent and dependent variables. If the true relationship is nonlinear, using linear regression can lead to poor model fit and inaccurate predictions.</p></li>
<li><p><strong>Complex Interactions:</strong> Linear regression is limited in handling complex interactions between variables. If the relationship involves nonlinear effects, or higher-order terms, linear regression may not capture these nuances effectively.</p></li>
<li><p><strong>Assumption Violations:</strong> Linear regression assumes that the residuals (the differences between observed and predicted values) are normally distributed and have constant variance. If these assumptions are violated, the results and inferences from linear regression may be unreliable.</p></li>
<li><p><strong>Outliers and Influential Points:</strong> Linear regression is sensitive to outliers and influential data points. A single outlier can affect the regression line and the estimated coefficients.</p></li>
<li><p><strong>Categorical and Binary Variables:</strong> While linear regression can handle continuous and numeric independent variables, it struggles with categorical or binary predictors without appropriate encoding or treatment.</p></li>
<li><p><strong>Multi-Collinearity:</strong> When the independent variables in a linear regression model are highly correlated with each other, it can lead to multicollinearity issues. This makes it challenging to interpret the individual contributions of these variables.</p></li>
<li><p><strong>Limited in Handling Non-Parametric Data:</strong> Linear regression is a parametric method, meaning it makes assumptions about the underlying data distribution. If the data is non-parametric or has heavy-tailed distributions, linear regression may not be appropriate.</p></li>
</ol>
<p>In cases where these limitations are significant, other techniques such as nonlinear regression, generalized linear models, decision trees, support vector machines, neural networks, or other advanced statistical and machine learning methods may be more suitable. The choice of the appropriate modeling technique depends on the data.</p>
</section>
<section id="logistic-regression-predicting-a-binary-response-variable">
<h2><span class="section-number">9.4.2. </span>Logistic Regression: Predicting a Binary Response Variable<a class="headerlink" href="#logistic-regression-predicting-a-binary-response-variable" title="Link to this heading">#</a></h2>
<p>Logistic regression is a powerful statistical method tailored specifically for binary classification tasks. Its primary objective is to predict a binary response variable, such as yes/no or 0/1, by leveraging the impact of one or more predictor variables. This approach is effectively represented by the logistic regression model <span id="id2">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<section id="logistic-model">
<h3><span class="section-number">9.4.2.1. </span>Logistic Model<a class="headerlink" href="#logistic-model" title="Link to this heading">#</a></h3>
<p>At the core of logistic regression is its ingenious model, which seamlessly integrates predictor variables within a linear framework, further molded by the logistic function. This fusion aims to predict the probability of the positive class (class 1) in a binary response scenario. The model is encapsulated by the following equation <span id="id3">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ee2d5749-3172-45ab-8044-fe542ed4c0e4">
<span class="eqno">(9.79)<a class="headerlink" href="#equation-ee2d5749-3172-45ab-8044-fe542ed4c0e4" title="Permalink to this equation">#</a></span>\[\begin{align}
\ln\left(\frac{p(X)}{1 - p(X)}\right) = \log_e\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p,
\end{align}\]</div>
<p>In this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X = (X_1, ..., X_p)\)</span> constitutes the vector of predictor variables (features).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0, \beta_1, ..., \beta_p\)</span> take on the roles of coefficients, encompassing both the intercept and each predictor variable’s contribution.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(X)\)</span> designates the probability of the positive class, informed by the predictor variables.</p></li>
</ul>
<p>Notably, the term <span class="math notranslate nohighlight">\(\log[p(X)/[1 - p(X)]]\)</span>, dubbed <strong>the log-odds</strong>, encapsulates the relationship between the probability of positive and negative outcomes <span id="id4">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>. The odds ratio, which is the exponentiated value of the log-odds, represents the ratio of the odds of the positive class to the odds of the negative class.</p>
</section>
<section id="estimating-probabilities">
<h3><span class="section-number">9.4.2.2. </span>Estimating Probabilities<a class="headerlink" href="#estimating-probabilities" title="Link to this heading">#</a></h3>
<p>The logistic model allows us to deduce the estimated probability of the positive class given the predictor variables (<span class="math notranslate nohighlight">\(p(X)\)</span>) through the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0562b62f-d8e8-4efd-a696-2f8e39fbcdbe">
<span class="eqno">(9.80)<a class="headerlink" href="#equation-0562b62f-d8e8-4efd-a696-2f8e39fbcdbe" title="Permalink to this equation">#</a></span>\[\begin{align}
p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}},
\end{align}\]</div>
<p>In this equation, the exponential function (approximately 2.71828) plays a pivotal role in the transformation. This transformation, facilitated by the logistic function, also known as the sigmoid function, molds the linear combination of predictor variables into a probability value that spans the entire spectrum between 0 and 1.</p>
</section>
<section id="sigmoid-logistic-function">
<h3><span class="section-number">9.4.2.3. </span>Sigmoid (Logistic) Function<a class="headerlink" href="#sigmoid-logistic-function" title="Link to this heading">#</a></h3>
<p>The equation given below:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a5b6aacd-c642-477b-8c70-fb188ebccc20">
<span class="eqno">(9.81)<a class="headerlink" href="#equation-a5b6aacd-c642-477b-8c70-fb188ebccc20" title="Permalink to this equation">#</a></span>\[\begin{equation}
\ln\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
\end{equation}\]</div>
<p>is closely connected to the sigmoid (logistic) function within the context of logistic regression, a method used for binary classification.</p>
<p>In logistic regression, the objective is to estimate the likelihood of the positive outcome (class 1) based on the values of certain predictor variables, such as <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span>. The left side of the equation, <span class="math notranslate nohighlight">\(\ln\left(\frac{p(X)}{1 - p(X)}\right)\)</span>, signifies the log-odds (logit) of the probability of the positive outcome given the predictor variables. This transformation linearizes the connection between the predictor variables and the probability.</p>
<p>Now, let’s introduce the sigmoid function. The <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>, denoted as <span class="math notranslate nohighlight">\(f(x) = \dfrac{1}{1 + e^{-x}}\)</span>, converts a real-number <span class="math notranslate nohighlight">\(x\)</span> into a probability ranging from 0 to 1. In the logistic regression model, this can be related to <span class="math notranslate nohighlight">\(p(X)\)</span> as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ae9d8d88-48f3-4f75-9659-47795a1d6622">
<span class="eqno">(9.82)<a class="headerlink" href="#equation-ae9d8d88-48f3-4f75-9659-47795a1d6622" title="Permalink to this equation">#</a></span>\[\begin{equation}p(X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)}}\end{equation}\]</div>
<p>In this expression, <span class="math notranslate nohighlight">\(p(X)\)</span> represents the probability of the positive outcome. The right-hand side is a linear combination of predictor variables, each multiplied by coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>. The sigmoid function, also known as the logistic function, is applied to this combination, ensuring that the result, <span class="math notranslate nohighlight">\(p(X)\)</span>, remains within the range of 0 to 1.</p>
<p>In essence, the logistic regression model incorporates the sigmoid function to transform the linear combination of predictor variables into a valid probability value, enabling binary classification decisions based on this probability threshold.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">scipy.special.expit</span></code> is a function within the SciPy library, a widely recognized open-source resource for scientific and mathematical computations. This function, also known as the “exponential of the logistic function,” serves the purpose of executing a logistic sigmoid transformation. It operates on either a single value or an array as input and employs the logistic sigmoid function, mathematically expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a5e3662d-4d32-4e4d-accf-3a075a45e6b2">
<span class="eqno">(9.83)<a class="headerlink" href="#equation-a5e3662d-4d32-4e4d-accf-3a075a45e6b2" title="Permalink to this equation">#</a></span>\[\begin{equation}f(x) = \frac{1}{1 + e^{-x}}\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(e\)</span> signifies the natural logarithm base, while <span class="math notranslate nohighlight">\(x\)</span> denotes the input variable. The fundamental role of the logistic sigmoid function is to transform input values into a bounded range of 0 to 1. This property finds extensive application in various domains, including machine learning, specifically in logistic regression, where it is instrumental in modeling the probability of binary outcomes.</p>
</div>
<p><font color='Blue'><b>Example:</b></font>
Let’s examine a straightforward scenario with a single variable, denoted as X, and a dependent variable, y. In this instance, we generate X values from a normal distribution with a mean (<span class="math notranslate nohighlight">\(\mu\)</span>) of 10 and a standard deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>) of 2. The variable y is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d640fb7-2791-44cd-a16a-5f6bce6bebe7">
<span class="eqno">(9.84)<a class="headerlink" href="#equation-8d640fb7-2791-44cd-a16a-5f6bce6bebe7" title="Permalink to this equation">#</a></span>\[\begin{equation}
y = \begin{cases}
1, &amp; \text{if } x &gt; \mu \\
0, &amp; \text{otherwise}.
\end{cases}
\end{equation}\]</div>
<p>Next, we estimate the probability of the outcome using linear regression, which is depicted in the top plot. It’s worth noting that some of the estimated probabilities turn out to be negative, which is problematic because probabilities should always fall within the range of 0 to 1.</p>
<p>To address this issue, we employ logistic regression, as shown in the bottom plot. In logistic regression, all estimated probabilities strictly conform to the valid probability range, ensuring that they stay between 0 and 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="c1"># from scipy.special import expit</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>

<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span>  <span class="c1"># mean and standard deviation</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">300</span><span class="p">)</span>

<span class="c1"># Create the models</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">()]</span>

<span class="c1"># Titles for the plots</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span> <span class="s2">&quot;Linear Regression Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Logistic Regression Model&quot;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;hspace&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Px</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">X_gen</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="c1"># Or we could utilize scipy.special.expit</span>
        <span class="c1"># Px = expit(X_gen * model.coef_ + model.intercept_).ravel()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Px</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">*</span> <span class="n">X_gen</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#b496ff&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;#602ce5&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Sample Data&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_gen</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#C60004&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;#26912d&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Probability(X)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c9597b87238b84bd791e87563687e107b6048dc481db990cfd2870bad443a2ad.png" src="../_images/c9597b87238b84bd791e87563687e107b6048dc481db990cfd2870bad443a2ad.png" />
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Logistic regression is primarily used for classification, not regression, despite its name. It’s a statistical model used to predict the probability of a binary outcome, typically denoted as class 0 and class 1. The logistic regression model estimates the probability that a given input belongs to one of these two classes. This makes it a valuable tool for various classification tasks, such as spam detection, disease diagnosis, or sentiment analysis. In logistic regression, the output is a probability score, and a threshold (often 0.5) is applied to classify the input into one of the two classes. Therefore, it’s fundamentally a classification technique, even though the term “regression” is in its name.</p>
</div>
</section>
<section id="finding-the-coefficients">
<h3><span class="section-number">9.4.2.4. </span>Finding the Coefficients<a class="headerlink" href="#finding-the-coefficients" title="Link to this heading">#</a></h3>
<p>In logistic regression, the goal is to find the optimal coefficients (<span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, …, <span class="math notranslate nohighlight">\(\beta_p\)</span>) of the model that best fits the data. This is achieved through the Maximum Likelihood Estimation (MLE) method. MLE aims to determine the coefficient values that maximize the likelihood of observing the actual binary outcomes in the dataset, given the predictor variables:</p>
<ol class="arabic simple">
<li><p><strong>Formulate the Likelihood Function</strong>: Begin by defining the likelihood function, denoted as <span class="math notranslate nohighlight">\(L(\beta_0, \beta_1, ..., \beta_p)\)</span>, which represents the probability of observing the binary outcomes (0s and 1s) based on the logistic regression model. This function is a mathematical representation of the relationship between the coefficients and the observed data.</p></li>
<li><p><strong>Take the Natural Logarithm</strong>: Simplify the likelihood calculations by working with the natural logarithm of the likelihood function, denoted as <span class="math notranslate nohighlight">\(\ell(\beta_0, \beta_1, ..., \beta_p)\)</span>. This transformation preserves the underlying structure while making computations more manageable.</p></li>
<li><p><strong>Maximize the Log-Likelihood</strong>: Utilize optimization techniques, often numerical methods, to find the coefficient values that maximize the log-likelihood function <span class="math notranslate nohighlight">\(\ell(\beta_0, \beta_1, ..., \beta_p)\)</span>. The goal is to identify the coefficient values that make the observed data most likely under the logistic regression model.</p></li>
<li><p><strong>Interpret the Coefficients</strong>: Once the estimated coefficients <span class="math notranslate nohighlight">\(\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_p\)</span> are obtained, interpret their values. Each coefficient (excluding <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, the intercept) represents the change in the log-odds (logit) of the positive outcome (class 1) associated with a one-unit change in the corresponding predictor variable while holding other variables constant.</p></li>
</ol>
<p>Practical implementations of logistic regression, especially with multiple predictor variables (<span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, …, <span class="math notranslate nohighlight">\(X_p\)</span>) and non-linear optimization, can be complex. The choice of optimization method may depend on the software libraries or statistical packages you are using.</p>
<p>Most modern statistical software packages for data analysis, such as Python’s scikit-learn or R’s glm function, provide built-in functions to perform logistic regression. These functions automatically handle the MLE process and provide the estimated coefficients. However, if manual implementation of logistic regression is necessary, you’ll likely rely on optimization algorithms like gradient descent or more advanced techniques. Proficiency in numerical optimization and statistical concepts is crucial for successfully implementing this process from scratch.</p>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a statistical model. The goal of MLE is to find the parameter values that make the observed data most probable under the assumed probabilistic model. This technique is widely used in various fields, including statistics, econometrics, machine learning, and many other areas where data analysis and modeling are essential.</p>
<ol class="arabic simple">
<li><p><strong>Likelihood Function</strong>: The likelihood function, denoted as <span class="math notranslate nohighlight">\(L(\theta)\)</span>, is a fundamental concept in MLE. It represents the probability of observing the given data points, assuming a specific parametric model. For a continuous distribution, the likelihood is a function of the parameters of that distribution.</p></li>
<li><p><strong>Parameter Estimation</strong>: In many statistical models, there are parameters (denoted as <span class="math notranslate nohighlight">\(\theta\)</span>) that need to be estimated from the data. The likelihood function is a function of these parameters. The MLE method seeks the values of these parameters that maximize the likelihood function.</p></li>
<li><p><strong>Log-Likelihood</strong>: To simplify calculations and numerical stability, it’s common to work with the natural logarithm of the likelihood function, denoted as <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>. Maximizing the log-likelihood is equivalent to maximizing the likelihood itself, as the logarithm is a monotonically increasing function.</p></li>
<li><p><strong>Optimization</strong>: Finding the values of the parameters that maximize the log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> is an optimization problem. Various optimization algorithms, such as gradient descent, Newton-Raphson, or more advanced techniques, can be employed to search for the optimal parameter values.</p></li>
<li><p><strong>Statistical Inference</strong>: Once the MLE estimates of the parameters, denoted as <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, are obtained, they can be used for statistical inference. This includes making predictions, calculating confidence intervals, performing hypothesis tests, and evaluating the goodness of fit of the model to the data.</p></li>
</ol>
<p>MLE is a powerful and widely used method because it provides estimates that are asymptotically unbiased and efficient under certain conditions. However, it’s essential to ensure that the chosen model is appropriate for the data, as MLE can be sensitive to model misspecification. Additionally, for complex models, numerical optimization may be computationally intensive, and care must be taken to handle issues like local optima.</p>
</div>
</section>
<section id="translating-theory-into-python">
<h3><span class="section-number">9.4.2.5. </span>Translating Theory into Python<a class="headerlink" href="#translating-theory-into-python" title="Link to this heading">#</a></h3>
<p>In the world of Python, the practical implementation of logistic regression is greatly simplified by leveraging powerful libraries like scikit-learn. This Python library provides a comprehensive toolkit for machine learning, and the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class from scikit-learn is an invaluable tool for binary classification tasks. By utilizing this class, you can effortlessly apply logistic regression to your dataset, allowing you to fit the model and gain access to the estimated coefficients.</p>
<p>For those eager to explore the depths of logistic regression within the scikit-learn ecosystem, the official scikit-learn documentation for the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class serves as a valuable resource. It acts as a gateway to the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class, providing detailed insights and instructions on how to utilize it effectively: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit-learn Logistic Regression Documentation</a>. This documentation is an essential reference for both beginners and experienced practitioners, offering guidance on various parameters, techniques, and best practices for applying logistic regression in diverse real-world scenarios.</p>
<p><font color='Blue'><b>Example:</b></font> In the next example, we turn our attention to the utilization of the Default dataset from the ISLR repository <span id="id5">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>. This dataset, available at (<a class="reference external" href="https://www.statlearning.com/resources-python">https://www.statlearning.com/resources-python</a>). This dataset encapsulates a comprehensive record of instances where customers defaulted on their credit obligations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="n">Default</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Default.csv&#39;</span><span class="p">)</span>
<span class="n">Default</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">title</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Default</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">Default</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Default</th>
      <th>Student</th>
      <th>Balance</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>No</td>
      <td>No</td>
      <td>729.526495</td>
      <td>44361.625074</td>
    </tr>
    <tr>
      <th>1</th>
      <td>No</td>
      <td>Yes</td>
      <td>817.180407</td>
      <td>12106.134700</td>
    </tr>
    <tr>
      <th>2</th>
      <td>No</td>
      <td>No</td>
      <td>1073.549164</td>
      <td>31767.138947</td>
    </tr>
    <tr>
      <th>3</th>
      <td>No</td>
      <td>No</td>
      <td>529.250605</td>
      <td>35704.493935</td>
    </tr>
    <tr>
      <th>4</th>
      <td>No</td>
      <td>No</td>
      <td>785.655883</td>
      <td>38463.495879</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>In the context of credit cards, “default” refers to the failure of a cardholder to meet the agreed-upon terms and obligations associated with the credit card account. This typically includes missing minimum monthly payments, exceeding the credit limit, or violating other terms specified in the credit card agreement. When a cardholder defaults, it indicates a breach of the contract between the cardholder and the credit card issuer.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">gridspec_kw</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;width_ratios&#39;</span><span class="p">:[</span><span class="mf">.6</span><span class="p">,</span><span class="mf">.2</span><span class="p">,</span><span class="mf">.2</span><span class="p">]})</span>
<span class="n">CP</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Yes&#39;</span><span class="p">:</span> <span class="s1">&#39;MediumSeaGreen&#39;</span><span class="p">,</span> <span class="s1">&#39;No&#39;</span><span class="p">:</span> <span class="s1">&#39;OrangeRed&#39;</span><span class="p">}</span>
<span class="c1"># Left</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Default</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Default</span><span class="o">.</span><span class="n">Default</span> <span class="o">==</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;Balance&#39;</span><span class="p">],</span>
              <span class="n">Default</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Default</span><span class="o">.</span><span class="n">Default</span> <span class="o">==</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;Income&#39;</span><span class="p">],</span> 
              <span class="n">edgecolors</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Yes&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Default</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Default</span><span class="o">.</span><span class="n">Default</span> <span class="o">==</span> <span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="s1">&#39;Balance&#39;</span><span class="p">],</span>
              <span class="n">Default</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Default</span><span class="o">.</span><span class="n">Default</span> <span class="o">==</span> <span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="s1">&#39;Income&#39;</span><span class="p">],</span> 
              <span class="n">edgecolors</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;No&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">8e4</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Balance&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Income&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Default&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="c1"># Center</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Default&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Balance&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">Default</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="n">CP</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mf">3e3</span><span class="p">])</span>
<span class="c1"># Right</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Default&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Income&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">Default</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="n">CP</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mf">8e4</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a0eaa9581dd7fb625f33e2f47e749b7b30530145a7aa5ba6c640bed18e72adcd.png" src="../_images/a0eaa9581dd7fb625f33e2f47e749b7b30530145a7aa5ba6c640bed18e72adcd.png" />
</div>
</div>
<p>The Default dataset is portrayed through three enlightening panels. Let’s explore each <span id="id6">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<p><strong>Left Panel:</strong>
Within this panel, the yearly incomes and monthly credit card balances of diverse individuals are visually depicted. Individuals who encountered default in their credit card payments are distinctly portrayed in Orange, while those who successfully averted default are prominently showcased in Green. This visualization immediately illuminates the dynamic interplay between income, credit card balance, and credit card payment outcomes.</p>
<p><strong>Center Panel:</strong>
At the heart of this panel, a series of boxplots takes prominence, providing insights into the distribution of credit card balances in relation to default status. These boxplots offer a comprehensive overview of medians, quartiles, and potential outliers within each default group. The contrast between individuals who experienced default and those who did not is vividly delineated, succinctly revealing disparities in balances.</p>
<p><strong>Right Panel:</strong>
Complementing the center panel, the right panel introduces a pair of boxplots that cast a spotlight on income variation according to default status. These boxplots concisely capture nuanced shifts in income distribution for both defaulters and non-defaulters, offering a visual narrative that illuminates the role income plays in credit card payment outcomes.</p>
<p>The Student status can be effectively represented using a <a class="reference external" href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">dummy variable</a>, which is a common technique in data analysis and machine learning. This involves creating a binary variable that takes the value of 1 if the individual is a student, and 0 if the individual is not a student. In mathematical terms, this encoding can be expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a37c6ac0-dd01-4d4e-9891-d550e0e45fa5">
<span class="eqno">(9.85)<a class="headerlink" href="#equation-a37c6ac0-dd01-4d4e-9891-d550e0e45fa5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Student} =
\begin{cases}
1, &amp; \text{if Student}, \\
0, &amp; \text{if Non-Student}.
\end{cases}
\end{equation}\]</div>
<p>In this context:</p>
<ul class="simple">
<li><p>When an individual’s Student status is true, the corresponding value of the dummy variable is set to 1, indicating that the individual is a student.</p></li>
<li><p>Conversely, if an individual is not a student, the dummy variable takes the value of 0, signifying the non-student status.</p></li>
</ul>
<p>This approach simplifies the representation of Student status in a format that can be effectively utilized in various analyses and predictive modeling scenarios. It provides a straightforward and standardized way to incorporate categorical information like Student status into computational workflows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">Default</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Default&#39;</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Default</span><span class="o">.</span><span class="n">Default</span><span class="o">.</span><span class="n">factorize</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Student&#39;</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Default</span><span class="o">.</span><span class="n">Student</span><span class="o">.</span><span class="n">factorize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;From:&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Default</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;To&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>From:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Default</th>
      <th>Student</th>
      <th>Balance</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>No</td>
      <td>No</td>
      <td>729.526495</td>
      <td>44361.625074</td>
    </tr>
    <tr>
      <th>1</th>
      <td>No</td>
      <td>Yes</td>
      <td>817.180407</td>
      <td>12106.134700</td>
    </tr>
    <tr>
      <th>2</th>
      <td>No</td>
      <td>No</td>
      <td>1073.549164</td>
      <td>31767.138947</td>
    </tr>
    <tr>
      <th>3</th>
      <td>No</td>
      <td>No</td>
      <td>529.250605</td>
      <td>35704.493935</td>
    </tr>
    <tr>
      <th>4</th>
      <td>No</td>
      <td>No</td>
      <td>785.655883</td>
      <td>38463.495879</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>To
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Default</th>
      <th>Student</th>
      <th>Balance</th>
      <th>Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>729.526495</td>
      <td>44361.625074</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>817.180407</td>
      <td>12106.134700</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>1073.549164</td>
      <td>31767.138947</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>529.250605</td>
      <td>35704.493935</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>785.655883</td>
      <td>38463.495879</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>A dummy variable, often referred to as an indicator or binary variable, is a fundamental concept in the realm of statistics and econometrics. It essentially serves as a numerical representation of categorical data or group membership by adopting either of two values, which are typically 0 or 1. The primary purpose of dummy variables is to provide a means to incorporate categorical information into quantitative analyses. The key aspects of dummy variables can be elucidated as follows:</p>
<ol class="arabic simple">
<li><p><strong>Representation of Categorical Data:</strong> Dummy variables find utility in symbolizing categories or distinct levels within a categorical variable. For instance, if we consider a categorical variable like “Color” encompassing values such as “Red,” “Green,” and “Blue,” we can create three corresponding dummy variables. These dummy variables take on values of 0 or 1 to indicate whether a particular observation belongs to a given category.</p></li>
<li><p><strong>Binary Coding Approach:</strong> The encoding of dummy variables follows a binary convention. In the context of a categorical variable featuring “k” categories, “k-1” dummy variables are generated. Each individual dummy variable is associated with a specific category, with a value of 1 denoting that an observation pertains to that category, while a value of 0 signifies that it does not. This practice is adopted to prevent multicollinearity, which arises when including all “k” dummy variables could lead to interdependencies.</p></li>
<li><p><strong>Utilization in Regression Analysis:</strong> Dummy variables play a prevalent role in regression analysis, particularly within the domain of multiple regression. They facilitate the integration of categorical variables into regression models, permitting the assessment of the influence of different categories on the dependent variable. Through the incorporation of dummy variables, one can delineate the effects of each category independently.</p></li>
<li><p><strong>Illustrative Scenario:</strong> As an illustration, consider an investigation into the impact of education levels on income. A categorical variable representing education may include categories such as “High School,” “Bachelor’s,” and “Master’s.” In this context, two dummy variables would be fashioned—one for “Bachelor’s” and one for “Master’s.” If an individual possesses a Bachelor’s degree, the “Bachelor’s” dummy variable assumes a value of 1, while the “Master’s” dummy variable retains a value of 0. Conversely, if an individual holds a Master’s degree, the “Master’s” dummy variable would be 1, and the “Bachelor’s” dummy variable would be 0.</p></li>
</ol>
<p>Dummy variables serve as a fundamental tool for translating categorical data into a format amenable to quantitative analysis. Their application is pervasive, particularly in the realm of statistical modeling, where they are harnessed extensively in regression analyses to accommodate categorical variables and investigate their impact on other variables of interest.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pandas.factorize()</span></code> function is a useful tool in the pandas library for converting categorical data into numerical representations. It assigns a unique integer to each unique category (factor) in the input array, which makes it easier to work with categorical data in numerical computations and machine learning algorithms:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Sample categorical data</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">]</span>

<span class="c1"># Using pandas.factorize() to convert categorical data to numerical labels</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">uniques</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded Labels:&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unique Categories:&quot;</span><span class="p">,</span> <span class="n">uniques</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Original</span> <span class="n">Data</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">]</span>
<span class="n">Encoded</span> <span class="n">Labels</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">Unique</span> <span class="n">Categories</span><span class="p">:</span> <span class="n">Index</span><span class="p">([</span><span class="s1">&#39;apple&#39;</span><span class="p">,</span> <span class="s1">&#39;banana&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;object&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the example above, the <code class="docutils literal notranslate"><span class="pre">pandas.factorize()</span></code> function converts the categorical data (‘apple’, ‘banana’, ‘apple’, ‘banana’, ‘apple’, ‘orange’) into numerical labels (0, 1, 0, 1, 0, 2), and it also returns the unique categories as an index object.</p>
</div>
</section>
</section>
<section id="simple-logistic-regression">
<h2><span class="section-number">9.4.3. </span>Simple Logistic Regression<a class="headerlink" href="#simple-logistic-regression" title="Link to this heading">#</a></h2>
<p>Let’s explore a logistic regression approach characterized by the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-72a3a733-f0b6-4fff-bad6-af7824a5060a">
<span class="eqno">(9.86)<a class="headerlink" href="#equation-72a3a733-f0b6-4fff-bad6-af7824a5060a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 \times \text{Balance}
\end{equation}\]</div>
<p>This formulation encapsulates the likelihood of default occurrence. It revolves around the effect of the ‘Balance’ predictor. While we initially mentioned ‘Student’ status and ‘Income’ as contributors, for clarity, we’ll focus on ‘Balance’ in this representation.</p>
<p>The probability of default, denoted as <span class="math notranslate nohighlight">\( p(X) \)</span>, is expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b0de2051-e5d7-4581-8ed9-52a96914ddb9">
<span class="eqno">(9.87)<a class="headerlink" href="#equation-b0de2051-e5d7-4581-8ed9-52a96914ddb9" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(X) = \frac{e^{\beta_0 + \beta_1 \times \text{Balance}}}
{1 + e^{\beta_0 + \beta_1 \times \text{Balance}}}
\end{equation}\]</div>
<p>Breaking down the elements of the equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> and <span class="math notranslate nohighlight">\( \beta_1 \)</span> are coefficients associated with the intercept and ‘Balance’, respectively.</p></li>
<li><p>‘Balance’ serves as the predictor variable that shapes the logistic regression model’s outcome.</p></li>
<li><p>The natural logarithm (<span class="math notranslate nohighlight">\( \log \)</span>) is applied to the odds ratio <span class="math notranslate nohighlight">\( \frac{p(X)}{1 - p(X)} \)</span>, enabling the representation of the relationship between the ‘Balance’ predictor and the log-odds of default in a linear manner.</p></li>
<li><p>The exponentiation (<span class="math notranslate nohighlight">\( e^{\dots} \)</span>) incorporated within the formula translates the log-odds into a probability scale.</p></li>
</ul>
<p>In essence, this logistic regression equation dissects the impact of ‘Balance’ on predicting the probability of default, providing a focused lens into this specific predictor’s role in the model.</p>
<p>Within the context of the Default dataset, logistic regression serves as a potent tool for modeling the probability of encountering default. To illustrate, consider the probability of default given a specific balance, which can be expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-676a99be-96bc-4adb-a363-4ad0b658ff16">
<span class="eqno">(9.88)<a class="headerlink" href="#equation-676a99be-96bc-4adb-a363-4ad0b658ff16" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Pr (Default = Yes | balance)}.
\end{equation}\]</div>
<p>In this formulation, logistic regression enables us to quantitatively assess the likelihood of a default occurrence based on the observed balance value. This probability-based perspective provides valuable insights into the potential outcomes within the dataset, shedding light on the dynamics between credit card balances and the probability of encountering default.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Default&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Generating test data</span>
<span class="n">X_gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Pred_proba</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_gen</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Conditional probability is a fundamental concept in probability theory that deals with the likelihood of an event occurring given that another event has already occurred. In other words, it quantifies the probability of an event happening under a specific condition or constraint.</p>
<p>Mathematically, the conditional probability of event A occurring given event B has occurred is denoted as P(A|B) and is calculated as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-212d0142-be55-4fd2-9a54-80cace8be911">
<span class="eqno">(9.89)<a class="headerlink" href="#equation-212d0142-be55-4fd2-9a54-80cace8be911" title="Permalink to this equation">#</a></span>\[\begin{equation} P(A|B) = \frac{P(A \cap B)}{P(B)} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(A|B) \)</span> is the conditional probability of event A given event B.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(A \cap B) \)</span> is the probability that both events A and B occur.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(B) \)</span> is the probability of event B occurring.</p></li>
</ul>
<p>Key points to remember about conditional probability:</p>
<ol class="arabic simple">
<li><p>The denominator, <span class="math notranslate nohighlight">\( P(B) \)</span>, ensures that we are considering only the subset of situations in which event B has occurred.</p></li>
<li><p>If events A and B are independent, then <span class="math notranslate nohighlight">\( P(A|B) = P(A) \)</span>, meaning that the occurrence of event B has no impact on the probability of event A.</p></li>
<li><p>Conditional probability is essential for understanding real-world situations where outcomes are influenced by certain conditions. For instance, in medical diagnosis, the probability of having a disease may change based on the results of specific tests.</p></li>
<li><p>The concept of conditional probability forms the basis for Bayes’ theorem, a powerful tool for updating probabilities based on new evidence.</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#7aea49&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;#38761d&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_gen</span><span class="p">,</span> <span class="n">Pred_proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#C60004&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Balance&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Probability of Default&#39;</span><span class="p">,</span>
              <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Estimated probability of default using logistic regression&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5d057e0805eb572cff1801ecfccfacffc9a1c0b23662d3cfdb31b2e56fbf5f47.png" src="../_images/5d057e0805eb572cff1801ecfccfacffc9a1c0b23662d3cfdb31b2e56fbf5f47.png" />
</div>
</div>
<p>The predicted probabilities of default are showcased, derived from the logistic regression method. Unlike the left panel, these probabilities align with the fundamental concept of probability, always lying within the range of 0 to 1. This adherence to the proper probability range is a distinctive trait of logistic regression’s predictions.</p>
<p>An alternative approach is to utilize the <a class="reference external" href="https://www.statsmodels.org/devel/examples/notebooks/generated/glm_formula.html"><strong>Statsmodels Generalized Linear Models</strong></a> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>

<span class="c1"># Also recall our Reg_Result for OLS Reression results</span>
<span class="k">def</span><span class="w"> </span><span class="nf">Reg_Result</span><span class="p">(</span><span class="n">Inp</span><span class="p">):</span>
    <span class="n">Temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_html</span><span class="p">(</span><span class="n">Inp</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">as_html</span><span class="p">(),</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Temp</span><span class="o">.</span><span class="n">style</span>\
    <span class="o">.</span><span class="n">format</span><span class="p">({</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;P&gt;|t|&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;std err&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:.4e}</span><span class="s1">&#39;</span><span class="p">})</span>\
    <span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;coef&#39;</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Lime&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">set_properties</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;std err&#39;</span><span class="p">],</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;background-color&#39;</span><span class="p">:</span> <span class="s1">&#39;DimGray&#39;</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;White&#39;</span><span class="p">}))</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">glm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;Default ~ Balance&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">Results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="n">Reg_Result</span><span class="p">(</span><span class="n">Results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                Default   No. Observations:                10000
Model:                            GLM   Df Residuals:                     9998
Model Family:                Binomial   Df Model:                            1
Link Function:                  Logit   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -798.23
Date:                Sat, 27 Jan 2024   Deviance:                       1596.5
Time:                        23:46:35   Pearson chi2:                 7.15e+03
No. Iterations:                     9   Pseudo R-squ. (CS):             0.1240
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -10.6513      0.361    -29.491      0.000     -11.359      -9.943
Balance        0.0055      0.000     24.952      0.000       0.005       0.006
==============================================================================
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
#T_f7a0b_row0_col0 {
  width: 10em;
  background: linear-gradient(90deg, Lime 99.9%, transparent 99.9%);
}
#T_f7a0b_row0_col1, #T_f7a0b_row1_col1 {
  background-color: DimGray;
  color: White;
}
#T_f7a0b_row1_col0 {
  width: 10em;
  background: linear-gradient(90deg, transparent 99.9%, Lime 99.9%, Lime 100.0%, transparent 100.0%);
}
</style>
<table id="T_f7a0b">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_f7a0b_level0_col0" class="col_heading level0 col0" >coef</th>
      <th id="T_f7a0b_level0_col1" class="col_heading level0 col1" >std err</th>
      <th id="T_f7a0b_level0_col2" class="col_heading level0 col2" >z</th>
      <th id="T_f7a0b_level0_col3" class="col_heading level0 col3" >P>|z|</th>
      <th id="T_f7a0b_level0_col4" class="col_heading level0 col4" >[0.025</th>
      <th id="T_f7a0b_level0_col5" class="col_heading level0 col5" >0.975]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_f7a0b_level0_row0" class="row_heading level0 row0" >Intercept</th>
      <td id="T_f7a0b_row0_col0" class="data row0 col0" >-1.0651e+01</td>
      <td id="T_f7a0b_row0_col1" class="data row0 col1" >3.6100e-01</td>
      <td id="T_f7a0b_row0_col2" class="data row0 col2" >-29.491000</td>
      <td id="T_f7a0b_row0_col3" class="data row0 col3" >0.000000</td>
      <td id="T_f7a0b_row0_col4" class="data row0 col4" >-11.359000</td>
      <td id="T_f7a0b_row0_col5" class="data row0 col5" >-9.943000</td>
    </tr>
    <tr>
      <th id="T_f7a0b_level0_row1" class="row_heading level0 row1" >Balance</th>
      <td id="T_f7a0b_row1_col0" class="data row1 col0" >5.5000e-03</td>
      <td id="T_f7a0b_row1_col1" class="data row1 col1" >0.0000e+00</td>
      <td id="T_f7a0b_row1_col2" class="data row1 col2" >24.952000</td>
      <td id="T_f7a0b_row1_col3" class="data row1 col3" >0.000000</td>
      <td id="T_f7a0b_row1_col4" class="data row1 col4" >0.005000</td>
      <td id="T_f7a0b_row1_col5" class="data row1 col5" >0.006000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>In particular, for</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">glm</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;Default ~ Balance&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
</pre></div>
</div>
<p>we have,</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">formula='Default</span> <span class="pre">~</span> <span class="pre">Balance'</span></code>: This is the formula notation used to specify the relationship between the response variable (‘Default’) and the predictor variable (‘Balance’). The tilde symbol (~) separates the response variable from the predictor variable. In this case, it signifies that we are modeling the influence of ‘Balance’ on the likelihood of ‘Default’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">family=sm.families.Binomial()</span></code>: This parameter defines the error distribution or family of the model. In this case, we’re using the Binomial distribution, which is appropriate for binary response variables. The <code class="docutils literal notranslate"><span class="pre">sm.families.Binomial()</span></code> indicates that we’re using the Binomial distribution from the Statsmodels library.</p></li>
</ol>
</section>
<section id="multiple-logistic-regression">
<h2><span class="section-number">9.4.4. </span>Multiple Logistic Regression<a class="headerlink" href="#multiple-logistic-regression" title="Link to this heading">#</a></h2>
<p>Let’s delve into an alternative logistic regression approach characterized by the following equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-68780ff6-22d1-469a-871e-5cd2603cd71e">
<span class="eqno">(9.90)<a class="headerlink" href="#equation-68780ff6-22d1-469a-871e-5cd2603cd71e" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 \times \text{Student} + \beta_2 \times \text{Balance} + \beta_3 \times \text{Income}
\end{equation}\]</div>
<p>This formulation encapsulates the likelihood of encountering default. It hinges on the contribution of multiple predictors, including ‘Student’ status, ‘Balance’, and ‘Income’. The probability of default, <span class="math notranslate nohighlight">\( p(X) \)</span>, is expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ce7d010a-4d1d-4819-8f2f-3079c4f77278">
<span class="eqno">(9.91)<a class="headerlink" href="#equation-ce7d010a-4d1d-4819-8f2f-3079c4f77278" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(X) = \dfrac{e^{\beta_0 + \beta_1 \times \text{Student} + \beta_2 \times \text{Balance} + \beta_3 \times \text{Income}}}
{1 + e^{\beta_0 + \beta_1 \times \text{Student} + \beta_2 \times \text{Balance} + \beta_3 \times \text{Income}}}
\end{equation}\]</div>
<p>In this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span>, <span class="math notranslate nohighlight">\( \beta_1 \)</span>, <span class="math notranslate nohighlight">\( \beta_2 \)</span>, and <span class="math notranslate nohighlight">\( \beta_3 \)</span> are the coefficients corresponding to the intercept, ‘Student’ status, ‘Balance’, and ‘Income’, respectively.</p></li>
<li><p>‘Student’, ‘Balance’, and ‘Income’ are the predictor variables that contribute to the logistic regression model.</p></li>
<li><p>The natural logarithm (<span class="math notranslate nohighlight">\( \log \)</span>) is applied to the odds ratio <span class="math notranslate nohighlight">\( \frac{p(X)}{1 - p(X)} \)</span> to linearly model the relationship between the predictors and the log-odds of default.</p></li>
<li><p>The exponentiation (<span class="math notranslate nohighlight">\( e^{\dots} \)</span>) within the formula facilitates the transformation of the log-odds back to the probability scale.</p></li>
</ul>
<p>In essence, this logistic regression equation accounts for the combined influence of ‘Student’ status, ‘Balance’, and ‘Income’ in predicting the probability of default.</p>
<p>An alternative approach is to utilize the <a class="reference external" href="https://www.statsmodels.org/devel/examples/notebooks/generated/glm_formula.html"><strong>Statsmodels Generalized Linear Models</strong></a> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;Default ~ Student + Income + Balance&#39;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">glm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">Results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="n">Reg_Result</span><span class="p">(</span><span class="n">Results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                Default   No. Observations:                10000
Model:                            GLM   Df Residuals:                     9996
Model Family:                Binomial   Df Model:                            3
Link Function:                  Logit   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -785.77
Date:                Sat, 27 Jan 2024   Deviance:                       1571.5
Time:                        23:46:35   Pearson chi2:                 7.00e+03
No. Iterations:                     9   Pseudo R-squ. (CS):             0.1262
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -10.8690      0.492    -22.079      0.000     -11.834      -9.904
Student       -0.6468      0.236     -2.738      0.006      -1.110      -0.184
Income      3.033e-06    8.2e-06      0.370      0.712    -1.3e-05    1.91e-05
Balance        0.0057      0.000     24.737      0.000       0.005       0.006
==============================================================================
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
#T_0185d_row0_col0 {
  width: 10em;
  background: linear-gradient(90deg, Lime 99.9%, transparent 99.9%);
}
#T_0185d_row0_col1, #T_0185d_row1_col1, #T_0185d_row2_col1, #T_0185d_row3_col1 {
  background-color: DimGray;
  color: White;
}
#T_0185d_row1_col0 {
  width: 10em;
  background: linear-gradient(90deg, transparent 94.0%, Lime 94.0%, Lime 99.9%, transparent 99.9%);
}
#T_0185d_row2_col0 {
  width: 10em;
  background: linear-gradient(90deg, transparent 99.9%, Lime 99.9%, Lime 99.9%, transparent 99.9%);
}
#T_0185d_row3_col0 {
  width: 10em;
  background: linear-gradient(90deg, transparent 99.9%, Lime 99.9%, Lime 100.0%, transparent 100.0%);
}
</style>
<table id="T_0185d">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_0185d_level0_col0" class="col_heading level0 col0" >coef</th>
      <th id="T_0185d_level0_col1" class="col_heading level0 col1" >std err</th>
      <th id="T_0185d_level0_col2" class="col_heading level0 col2" >z</th>
      <th id="T_0185d_level0_col3" class="col_heading level0 col3" >P>|z|</th>
      <th id="T_0185d_level0_col4" class="col_heading level0 col4" >[0.025</th>
      <th id="T_0185d_level0_col5" class="col_heading level0 col5" >0.975]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_0185d_level0_row0" class="row_heading level0 row0" >Intercept</th>
      <td id="T_0185d_row0_col0" class="data row0 col0" >-1.0869e+01</td>
      <td id="T_0185d_row0_col1" class="data row0 col1" >4.9200e-01</td>
      <td id="T_0185d_row0_col2" class="data row0 col2" >-22.079000</td>
      <td id="T_0185d_row0_col3" class="data row0 col3" >0.000000</td>
      <td id="T_0185d_row0_col4" class="data row0 col4" >-11.834000</td>
      <td id="T_0185d_row0_col5" class="data row0 col5" >-9.904000</td>
    </tr>
    <tr>
      <th id="T_0185d_level0_row1" class="row_heading level0 row1" >Student</th>
      <td id="T_0185d_row1_col0" class="data row1 col0" >-6.4680e-01</td>
      <td id="T_0185d_row1_col1" class="data row1 col1" >2.3600e-01</td>
      <td id="T_0185d_row1_col2" class="data row1 col2" >-2.738000</td>
      <td id="T_0185d_row1_col3" class="data row1 col3" >0.006000</td>
      <td id="T_0185d_row1_col4" class="data row1 col4" >-1.110000</td>
      <td id="T_0185d_row1_col5" class="data row1 col5" >-0.184000</td>
    </tr>
    <tr>
      <th id="T_0185d_level0_row2" class="row_heading level0 row2" >Income</th>
      <td id="T_0185d_row2_col0" class="data row2 col0" >3.0330e-06</td>
      <td id="T_0185d_row2_col1" class="data row2 col1" >8.2000e-06</td>
      <td id="T_0185d_row2_col2" class="data row2 col2" >0.370000</td>
      <td id="T_0185d_row2_col3" class="data row2 col3" >0.712000</td>
      <td id="T_0185d_row2_col4" class="data row2 col4" >-0.000013</td>
      <td id="T_0185d_row2_col5" class="data row2 col5" >0.000019</td>
    </tr>
    <tr>
      <th id="T_0185d_level0_row3" class="row_heading level0 row3" >Balance</th>
      <td id="T_0185d_row3_col0" class="data row3 col0" >5.7000e-03</td>
      <td id="T_0185d_row3_col1" class="data row3 col1" >0.0000e+00</td>
      <td id="T_0185d_row3_col2" class="data row3 col2" >24.737000</td>
      <td id="T_0185d_row3_col3" class="data row3 col3" >0.000000</td>
      <td id="T_0185d_row3_col4" class="data row3 col4" >0.005000</td>
      <td id="T_0185d_row3_col5" class="data row3 col5" >0.006000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>As an illustration for the recent model, let’s take the case of two individuals: James, a student, and Robert, a non-student. Both individuals possess a credit card balance of $1,200 and an income of $40,000. We can now examine the estimated probabilities of default for these two individuals:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df0</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Student&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;Income&#39;</span><span class="p">:[</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">],</span><span class="s1">&#39;Balance&#39;</span><span class="p">:[</span><span class="mi">1500</span><span class="p">,</span><span class="mi">1500</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;James&#39;</span><span class="p">,</span> <span class="s1">&#39;Robert&#39;</span><span class="p">])</span>
<span class="n">display</span><span class="p">(</span><span class="n">df0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Student</th>
      <th>Income</th>
      <th>Balance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>James</th>
      <td>1</td>
      <td>40</td>
      <td>1500</td>
    </tr>
    <tr>
      <th>Robert</th>
      <td>0</td>
      <td>40</td>
      <td>1500</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">Results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Student&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">df0</span><span class="p">[</span><span class="s1">&#39;Student&#39;</span><span class="p">]</span>\
<span class="o">+</span><span class="n">Results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">df0</span><span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">Results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">df0</span><span class="p">[</span><span class="s1">&#39;Balance&#39;</span><span class="p">])</span>
<span class="n">P</span><span class="o">=</span><span class="n">E</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">E</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">P</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Probability of defaults&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Probability of defaults</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>James</th>
      <td>0.051615</td>
    </tr>
    <tr>
      <th>Robert</th>
      <td>0.094135</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This observation highlights that students generally exhibit higher probabilities of default compared to non-students.</p>
</section>
<section id="confounding-optional-content">
<h2><span class="section-number">9.4.5. </span>Confounding (Optional Content)<a class="headerlink" href="#confounding-optional-content" title="Link to this heading">#</a></h2>
<p>Confounding in statistics refers to a situation where the relationship between two variables is distorted or influenced by the presence of a third variable, known as a confounder or a confounding variable. This confounder is related to both the independent and dependent variables, creating a scenario where the observed association between the independent and dependent variables might be misleading or inaccurate <span id="id7">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>Confounding variables can introduce bias and lead to incorrect conclusions when analyzing the relationship between variables. To address confounding, researchers need to carefully control or adjust for the effects of confounding variables in their analyses. This can be achieved through techniques like stratification, matching, or statistical modeling, such as multiple regression.</p>
<p>In essence, confounding occurs when the observed relationship between two variables is not a true causal relationship but is influenced by the presence of a third variable that affects both variables being studied. Identifying and accounting for confounding is crucial to ensuring accurate and reliable statistical analyses and interpretations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="c1"># Non-Students</span>
<span class="c1"># Extracting data for non-students</span>
<span class="n">X_NotStudent</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Student</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_NotStudent</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Student</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Default&#39;</span><span class="p">]</span>
<span class="c1"># Creating a logistic regression model for non-students</span>
<span class="n">log_reg_NotStudent</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">log_reg_NotStudent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_NotStudent</span><span class="p">,</span> <span class="n">y_NotStudent</span><span class="p">)</span>

<span class="c1"># Students</span>
<span class="c1"># Extracting data for students</span>
<span class="n">X_Student</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Student</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_Student</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Student</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Default&#39;</span><span class="p">]</span>
<span class="c1"># Creating a logistic regression model for students</span>
<span class="n">log_reg_Student</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">log_reg_Student</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_Student</span><span class="p">,</span> <span class="n">y_Student</span><span class="p">)</span>

<span class="c1"># Generating test data</span>
<span class="n">X_gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Balance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Predicting probabilities for non-students and students</span>
<span class="n">predicted_proba_NotStudent</span> <span class="o">=</span> <span class="n">log_reg_NotStudent</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_gen</span><span class="p">)</span>
<span class="n">predicted_proba_Student</span> <span class="o">=</span> <span class="n">log_reg_Student</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_gen</span><span class="p">)</span>


<span class="n">Group</span> <span class="o">=</span> <span class="n">Default</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Student&#39;</span><span class="p">,</span><span class="s1">&#39;Default&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="s1">&#39;Default&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Group</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Default</th>
      <th>No</th>
      <th>Yes</th>
    </tr>
    <tr>
      <th>Student</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>No</th>
      <td>6850</td>
      <td>206</td>
    </tr>
    <tr>
      <th>Yes</th>
      <td>2817</td>
      <td>127</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span><span class="n">gridspec_kw</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;width_ratios&#39;</span><span class="p">:[</span><span class="mf">.75</span><span class="p">,</span><span class="mf">.25</span><span class="p">]})</span>

<span class="c1"># Plot default probabilities for Non-Students and Students</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_gen</span><span class="p">,</span> <span class="n">predicted_proba_NotStudent</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;No&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Non-Student&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_gen</span><span class="p">,</span> <span class="n">predicted_proba_Student</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Student&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">Temp</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>

<span class="c1"># Plot dashed lines for overall default rates</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">Group</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;Yes&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">Group</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;No&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">],</span>
                 <span class="n">lw</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Overall Student&#39;</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">Temp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xmax</span><span class="o">=</span><span class="n">Temp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">Group</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="s1">&#39;Yes&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">Group</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="s1">&#39;No&#39;</span><span class="p">],</span>
                 <span class="n">lw</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">CP</span><span class="p">[</span><span class="s1">&#39;No&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Overall Non-Student&#39;</span><span class="p">,</span>  <span class="n">xmin</span><span class="o">=</span><span class="n">Temp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xmax</span><span class="o">=</span><span class="n">Temp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="c1"># Set common labels, ticks, and limits for the left plot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Default Probability vs. Credit Card Balance&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Default Rate&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Credit Card Balance&#39;</span><span class="p">,</span>
              <span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_gen</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_gen</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span>
              <span class="n">yticks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
                 <span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create a boxplot for Student vs. Non-Student balance</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Student&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Balance&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">Default</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="n">CP</span><span class="p">)</span>
<span class="c1"># Set common labels and title for the right plot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Credit Card Balance Distribution for</span><span class="se">\n</span><span class="s1">Student vs. Non-Student&#39;</span><span class="p">,</span>
              <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Student&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Credit Card Balance&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7e39a8ef42e273f8f4b603cb0b42dab1b10f3bb95e84689da658ab3d3ecdd395.png" src="../_images/7e39a8ef42e273f8f4b603cb0b42dab1b10f3bb95e84689da658ab3d3ecdd395.png" />
</div>
</div>
<p>The right-hand panel of the figure provides a clear explanation for the observed disparity. It’s evident that the variables “student” and “balance” exhibit a correlation. Typically, students maintain higher levels of debt, and this higher indebtedness is associated with an elevated likelihood of default. In essence, students tend to carry larger credit card balances, a fact we glean from the left-hand panel of the figure, which also shows that substantial credit card balances are linked to elevated default rates.</p>
<p>Even though an individual student with a specific credit card balance would generally have a lower default probability compared to a non-student with the same balance, the aggregate behavior emerges due to students, on average, maintaining larger credit card balances. Consequently, when considering the overall scenario, students exhibit a higher default rate than non-students. This distinction is pivotal, especially for a credit card company aiming to make informed decisions about extending credit.</p>
<p>The level of risk associated with a student surpasses that of a non-student when no information about the student’s credit card balance is available. However, when comparing a student to a non-student with an equivalent credit card balance, the student is actually less risky.</p>
<p><strong>Visual representation</strong>:
The left-hand panel illustrates default rates for students (orange) and non-students (green). The solid lines depict the default rate based on the balance, while the horizontal broken lines indicate the overall default rates. In the right-hand panel, we see boxplots representing the distribution of balance for students (orange) and non-students (green).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlGn&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Correlation_Plot</span> <span class="p">(</span><span class="n">Df</span><span class="p">,</span><span class="n">Fig_Size</span><span class="p">):</span>
    <span class="n">Correlation_Matrix</span> <span class="o">=</span> <span class="n">Df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Correlation_Matrix</span><span class="p">)</span>
    <span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="p">)):</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">Fig_Size</span><span class="p">,</span><span class="n">Fig_Size</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Correlation_Matrix</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.2f&#39;</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;RdYlGn&#39;</span><span class="p">,</span>
                <span class="n">linewidths</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">},</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;shrink&quot;</span><span class="p">:</span> <span class="mf">.7</span><span class="p">})</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
<span class="n">Correlation_Plot</span> <span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_79de6_row0_col0, #T_79de6_row1_col1, #T_79de6_row2_col2, #T_79de6_row3_col3 {
  background-color: #006837;
  color: #f1f1f1;
}
#T_79de6_row0_col1, #T_79de6_row1_col0 {
  background-color: #f8fcb6;
  color: #000000;
}
#T_79de6_row0_col2, #T_79de6_row2_col0 {
  background-color: #b3df72;
  color: #000000;
}
#T_79de6_row0_col3, #T_79de6_row3_col0 {
  background-color: #fffcba;
  color: #000000;
}
#T_79de6_row1_col2, #T_79de6_row2_col1 {
  background-color: #d7ee8a;
  color: #000000;
}
#T_79de6_row1_col3, #T_79de6_row3_col1 {
  background-color: #dd3d2d;
  color: #f1f1f1;
}
#T_79de6_row2_col3, #T_79de6_row3_col2 {
  background-color: #fee797;
  color: #000000;
}
</style>
<table id="T_79de6">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_79de6_level0_col0" class="col_heading level0 col0" >Default</th>
      <th id="T_79de6_level0_col1" class="col_heading level0 col1" >Student</th>
      <th id="T_79de6_level0_col2" class="col_heading level0 col2" >Balance</th>
      <th id="T_79de6_level0_col3" class="col_heading level0 col3" >Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_79de6_level0_row0" class="row_heading level0 row0" >Default</th>
      <td id="T_79de6_row0_col0" class="data row0 col0" >1.00</td>
      <td id="T_79de6_row0_col1" class="data row0 col1" >0.04</td>
      <td id="T_79de6_row0_col2" class="data row0 col2" >0.35</td>
      <td id="T_79de6_row0_col3" class="data row0 col3" >-0.02</td>
    </tr>
    <tr>
      <th id="T_79de6_level0_row1" class="row_heading level0 row1" >Student</th>
      <td id="T_79de6_row1_col0" class="data row1 col0" >0.04</td>
      <td id="T_79de6_row1_col1" class="data row1 col1" >1.00</td>
      <td id="T_79de6_row1_col2" class="data row1 col2" >0.20</td>
      <td id="T_79de6_row1_col3" class="data row1 col3" >-0.75</td>
    </tr>
    <tr>
      <th id="T_79de6_level0_row2" class="row_heading level0 row2" >Balance</th>
      <td id="T_79de6_row2_col0" class="data row2 col0" >0.35</td>
      <td id="T_79de6_row2_col1" class="data row2 col1" >0.20</td>
      <td id="T_79de6_row2_col2" class="data row2 col2" >1.00</td>
      <td id="T_79de6_row2_col3" class="data row2 col3" >-0.15</td>
    </tr>
    <tr>
      <th id="T_79de6_level0_row3" class="row_heading level0 row3" >Income</th>
      <td id="T_79de6_row3_col0" class="data row3 col0" >-0.02</td>
      <td id="T_79de6_row3_col1" class="data row3 col1" >-0.75</td>
      <td id="T_79de6_row3_col2" class="data row3 col2" >-0.15</td>
      <td id="T_79de6_row3_col3" class="data row3 col3" >1.00</td>
    </tr>
  </tbody>
</table>
</div><img alt="../_images/c0422c3ed33dd5ae8876ba610083a773183c92900895f50e42c17cc4f446f65b.png" src="../_images/c0422c3ed33dd5ae8876ba610083a773183c92900895f50e42c17cc4f446f65b.png" />
</div>
</div>
<p>The table you provided is a correlation matrix that shows the pairwise correlations between different variables in the dataset. Each value in the table represents the correlation coefficient between two variables. The correlation coefficient measures the strength and direction of the linear relationship between two variables:</p>
<ul class="simple">
<li><p><strong>Default and Student:</strong> The correlation coefficient is 0.04. This indicates a very weak positive correlation between being a student and defaulting on payments. In other words, being a student has a very slight tendency to be associated with a slightly higher probability of defaulting.</p></li>
<li><p><strong>Default and Balance:</strong> The correlation coefficient is 0.35. This value indicates a moderate positive correlation between credit card balance and default. A higher credit card balance is associated with a moderately higher probability of defaulting.</p></li>
<li><p><strong>Default and Income:</strong> The correlation coefficient is -0.02. This indicates a very weak negative correlation between income and default. There’s almost no noticeable linear relationship between income and the probability of defaulting.</p></li>
<li><p><strong>Student and Balance:</strong> The correlation coefficient is 0.20. This shows a weak positive correlation between being a student and credit card balance. Being a student is somewhat associated with having a slightly higher credit card balance.</p></li>
<li><p><strong>Student and Income:</strong> The correlation coefficient is -0.75. This indicates a strong negative correlation between being a student and income. Being a student is significantly associated with having lower income.</p></li>
<li><p><strong>Balance and Income:</strong> The correlation coefficient is -0.15. This represents a weak negative correlation between credit card balance and income. Individuals with higher credit card balances tend to have slightly lower incomes.</p></li>
</ul>
<p>In summary, the correlation matrix provides insights into the relationships between different variables. Positive correlation values indicate that as one variable increases, the other tends to increase as well. Negative correlation values suggest that as one variable increases, the other tends to decrease. The magnitude of the correlation coefficient indicates the strength of the relationship, with values closer to 1 or -1 indicating a stronger correlation. However, correlation does not imply causation, and other factors may contribute to these relationships.</p>
</section>
<section id="example-synthetic-dataset">
<h2><span class="section-number">9.4.6. </span>Example: Synthetic Dataset<a class="headerlink" href="#example-synthetic-dataset" title="Link to this heading">#</a></h2>
<p><font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:</p>
<ul class="simple">
<li><p><strong>Number of Samples:</strong> 1000</p></li>
<li><p><strong>Number of Features:</strong> 2</p></li>
<li><p><strong>Number of Classes:</strong> 2</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as ‘Feature 1’ and ‘Feature 2’.</p></li>
</ul>
<p><strong>Outcome (Target Variable):</strong></p>
<ul class="simple">
<li><p>The dataset also includes a target variable called ‘Outcome.’ This variable assigns each data point to one of two distinct classes, identified as ‘Class 0’ and ‘Class 1’.</p></li>
</ul>
<p>The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">]</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>

<span class="c1"># Scatter plot of data points</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Outcome&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Synthetic Dataset&#39;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4682a5d8e6d34a213ca52075edba51bae62d660f9ffc4140142d2fede97b415b.png" src="../_images/4682a5d8e6d34a213ca52075edba51bae62d660f9ffc4140142d2fede97b415b.png" />
</div>
</div>
<p>The synthetic dataset depicted above exhibits a balanced distribution between two classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span> <span class="n">colors</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Count&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Outcome&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/feb012e2a4a95e1d60bb169cda600d82c779a55d8fbaf5f2b655c03336d05c88.png" src="../_images/feb012e2a4a95e1d60bb169cda600d82c779a55d8fbaf5f2b655c03336d05c88.png" />
</div>
</div>
<p>Next, Logistic Regression from the scikit-learn library <span id="id8">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span> is employed for the classification of the aforementioned dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#f7dfdf&#39;</span><span class="p">,</span> <span class="s1">&#39;#e3d3f2&#39;</span><span class="p">])</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">solver</span> <span class="o">=</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                   <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                   <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                   <span class="n">xlabel</span><span class="o">=</span> <span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                   <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">grid_resolution</span> <span class="o">=</span> <span class="mi">300</span><span class="p">)</span>
<span class="c1"># Scatter plot of data points</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Outcome&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8c2566aab2d8113d0565db38e8bd603f6569b350a7078f617f10f01406f6486a.png" src="../_images/8c2566aab2d8113d0565db38e8bd603f6569b350a7078f617f10f01406f6486a.png" />
</div>
</div>
<p>In practical application, it is imperative to take into account the division of the dataset into two distinct subsets: one designated for model training and the other for model testing. This separation can be achieved using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function, as exemplified below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape of X_train = </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape of y_train = </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape of X_test = </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape of y_test = </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_gen_cr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                             <span class="n">output_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Results</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Train Data:&#39;</span><span class="p">)</span>
<span class="n">_gen_cr</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test Data:&#39;</span><span class="p">)</span>
<span class="n">_gen_cr</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of X_train = (750, 2)
Shape of y_train = (750,)
Shape of X_test = (250, 2)
Shape of y_test = (250,)

Train Data:
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
</style>
<table id="T_7a46b">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_7a46b_level0_col0" class="col_heading level0 col0" >precision</th>
      <th id="T_7a46b_level0_col1" class="col_heading level0 col1" >recall</th>
      <th id="T_7a46b_level0_col2" class="col_heading level0 col2" >f1-score</th>
      <th id="T_7a46b_level0_col3" class="col_heading level0 col3" >support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_7a46b_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_7a46b_row0_col0" class="data row0 col0" >0.965</td>
      <td id="T_7a46b_row0_col1" class="data row0 col1" >0.965</td>
      <td id="T_7a46b_row0_col2" class="data row0 col2" >0.965</td>
      <td id="T_7a46b_row0_col3" class="data row0 col3" >372.000</td>
    </tr>
    <tr>
      <th id="T_7a46b_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_7a46b_row1_col0" class="data row1 col0" >0.966</td>
      <td id="T_7a46b_row1_col1" class="data row1 col1" >0.966</td>
      <td id="T_7a46b_row1_col2" class="data row1 col2" >0.966</td>
      <td id="T_7a46b_row1_col3" class="data row1 col3" >378.000</td>
    </tr>
    <tr>
      <th id="T_7a46b_level0_row2" class="row_heading level0 row2" >accuracy</th>
      <td id="T_7a46b_row2_col0" class="data row2 col0" >0.965</td>
      <td id="T_7a46b_row2_col1" class="data row2 col1" >0.965</td>
      <td id="T_7a46b_row2_col2" class="data row2 col2" >0.965</td>
      <td id="T_7a46b_row2_col3" class="data row2 col3" >0.965</td>
    </tr>
    <tr>
      <th id="T_7a46b_level0_row3" class="row_heading level0 row3" >macro avg</th>
      <td id="T_7a46b_row3_col0" class="data row3 col0" >0.965</td>
      <td id="T_7a46b_row3_col1" class="data row3 col1" >0.965</td>
      <td id="T_7a46b_row3_col2" class="data row3 col2" >0.965</td>
      <td id="T_7a46b_row3_col3" class="data row3 col3" >750.000</td>
    </tr>
    <tr>
      <th id="T_7a46b_level0_row4" class="row_heading level0 row4" >weighted avg</th>
      <td id="T_7a46b_row4_col0" class="data row4 col0" >0.965</td>
      <td id="T_7a46b_row4_col1" class="data row4 col1" >0.965</td>
      <td id="T_7a46b_row4_col2" class="data row4 col2" >0.965</td>
      <td id="T_7a46b_row4_col3" class="data row4 col3" >750.000</td>
    </tr>
  </tbody>
</table>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Data:
</pre></div>
</div>
<div class="output text_html"><style type="text/css">
</style>
<table id="T_8f26e">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_8f26e_level0_col0" class="col_heading level0 col0" >precision</th>
      <th id="T_8f26e_level0_col1" class="col_heading level0 col1" >recall</th>
      <th id="T_8f26e_level0_col2" class="col_heading level0 col2" >f1-score</th>
      <th id="T_8f26e_level0_col3" class="col_heading level0 col3" >support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_8f26e_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_8f26e_row0_col0" class="data row0 col0" >0.952</td>
      <td id="T_8f26e_row0_col1" class="data row0 col1" >0.938</td>
      <td id="T_8f26e_row0_col2" class="data row0 col2" >0.945</td>
      <td id="T_8f26e_row0_col3" class="data row0 col3" >128.000</td>
    </tr>
    <tr>
      <th id="T_8f26e_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_8f26e_row1_col0" class="data row1 col0" >0.935</td>
      <td id="T_8f26e_row1_col1" class="data row1 col1" >0.951</td>
      <td id="T_8f26e_row1_col2" class="data row1 col2" >0.943</td>
      <td id="T_8f26e_row1_col3" class="data row1 col3" >122.000</td>
    </tr>
    <tr>
      <th id="T_8f26e_level0_row2" class="row_heading level0 row2" >accuracy</th>
      <td id="T_8f26e_row2_col0" class="data row2 col0" >0.944</td>
      <td id="T_8f26e_row2_col1" class="data row2 col1" >0.944</td>
      <td id="T_8f26e_row2_col2" class="data row2 col2" >0.944</td>
      <td id="T_8f26e_row2_col3" class="data row2 col3" >0.944</td>
    </tr>
    <tr>
      <th id="T_8f26e_level0_row3" class="row_heading level0 row3" >macro avg</th>
      <td id="T_8f26e_row3_col0" class="data row3 col0" >0.944</td>
      <td id="T_8f26e_row3_col1" class="data row3 col1" >0.944</td>
      <td id="T_8f26e_row3_col2" class="data row3 col2" >0.944</td>
      <td id="T_8f26e_row3_col3" class="data row3 col3" >250.000</td>
    </tr>
    <tr>
      <th id="T_8f26e_level0_row4" class="row_heading level0 row4" >weighted avg</th>
      <td id="T_8f26e_row4_col0" class="data row4 col0" >0.944</td>
      <td id="T_8f26e_row4_col1" class="data row4 col1" >0.944</td>
      <td id="T_8f26e_row4_col2" class="data row4 col2" >0.944</td>
      <td id="T_8f26e_row4_col3" class="data row4 col3" >250.000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>The <code class="docutils literal notranslate"><span class="pre">classification_report</span></code> function in scikit-learn is a powerful tool for generating a comprehensive report that evaluates the performance of a classification model. It provides a breakdown of various metrics such as precision, recall, F1-score, and support for each class in the classification problem. This report helps you understand how well the model is performing for different classes and allows you to make informed decisions about its performance <span id="id9">[<a class="reference internal" href="../References.html#id160" title="F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_true</span></code>: The true target values for the test set.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred</span></code>: The predicted target values for the test set.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_names</span></code>: A list of class names. If provided, it helps label the rows of the report.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>: Optional weights that can be assigned to individual samples.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">digits</span></code>: The number of decimal places for displaying the results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dict</span></code>: If True, the function returns the classification report as a dictionary.</p></li>
</ul>
<p>The output of <code class="docutils literal notranslate"><span class="pre">classification_report</span></code> is a nicely formatted string (if <code class="docutils literal notranslate"><span class="pre">output_dict</span></code> is False) or a dictionary (if <code class="docutils literal notranslate"><span class="pre">output_dict</span></code> is True) containing precision, recall, F1-score, and support metrics for each class. The metrics are calculated based on the true and predicted labels.</p>
<p>For example, if you have a binary classification problem with classes “Positive” and “Negative,” the report might look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>              <span class="n">precision</span>    <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>   <span class="n">support</span>

    <span class="n">Positive</span>       <span class="mf">0.80</span>      <span class="mf">0.90</span>      <span class="mf">0.85</span>       <span class="mi">100</span>
    <span class="n">Negative</span>       <span class="mf">0.70</span>      <span class="mf">0.50</span>      <span class="mf">0.58</span>        <span class="mi">50</span>

    <span class="n">accuracy</span>                           <span class="mf">0.77</span>       <span class="mi">150</span>
   <span class="n">macro</span> <span class="n">avg</span>       <span class="mf">0.75</span>      <span class="mf">0.70</span>      <span class="mf">0.71</span>       <span class="mi">150</span>
<span class="n">weighted</span> <span class="n">avg</span>       <span class="mf">0.76</span>      <span class="mf">0.77</span>      <span class="mf">0.76</span>       <span class="mi">150</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Precision</strong>: The ability of the model to correctly identify positive instances among the instances it labeled as positive.</p></li>
<li><p><strong>Recall</strong>: The ability of the model to correctly identify all positive instances among all actual positive instances.</p></li>
<li><p><strong>F1-score</strong>: The harmonic mean of precision and recall. It balances precision and recall and is useful when classes are imbalanced.</p></li>
<li><p><strong>Support</strong>: The number of actual occurrences of the class in the test set.</p></li>
</ul>
</div>
<p>The performance of the trained model on both the training and testing datasets is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Create a loop for train and test sets</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train Set&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test Set&#39;</span><span class="p">)]):</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_set</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> 
                     <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                    <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
        
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Outcome:&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s1"> - Logistic Regression&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b9e974f68ef1b57f15654daddb4f45e833790cf6bc2c7667a9d34cca196e78c4.png" src="../_images/b9e974f68ef1b57f15654daddb4f45e833790cf6bc2c7667a9d34cca196e78c4.png" />
</div>
</div>
<p>Upon closer examination, it becomes evident that certain data points from the set X have been misclassified. These instances are visually highlighted with yellow circles in the subsequent figure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Create a loop for train and test sets</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train Set&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test Set&#39;</span><span class="p">)]):</span>
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_set</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> 
                     <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                    <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">num</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Outcome </span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
    <span class="c1"># Plot data points where y_set and log_reg(X_set) differ in color</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_set</span> <span class="o">!=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_set</span><span class="p">)],</span> 
                    <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_set</span> <span class="o">!=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_set</span><span class="p">)],</span>
                    <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;Yellow&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span> <span class="s1">&#39;h&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="s1">&#39;Inaccurate Predictions&#39;</span><span class="p">)</span>
    
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s1"> - Logistic Regression&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Remove the legend for each panel</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend</span><span class="p">()</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

<span class="c1"># Create a single legend for both subplots at the top</span>
<span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">borderaxespad</span><span class="o">=</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/88e477628970a02716c72820c8b814cdeb933008a9f49bdecfe06a65a3d4954f.png" src="../_images/88e477628970a02716c72820c8b814cdeb933008a9f49bdecfe06a65a3d4954f.png" />
</div>
</div>
<p>Lastly, we present the confusion matrix below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>  <span class="c1"># Import matplotlib for plotting</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">confusion_matrix</span>  <span class="c1"># Import necessary functions/classes</span>

<span class="k">def</span><span class="w"> </span><span class="nf">format_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">true_neg</span><span class="p">,</span> <span class="n">false_pos</span><span class="p">,</span> <span class="n">false_neg</span><span class="p">,</span> <span class="n">true_pos</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">result</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1m</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s2"> Set Confusion Matrix</span><span class="se">\033</span><span class="s2">[0m:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">result</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">true_neg</span><span class="si">}</span><span class="s2"> instances were correctly predicted as class 0.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">result</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">true_pos</span><span class="si">}</span><span class="s2"> instances were correctly predicted as class 1.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">result</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">false_pos</span><span class="si">}</span><span class="s2"> instance was incorrectly predicted as class 1 when it was actually class 0.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">result</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">false_neg</span><span class="si">}</span><span class="s2"> instances were incorrectly predicted as class 0 when they were actually class 1.</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span><span class="w"> </span><span class="nf">plot_cm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">class_names</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">)):</span>
    <span class="c1"># Create a figure and axes for displaying confusion matrices side by side</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    
    <span class="n">datasets</span> <span class="o">=</span> <span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;Train&#39;</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Test&#39;</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dataset_name</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        <span class="c1"># Compute confusion matrix for the dataset predictions</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        
        <span class="n">result</span> <span class="o">=</span> <span class="n">format_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        
        <span class="c1"># Create a ConfusionMatrixDisplay and plot it on the respective axis</span>
        <span class="n">cm_display</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>\
                        <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                              <span class="n">im_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span> <span class="k">if</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s1">&#39;Train&#39;</span> <span class="k">else</span> <span class="s1">&#39;Blues&#39;</span><span class="p">),</span>
                              <span class="n">text_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s1"> Data&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Add a super title for the entire figure</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Confusion Matrices&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>

    <span class="c1"># Adjust the layout for better spacing</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_cm</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Train Set Confusion Matrix</span>:
- 359 instances were correctly predicted as class 0.
- 365 instances were correctly predicted as class 1.
- 13 instance was incorrectly predicted as class 1 when it was actually class 0.
- 13 instances were incorrectly predicted as class 0 when they were actually class 1.

<span class=" -Color -Color-Bold">Test Set Confusion Matrix</span>:
- 120 instances were correctly predicted as class 0.
- 116 instances were correctly predicted as class 1.
- 8 instance was incorrectly predicted as class 1 when it was actually class 0.
- 6 instances were incorrectly predicted as class 0 when they were actually class 1.
</pre></div>
</div>
<img alt="../_images/b825729e512067c0ff578caed4334031e91fa53ee017512fdb74148def4a1d17.png" src="../_images/b825729e512067c0ff578caed4334031e91fa53ee017512fdb74148def4a1d17.png" />
</div>
</div>
</section>
<section id="multiclass-classifier">
<h2><span class="section-number">9.4.7. </span>Multiclass Classifier<a class="headerlink" href="#multiclass-classifier" title="Link to this heading">#</a></h2>
<p>A multiclass classifier is a model capable of predicting one of more than two possible classes for a given input. For instance, a multiclass classifier could identify the color of a flower as red, blue, or green, or classify an animal as a dog, cat, or bird.</p>
<p>However, not all classification models are inherently equipped to handle multiclass problems. Many linear models are designed for binary classification, which involves only two possible classes, such as yes or no, positive or negative, and so on.</p>
<p>To extend a binary classification model to a multiclass classification model, various techniques can be employed. One of the most common ones is the one-vs.-rest (OvR) approach, also known as one-vs.-all (OvA) or binary relevance (BR).</p>
<section id="one-vs-rest-approach">
<h3><span class="section-number">9.4.7.1. </span>One-vs.-Rest Approach<a class="headerlink" href="#one-vs-rest-approach" title="Link to this heading">#</a></h3>
<p>The OvR approach is a simple and intuitive way to use binary classifiers for multiclass classification. The idea is to train one binary classifier for each class, where the target class is treated as positive and the rest of the classes are treated as negative.</p>
<p>For example, if you have three
classes, <font color='Red'><b>R</b></font>, <font color='Green'><b>G</b></font>, and <font color='Blue'><b>B</b></font>, you can create three binary
classifiers:</p>
<ul class="simple">
<li><p><strong>Classifier 1:</strong> <font color='Red'><b>R</b></font> vs (<font color='Blue'><b>B</b></font> or <font color='Green'><b>G</b></font>)</p></li>
<li><p><strong>Classifier 2:</strong> <font color='Green'><b>G</b></font> vs (<font color='Red'><b>R</b></font> or <font color='Blue'><b>B</b></font>)</p></li>
<li><p><strong>Classifier 3:</strong> <font color='Blue'><b>B</b></font> vs (<font color='Red'><b>R</b></font> or <font color='Green'><b>G</b></font>)</p></li>
</ul>
<figure class="align-center" id="id14">
<a class="reference internal image-reference" href="../_images/OVR.png"><img alt="../_images/OVR.png" src="../_images/OVR.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.5 </span><span class="caption-text">Example for One-vs.-Rest Approach.</span><a class="headerlink" href="#id14" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Each classifier will output a probability of belonging to its
positive class, such as <font color='Red'><b>R</b></font>, <font color='Green'><b>G</b></font>, and <font color='Blue'><b>B</b></font>. Then, you can
compare the probabilities and choose the class with the highest
probability as the final prediction.</p>
<p>The OvR approach has several advantages, such as simplicity, speed, and ease of implementation. However, it also has some drawbacks:</p>
<ul class="simple">
<li><p>It may encounter class imbalance, as the negative class is typically much larger than the positive class. This imbalance may affect the performance and accuracy of the classifiers.</p></li>
<li><p>It assumes that the classes are mutually exclusive, meaning that an input can only belong to one class. However, this assumption may not hold true for some problems, such as multi-label classification, where an input can belong to multiple classes.</p></li>
<li><p>It does not account for correlations or dependencies between the classes, which may impact the quality of the predictions.</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression">
<h3><span class="section-number">9.4.7.2. </span>Multinomial Logistic Regression<a class="headerlink" href="#multinomial-logistic-regression" title="Link to this heading">#</a></h3>
<p>Multinomial logistic regression is a model that can handle multiple classes directly, unlike binary logistic regression that can only handle two classes. It is an extension of binary logistic regression that creates a binary model for each class and then combines them using a softmax function. The softmax function is a mathematical function that turns a vector of numbers into a vector of probabilities that add up to one. It gives each class a probability, and the class with the highest probability is the most likely one. The model can be expressed as <span id="id10">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-df56ba27-bbde-437c-8d7c-deda9e46f82e">
<span class="eqno">(9.92)<a class="headerlink" href="#equation-df56ba27-bbde-437c-8d7c-deda9e46f82e" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\displaystyle \Pr(Y=k~|~X = x)={\frac {e^{{\boldsymbol {\beta }}_{k}\cdot \mathbf {X} _{i}}}{1+\sum _{j=1}^{K-1}e^{{\boldsymbol {\beta }}_{j}\cdot \mathbf {X} _{i}}}}\;\;\;\;,\;\;k=1,2,\ldots,K-1}.
\end{equation}\]</div>
<p>Multiclass logistic regression offers several benefits:</p>
<ul class="simple">
<li><p>It is more accurate and flexible, as it can reflect the true patterns and relationships in the data.</p></li>
<li><p>It avoids the problems of class imbalance or mutual exclusivity, as it assigns a probability to each class independently.</p></li>
<li><p>It can handle both ordinal and nominal classes, meaning that the classes can either have an order or not.</p></li>
</ul>
<p>However, multinomial logistic regression also has some limitations:</p>
<ul class="simple">
<li><p>It may be more costly and complex, as it requires more parameters and computations than binary logistic regression.</p></li>
<li><p>It may be more susceptible to the effects of regularization, a method used to prevent overfitting or underfitting. Regularization works by adding a penalty term to the cost function that depends on the size or complexity of the model parameters. The amount of regularization needs to be carefully chosen to balance the bias and variance of the model.</p></li>
</ul>
</section>
<section id="sklearn-logistic-regression">
<h3><span class="section-number">9.4.7.3. </span>Sklearn Logistic Regression<a class="headerlink" href="#sklearn-logistic-regression" title="Link to this heading">#</a></h3>
<p>Sklearn is a popular Python library for machine learning and data science. Sklearn provides a logistic regression model that can handle both binary and multiclass classification problems. Sklearn logistic regression allows us to use either the OvR or the multinomial approach by setting the <code class="docutils literal notranslate"><span class="pre">multi_class</span></code> hyperparameter to either <code class="docutils literal notranslate"><span class="pre">'ovr'</span></code> or <code class="docutils literal notranslate"><span class="pre">'multinomial'</span></code>, respectively. The default value is <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, which means that the algorithm will choose the best option based on the data and the solver <span id="id11">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">multi_class</span></code> hyperparameter is different from the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> hyperparameter, which is used to adjust the weights of the classes to account for class imbalance. The <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> hyperparameter can be set to either <code class="docutils literal notranslate"><span class="pre">'balanced'</span></code> or a dictionary of class weights <span id="id12">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>You can find more information and examples about sklearn logistic regression and the <code class="docutils literal notranslate"><span class="pre">multi_class</span></code> and <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> hyperparameters in the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">documentation</a> and this <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html">tutorial</a> <span id="id13">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p><font color='Blue'><b>Example</b></font>: The <strong>Iris Dataset</strong> serves as a quintessential example in explaining cross-validations. Crafted by the eminent British biologist and statistician Ronald A. Fisher in 1936, the dataset finds its roots in the realm of discriminant analysis. Named after the Iris flower species it encapsulates, this dataset stands as a foundational cornerstone in machine learning and statistics.</p>
<p><strong>Dataset Composition</strong></p>
<p>This dataset encompasses measurements of four distinct attributes in three diverse species of Iris flowers:</p>
<ol class="arabic simple">
<li><p>Sepal Length</p></li>
<li><p>Sepal Width</p></li>
<li><p>Petal Length</p></li>
<li><p>Petal Width</p></li>
</ol>
<p>The dataset encapsulates three distinct species of Iris flowers:</p>
<ol class="arabic simple">
<li><p>Iris setosa</p></li>
<li><p>Iris versicolor</p></li>
<li><p>Iris virginica</p></li>
</ol>
<p>Each Iris flower species is accompanied by precisely 50 samples, culminating in a total dataset size of 150 instances.</p>
<p>Before delving into that, let’s first examine the distribution of the dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Create a Pandas DataFrame</span>
<span class="n">iris_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">iris_df</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>

<span class="c1"># Print the first few rows of the DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">iris_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Pairplot: Scatterplots for each pair of features</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">iris_df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># _ = g.map_lower(sns.kdeplot, levels=4, color=&quot;.2&quot;)</span>



<span class="c1"># Create a 2x2 grid of subplots with a specified figure size</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Flatten the 2x2 grid into a 1D array of subplots</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Get the column names of the iris DataFrame, excluding the &#39;species&#39; column</span>
<span class="n">Cols</span> <span class="o">=</span> <span class="n">iris_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># Iterate through the columns using enumerate</span>
<span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Cols</span><span class="p">):</span>
    <span class="c1"># Create a violin plot using Seaborn</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">iris_df</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
    
    <span class="c1"># Set the title for the subplot</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Violinplot of </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s2"> by species&quot;</span><span class="p">)</span>

<span class="c1"># Adjust the layout of the subplots for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/df5b1e895bd1d39c264788ff56cb11853473c586101fe70911d25c63062048de.png" src="../_images/df5b1e895bd1d39c264788ff56cb11853473c586101fe70911d25c63062048de.png" />
<img alt="../_images/f5e951e161c20a4ba1a55b456fae0671b928f33e7fd7c2af63b897754427c2b7.png" src="../_images/f5e951e161c20a4ba1a55b456fae0671b928f33e7fd7c2af63b897754427c2b7.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span>

<span class="k">def</span><span class="w"> </span><span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">31</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Select only the first two features for visualization</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create color maps</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Red&quot;</span><span class="p">,</span> <span class="s2">&quot;Blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Green&quot;</span><span class="p">]</span>

<span class="c1"># Different values of multi_class</span>
<span class="n">multi_class_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">]</span>

<span class="c1"># Create a 1x2 plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">multi_class</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">multi_class_values</span><span class="p">):</span>
    <span class="c1"># Create a Logistic Regression classifier</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Evaluate the model</span>
    <span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Plot decision boundaries</span>
    <span class="n">display_dbd</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                                         <span class="n">cmap</span><span class="o">=</span>  <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">),</span>
                                                         <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                                         <span class="n">grid_resolution</span> <span class="o">=</span> <span class="mi">300</span><span class="p">)</span>

    <span class="c1"># Plot training points</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y_train</span><span class="p">],</span>
                    <span class="n">palette</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

    <span class="c1"># Set the title</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
               <span class="n">ylabel</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logistic Regression classification (multi_class = </span><span class="si">{</span><span class="n">multi_class</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>

    <span class="c1"># Print the classification report</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">multi_class</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Classification Report:&quot;</span><span class="p">,</span> <span class="mi">34</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Test Classification Report:&quot;</span><span class="p">,</span> <span class="mi">34</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Adjust the layout of the subplots for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">LogisticRegression: ovr</span>
<span class=" -Color -Color-Bold -Color-Bold-Blue">	Train Classification Report:</span>
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>0.975000</td>
      <td>0.975000</td>
      <td>0.975000</td>
      <td>40.000000</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>0.666667</td>
      <td>0.634146</td>
      <td>0.650000</td>
      <td>41.000000</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>0.658537</td>
      <td>0.692308</td>
      <td>0.675000</td>
      <td>39.000000</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.766667</td>
      <td>0.766667</td>
      <td>0.766667</td>
      <td>0.766667</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.766734</td>
      <td>0.767151</td>
      <td>0.766667</td>
      <td>120.000000</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.766802</td>
      <td>0.766667</td>
      <td>0.766458</td>
      <td>120.000000</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">	Test Classification Report:</span>
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>10.000000</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>1.000000</td>
      <td>0.777778</td>
      <td>0.875000</td>
      <td>9.000000</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>0.846154</td>
      <td>1.000000</td>
      <td>0.916667</td>
      <td>11.000000</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.933333</td>
      <td>0.933333</td>
      <td>0.933333</td>
      <td>0.933333</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.948718</td>
      <td>0.925926</td>
      <td>0.930556</td>
      <td>30.000000</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.943590</td>
      <td>0.933333</td>
      <td>0.931944</td>
      <td>30.000000</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">LogisticRegression: multinomial</span>
<span class=" -Color -Color-Bold -Color-Bold-Blue">	Train Classification Report:</span>
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>40.0</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>0.697674</td>
      <td>0.731707</td>
      <td>0.714286</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>0.702703</td>
      <td>0.666667</td>
      <td>0.684211</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.800000</td>
      <td>0.800000</td>
      <td>0.800000</td>
      <td>0.8</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.800126</td>
      <td>0.799458</td>
      <td>0.799499</td>
      <td>120.0</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.800084</td>
      <td>0.800000</td>
      <td>0.799749</td>
      <td>120.0</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">	Test Classification Report:</span>
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>0.875000</td>
      <td>0.777778</td>
      <td>0.823529</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>0.833333</td>
      <td>0.909091</td>
      <td>0.869565</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.900000</td>
      <td>0.900000</td>
      <td>0.900000</td>
      <td>0.9</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.902778</td>
      <td>0.895623</td>
      <td>0.897698</td>
      <td>30.0</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.901389</td>
      <td>0.900000</td>
      <td>0.899233</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/8c5083b83f7d99fce32514ad333c5e3acf3d5c9ef0ca893126faafcd3684e375.png" src="../_images/8c5083b83f7d99fce32514ad333c5e3acf3d5c9ef0ca893126faafcd3684e375.png" />
</div>
</div>
<p>For the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model with <code class="docutils literal notranslate"><span class="pre">multi_class='ovr'</span></code>:</p>
<ul class="simple">
<li><p>The training set accuracy is approximately 0.77, and the testing set accuracy is approximately 0.93. This means that the model correctly classified 77% of the samples in the training set and 93% of the samples in the testing set.</p></li>
<li><p>The precision, recall, and F1-score for each class vary. For example, the model has perfect precision, recall, and F1-score for the ‘setosa’ class in the testing set.</p></li>
</ul>
<p>For the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model with <code class="docutils literal notranslate"><span class="pre">multi_class='multinomial'</span></code>:</p>
<ul class="simple">
<li><p>The training set accuracy is approximately 0.80, and the testing set accuracy is approximately 0.90. This means that the model correctly classified 80% of the samples in the training set and 90% of the samples in the testing set.</p></li>
<li><p>The precision, recall, and F1-score for each class vary. For example, the model has perfect precision, recall, and F1-score for the ‘setosa’ class in the testing set.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C09S03.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9.3. </span>Multiple Linear Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C09S05.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.5. </span>K-Nearest Neighbors (K-NN)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-linear-regression">9.4.1. Why Not Linear Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-predicting-a-binary-response-variable">9.4.2. Logistic Regression: Predicting a Binary Response Variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-model">9.4.2.1. Logistic Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-probabilities">9.4.2.2. Estimating Probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-logistic-function">9.4.2.3. Sigmoid (Logistic) Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-coefficients">9.4.2.4. Finding the Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translating-theory-into-python">9.4.2.5. Translating Theory into Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-logistic-regression">9.4.3. Simple Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-logistic-regression">9.4.4. Multiple Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confounding-optional-content">9.4.5. Confounding (Optional Content)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">9.4.6. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classifier">9.4.7. Multiclass Classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-approach">9.4.7.1. One-vs.-Rest Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">9.4.7.2. Multinomial Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-logistic-regression">9.4.7.3. Sklearn Logistic Regression</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>