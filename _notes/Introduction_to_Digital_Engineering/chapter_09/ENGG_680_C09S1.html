

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>9.1. Prologue: Statistical Metrics and Evaluation &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_09/ENGG_680_C09S1';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2. An Introduction to Linear Regression" href="ENGG_680_C09S2.html" />
    <link rel="prev" title="9. An Introduction to Machine Learning" href="ReadMe.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">9. An Introduction to Machine Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Prologue: Statistical Metrics and Evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-test-sets">9.1.1. Training, Validation, and Test sets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error">9.1.2. Standard Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-statistic">9.1.3. R-squared Statistic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">9.1.4. Mean Squared Error (MSE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">9.1.5. Mean Absolute Error (MAE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-matrix">9.1.6. Correlation Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">9.1.7. Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-evaluating-correct-classifications">9.1.8. Accuracy: Evaluating Correct Classifications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#precision">9.1.9. Precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity-or-true-positive-rate">9.1.10. Recall (Sensitivity or True Positive Rate)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score">9.1.11. F1-Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#balanced-accuracy">9.1.12. Balanced Accuracy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matthews-correlation-coefficient-mcc">9.1.13. Matthews Correlation Coefficient (MCC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cohens-kappa">9.1.14. Cohen’s Kappa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learns-metrics">9.1.15. Scikit-learn’s metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additioal-information-for-statsmodels-api-optional-content">9.1.16. Additioal Information for <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code> (Optional Content)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-and-coefficient-precision-in-statistics">9.1.16.1. Standard Error and Coefficient Precision in Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-value-t-and-p-value-p-t-in-regression-analysis">9.1.16.2. t-value (<span class="math notranslate nohighlight">\(t\)</span>) and p-value (<span class="math notranslate nohighlight">\(P&gt;|t|\)</span>) in Regression Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-test">9.1.16.3. Omnibus Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#durbin-watson-statistic">9.1.16.4. Durbin-Watson Statistic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jarque-bera-jb-test">9.1.16.5. Jarque-Bera (JB) Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skewness">9.1.16.6. Skewness</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kurtosis">9.1.16.7. Kurtosis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-number-cond">9.1.16.8. Condition Number (Cond)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="prologue-statistical-metrics-and-evaluation">
<h1><span class="section-number">9.1. </span>Prologue: Statistical Metrics and Evaluation<a class="headerlink" href="#prologue-statistical-metrics-and-evaluation" title="Permalink to this heading">#</a></h1>
<p>To utilize the content of this page optimally, I recommend consulting the relevant topic to gain a deeper understanding of specific subjects.</p>
<section id="training-validation-and-test-sets">
<h2><span class="section-number">9.1.1. </span>Training, Validation, and Test sets<a class="headerlink" href="#training-validation-and-test-sets" title="Permalink to this heading">#</a></h2>
<p>The process of partitioning a dataset into training, validation, and test sets is of paramount importance when it comes to evaluating predictive models effectively. This practice is elucidated and supported by the following explanations <span id="id1">[<a class="reference internal" href="../References.html#id39" title="P. Dangeti. Statistics for Machine Learning. Packt Publishing, 2017. ISBN 9781788291224. URL: https://books.google.ca/books?id=C-dDDwAAQBAJ.">Dangeti, 2017</a>, <a class="reference internal" href="../References.html#id166" title="K.L. Priddy and P.E. Keller. Artificial Neural Networks: An Introduction. SPIE tutorial texts. SPIE Press, 2005. ISBN 9780819459879. URL: https://books.google.ca/books?id=BrnHR7esWmkC.">Priddy and Keller, 2005</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Training Set</strong>: The training set, constituting the largest portion of the dataset, serves as the bedrock for model development. Within this segment, machine learning algorithms glean insights from the data’s patterns. It is during this phase that the model is meticulously trained to recognize intricate relationships and make accurate predictions.</p></li>
<li><p><strong>Validation Set</strong>: Following the training phase, the model’s performance is rigorously assessed using the validation set. This evaluation process is instrumental in fine-tuning the model’s hyperparameters and gauging its ability to generalize to new data. The validation set holds a critical role in preventing the model from overfitting or underfitting the training data.</p></li>
<li><p><strong>Test Set</strong>: Different from the training set, the test set serves as an unbiased measure for evaluating the model’s performance on completely new and unseen data. Its primary function is to provide an estimate of how well the model is likely to perform when deployed in a real-world context.</p></li>
</ol>
<figure class="align-center" id="id24">
<a class="reference internal image-reference" href="../_images/Data_Distribution.jpg"><img alt="../_images/Data_Distribution.jpg" src="../_images/Data_Distribution.jpg" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9.2 </span><span class="caption-text">The dataset is divided into three subsets: training, validation, and test, with an 80% allocation for training data, and 10% each for validation and testing.</span><a class="headerlink" href="#id24" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The partitioning of a dataset into these three subsets represents a fundamental and established practice in the development of robust machine learning models. This approach is widely embraced both within the academic sphere and the broader field of data analysis.</p>
<p>There are scenarios where only the training and test sets are used, and the validation set is omitted. The decision to exclude the validation set can be based on the specifics of the problem and the available data. Here are a few situations where using only the training and test sets might be appropriate:</p>
<ol class="arabic simple">
<li><p><strong>Large Datasets</strong>: When you have a substantial amount of data, the training set can be sizeable enough to adequately train the model while still leaving a substantial portion for testing. In such cases, the need for a separate validation set is reduced.</p></li>
<li><p><strong>K-Fold Cross-Validation</strong>: Instead of a fixed validation set, you can perform K-fold cross-validation using the training data. This involves splitting the training data into K subsets (folds) and iteratively using each fold as a validation set while training on the remaining data. This can provide a more robust assessment of model performance without the need for a dedicated validation set.</p></li>
<li><p><strong>Limited Data Availability</strong>: In situations where data is scarce, allocating a portion to a validation set may significantly reduce the training data’s size, impacting the model’s ability to learn. In such cases, some practitioners choose to rely on the test set to evaluate the model’s performance.</p></li>
<li><p><strong>Time Series Data</strong>: When dealing with time series data, it’s common to use only a training set and a test set. This is because the order of data points is critical, and random shuffling, as typically done with validation sets, can lead to data leakage. In time series analysis, the test set often represents future data points.</p></li>
</ol>
<p>However, it’s essential to be cautious when omitting a validation set. The validation set is crucial for hyperparameter tuning and ensuring that the model generalizes well. If you choose to exclude it, you should be aware of the potential risks, such as overfitting or suboptimal model performance due to poor hyperparameter choices. The decision should be made based on a careful consideration of the specific problem, dataset size, and available resources.</p>
</section>
<section id="standard-error">
<h2><span class="section-number">9.1.2. </span>Standard Error<a class="headerlink" href="#standard-error" title="Permalink to this heading">#</a></h2>
<p>In statistics, the <strong>standard error (SE)</strong> is a way to measure how accurate an estimated value is, like the coefficients in a linear regression. It shows the average amount by which estimates can vary when we take multiple samples from the same group <span id="id2">[<a class="reference internal" href="../References.html#id22" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>]</span>.</p>
<p>The standard error (SE) is typically calculated using this formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-182de6d9-1794-4fe9-8889-d5c6f1dc2d12">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-182de6d9-1794-4fe9-8889-d5c6f1dc2d12" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{SE} = \frac{s}{\sqrt{n}} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( s \)</span> represents the sample standard deviation, which measures the variability of data points within the sample.</p></li>
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points (sample size).</p></li>
</ul>
<p>This formula provides a straightforward way to compute the standard error, which quantifies the precision of an estimated value based on a given sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Set the matplotlib style using a custom style file</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Define the URL for the data</span>
<span class="n">Link</span> <span class="o">=</span> <span class="s1">&#39;https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&amp;stationID=27211&amp;Year=2007&amp;Month=1&amp;Day=1&amp;time=&amp;timeframe=3&amp;submit=Download+Data&#39;</span>

<span class="c1"># Read the CSV data and select specific columns</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">Link</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">,</span> <span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">])</span>

<span class="c1"># Convert the &#39;Date/Time&#39; column to datetime format and set it as the index</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">)</span>

<span class="c1"># Display the DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Create a figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Create a line plot for &#39;Mean Temp (°C)&#39;</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;line&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Maroon&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span>
                          <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;Mean Temperature Over Time (CALGARY INT&#39;L CS)&quot;&quot;&quot;</span><span class="p">,</span>
                          <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Date/Time&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">,</span>
                          <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;2000-01-01&#39;</span><span class="p">,</span> <span class="s1">&#39;2008-01-01&#39;</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">])</span>

<span class="c1"># Ensure the plot layout is tight</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
      <th>Month</th>
      <th>Mean Temp (°C)</th>
    </tr>
    <tr>
      <th>Date/Time</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2000-06-01</th>
      <td>2000</td>
      <td>6</td>
      <td>13.0</td>
    </tr>
    <tr>
      <th>2000-07-01</th>
      <td>2000</td>
      <td>7</td>
      <td>17.2</td>
    </tr>
    <tr>
      <th>2000-08-01</th>
      <td>2000</td>
      <td>8</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>2000-09-01</th>
      <td>2000</td>
      <td>9</td>
      <td>11.1</td>
    </tr>
    <tr>
      <th>2000-10-01</th>
      <td>2000</td>
      <td>10</td>
      <td>5.6</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2007-07-01</th>
      <td>2007</td>
      <td>7</td>
      <td>19.9</td>
    </tr>
    <tr>
      <th>2007-08-01</th>
      <td>2007</td>
      <td>8</td>
      <td>14.6</td>
    </tr>
    <tr>
      <th>2007-09-01</th>
      <td>2007</td>
      <td>9</td>
      <td>10.6</td>
    </tr>
    <tr>
      <th>2007-10-01</th>
      <td>2007</td>
      <td>10</td>
      <td>6.9</td>
    </tr>
    <tr>
      <th>2007-11-01</th>
      <td>2007</td>
      <td>11</td>
      <td>-1.3</td>
    </tr>
  </tbody>
</table>
<p>90 rows × 3 columns</p>
</div></div><img alt="../_images/7a77eb186d53e66dfc2700cace20ffbd267189f19f09dffa3d4a5691ae70098d.png" src="../_images/7a77eb186d53e66dfc2700cace20ffbd267189f19f09dffa3d4a5691ae70098d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Month Name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">month_name</span><span class="p">()</span>
<span class="n">Standard_Error</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;Month Name&#39;</span><span class="p">])[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>\
                <span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Month&#39;</span><span class="p">])[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">Standard_Error</span> <span class="o">=</span> <span class="n">Standard_Error</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">&#39;Standard Error&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Standard_Error</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Standard Error</th>
    </tr>
    <tr>
      <th>Month</th>
      <th>Month Name</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <th>January</th>
      <td>1.293705</td>
    </tr>
    <tr>
      <th>2</th>
      <th>February</th>
      <td>1.107550</td>
    </tr>
    <tr>
      <th>3</th>
      <th>March</th>
      <td>1.895717</td>
    </tr>
    <tr>
      <th>4</th>
      <th>April</th>
      <td>0.756469</td>
    </tr>
    <tr>
      <th>5</th>
      <th>May</th>
      <td>0.692378</td>
    </tr>
    <tr>
      <th>6</th>
      <th>June</th>
      <td>0.337533</td>
    </tr>
    <tr>
      <th>7</th>
      <th>July</th>
      <td>0.382349</td>
    </tr>
    <tr>
      <th>8</th>
      <th>August</th>
      <td>0.588108</td>
    </tr>
    <tr>
      <th>9</th>
      <th>September</th>
      <td>0.465003</td>
    </tr>
    <tr>
      <th>10</th>
      <th>October</th>
      <td>0.641688</td>
    </tr>
    <tr>
      <th>11</th>
      <th>November</th>
      <td>1.187885</td>
    </tr>
    <tr>
      <th>12</th>
      <th>December</th>
      <td>1.062791</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We also have the option to compute this using the <code class="docutils literal notranslate"><span class="pre">scipy</span></code> package:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="n">Standard_Error</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;Month Name&#39;</span><span class="p">])[</span><span class="s1">&#39;Mean Temp (°C)&#39;</span><span class="p">]</span>\
                            <span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">Mean_Temperature_C</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                                 <span class="n">SE</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Standard_Error</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Month</th>
      <th>Month Name</th>
      <th>Mean_Temperature_C</th>
      <th>SE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>January</td>
      <td>-5.228571</td>
      <td>1.293705</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>February</td>
      <td>-5.400000</td>
      <td>1.107550</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>March</td>
      <td>-2.142857</td>
      <td>1.895717</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>April</td>
      <td>4.671429</td>
      <td>0.756469</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>May</td>
      <td>9.828571</td>
      <td>0.692378</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>June</td>
      <td>13.900000</td>
      <td>0.337533</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>July</td>
      <td>18.157143</td>
      <td>0.408748</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>August</td>
      <td>16.087500</td>
      <td>0.588108</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>September</td>
      <td>11.287500</td>
      <td>0.465003</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>October</td>
      <td>5.162500</td>
      <td>0.641688</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>November</td>
      <td>-1.250000</td>
      <td>1.187885</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>December</td>
      <td>-4.900000</td>
      <td>1.062791</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Create a figure and axis</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Create the bar plot with error bars</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Standard_Error</span><span class="p">[</span><span class="s1">&#39;Month Name&#39;</span><span class="p">],</span>
       <span class="n">height</span><span class="o">=</span><span class="n">Standard_Error</span><span class="o">.</span><span class="n">Mean_Temperature_C</span><span class="p">,</span>
       <span class="n">yerr</span><span class="o">=</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">Standard_Error</span><span class="o">.</span><span class="n">SE</span><span class="p">,</span>
       <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
       <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Bisque&#39;</span><span class="p">,</span>
       <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;Tomato&#39;</span><span class="p">,</span>
       <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
       <span class="n">error_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ecolor</span><span class="o">=</span><span class="s1">&#39;DarkRed&#39;</span><span class="p">))</span>

<span class="c1"># Set labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Month&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Temperature (°C)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Monthly Mean Temperature with Standard Errors</span><span class="se">\n</span><span class="s1">Error bars demonstrate a 95</span><span class="si">% c</span><span class="s1">onfidence interval&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Set the Y-axis limits for better visualization</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>

<span class="c1"># Rotate x-axis labels for better readability</span>
<span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>

<span class="c1"># Add a grid for clarity</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Add a horizontal line at y=0 for reference</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="c1"># Customize the appearance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a564e4225c1650c583d0d157d07ee3804ee60a04549fa4c5c020c0d4a2874e3e.png" src="../_images/a564e4225c1650c583d0d157d07ee3804ee60a04549fa4c5c020c0d4a2874e3e.png" />
</div>
</div>
<p>Let’s break down what it represents:</p>
<ol class="arabic simple">
<li><p><strong>X-Axis (Month Name)</strong>: The X-axis of the plot represents the months of the year, from January to December. Each month is labeled with its name.</p></li>
<li><p><strong>Y-Axis (Mean Temperature in °C)</strong>: The Y-axis represents the average (mean) temperature in degrees Celsius for each respective month. It shows the typical or average temperature for that month.</p></li>
<li><p><strong>Bars</strong>: The blue bars on the plot represent the mean temperature for each month. The height of each bar indicates the average temperature for that specific month. For example, you can visually compare which months have higher or lower average temperatures.</p></li>
<li><p><strong>Error Bars</strong>: The black vertical lines with caps extending from the top of the bars are error bars. These error bars represent the standard error associated with the mean temperature for each month. They show the level of uncertainty in the mean temperature estimate. Larger error bars indicate higher variability, meaning that the temperature data for that month varies more widely.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>If the error bar for a particular data point, such as March in your case, is longer than the actual bar, it typically means that the variability or uncertainty in that data point is relatively high compared to the actual value. This can happen for various reasons, such as:</p>
<ol class="arabic simple">
<li><p><strong>Small Sample Size:</strong> If you have a small sample size for the data point (March in this case), the estimate of the mean (the actual bar) might have higher uncertainty, leading to a longer error bar.</p></li>
<li><p><strong>Outliers:</strong> Outliers in the data can significantly affect the standard error and, consequently, the length of the error bars.</p></li>
<li><p><strong>Heteroscedasticity:</strong> If the variability of data is not consistent across all data points, it can result in longer error bars for data points with higher variability.</p></li>
<li><p><strong>Skewed Data:</strong> Data distributions that are not symmetric can lead to differences in the standard error and actual values, affecting the error bar length.</p></li>
</ol>
</div>
<p><font color='Blue'><b>Example:</b></font> In this example, we have two key variables, <span class="math notranslate nohighlight">\(y\)</span> and ‘y-hat,’ utilized to simulate and explain statistical metrics such as R-squared (<span class="math notranslate nohighlight">\(R^2\)</span>), Mean Squared Error (MSE), Mean Absolute Error (MAE), and more. <span class="math notranslate nohighlight">\(y\)</span> represents the observed data, which we aim to approximate, while <span class="math notranslate nohighlight">\(\hat{y}\)</span> is a constructed set of values used for this simulation. <span class="math notranslate nohighlight">\(y\)</span> serves as a representation of actual or observed data, although it’s important to note that in this case, we do not have a specific model generating <span class="math notranslate nohighlight">\(y\)</span> from ‘X’ or any real-world data. Instead, <span class="math notranslate nohighlight">\(y\)</span> is introduced as a reference point against which we can measure the performance of various statistical metrics. <span class="math notranslate nohighlight">\(\hat{y}\)</span> is a synthetic or hypothetical dataset, created for the purpose of this simulation. It is designed to emulate the values that a model might predict for <span class="math notranslate nohighlight">\(y\)</span> in a real-world scenario. The comparison between <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(\hat{y}\)</span> in this context provides a basis for understanding and evaluating statistical measures, shedding light on how well these metrics assess the relationship between observed and predicted data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create a model or use any other method to make predictions</span>
<span class="c1"># In this example, we use a simple quadratic model for demonstration</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># =============================================================================</span>
<span class="c1"># Figure</span>
<span class="c1"># =============================================================================</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;width_ratios&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">.6</span><span class="p">,</span> <span class="mf">.4</span><span class="p">]})</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Plot the data points and model predictions with enhanced styling</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
              <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Observed Data ($y$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
              <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Model Prediction ($\hat</span><span class="si">{y}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$X$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$y$ and $\hat</span><span class="si">{y}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;SkyBlue&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]],</span>
               <span class="p">[</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;--r&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Observed Data ($y$)&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Model Prediction ($\hat</span><span class="si">{y}</span><span class="s1">$)&#39;</span><span class="p">)</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_gridspec</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">),</span>
                            <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;LightCoral&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$|y - \hat</span><span class="si">{y}</span><span class="s1">|$&#39;</span><span class="p">,</span>
            <span class="n">yscale</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span>
            <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e1</span><span class="p">])</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;y and $\hat</span><span class="si">{y}</span><span class="s1">$ comparison&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0a75f9cc5fe80ad554d294b77e8ef4179063212a77ce95c866bd5721f61682ca.png" src="../_images/0a75f9cc5fe80ad554d294b77e8ef4179063212a77ce95c866bd5721f61682ca.png" />
</div>
</div>
</section>
<section id="r-squared-statistic">
<h2><span class="section-number">9.1.3. </span>R-squared Statistic<a class="headerlink" href="#r-squared-statistic" title="Permalink to this heading">#</a></h2>
<p>The R-squared statistic, symbolized as <span class="math notranslate nohighlight">\(R^2\)</span>, is a valuable metric used in linear regression analysis to gauge the proportion of the total variability observed in the dependent variable (<span class="math notranslate nohighlight">\(Y\)</span>) that can be explained by the independent variable(s) (<span class="math notranslate nohighlight">\(X\)</span>) included in the model. It takes values within the range of 0 to 1, where a value of 1 indicates that the model perfectly accounts for the variability in the dependent variable, while a value of 0 suggests that the model fails to explain any of the observed variability <span id="id3">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>The formula for calculating <span class="math notranslate nohighlight">\(R^2\)</span> is expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8de5498a-bf9b-4368-944e-7bf39a54ae00">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-8de5498a-bf9b-4368-944e-7bf39a54ae00" title="Permalink to this equation">#</a></span>\[\begin{align}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{align}\]</div>
<p>Here are the components in the formula:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> represents the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> signifies the actual value of the dependent variable for the <span class="math notranslate nohighlight">\(i\)</span>-th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> corresponds to the predicted value of the dependent variable for the <span class="math notranslate nohighlight">\(i\)</span>-th data point based on the regression model.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{y}\)</span> denotes the mean value of the dependent variable.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(R^2\)</span> is an essential tool in assessing the goodness of fit of a linear regression model. It quantifies how well the model’s predictions align with the actual data. A high <span class="math notranslate nohighlight">\(R^2\)</span> value implies that a substantial proportion of the variability in the dependent variable is captured by the model, indicating a better fit. Conversely, a low <span class="math notranslate nohighlight">\(R^2\)</span> suggests that the model is not effectively explaining the variability in the dependent variable, and improvements may be necessary.</p>
<p><span class="math notranslate nohighlight">\(R^2\)</span> has important uses, but it’s essential to complement its interpretation with other evaluation techniques, especially when working with complex models or in the presence of multicollinearity or heteroscedasticity. Combining <span class="math notranslate nohighlight">\(R^2\)</span> with domain knowledge and diagnostic tests ensures a comprehensive understanding of the model’s performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;#198c19&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;#304529&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$(y - \hat</span><span class="si">{y}</span><span class="s1">)^2$&#39;</span><span class="p">,</span>
            <span class="n">yscale</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span>
            <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">])</span>

<span class="n">markerline</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;#5830ea&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;#331689&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$(y - \bar</span><span class="si">{y}</span><span class="s1">)^2$&#39;</span><span class="p">,</span>
            <span class="n">yscale</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span>
            <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/324ddc4f3be764786f0870f44b7adc31b5e4f384b5fe36cb69fe134610774c22.png" src="../_images/324ddc4f3be764786f0870f44b7adc31b5e4f384b5fe36cb69fe134610774c22.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
    <span class="c1"># the mean of y</span>
    <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># Sum of Squares Error</span>
    <span class="n">SSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Total Sum of Squares</span>
    <span class="n">SST</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">SSE</span> <span class="o">/</span> <span class="n">SST</span><span class="p">)</span>

<span class="c1"># Calculate R-squared</span>
<span class="n">r_squared</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">r_squared</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.7726
</pre></div>
</div>
</div>
</div>
<p>Using sklearn’s built-in <code class="docutils literal notranslate"><span class="pre">r2_score</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="n">r_squared</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">r_squared</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.7726
</pre></div>
</div>
</div>
</div>
</section>
<section id="mean-squared-error-mse">
<h2><span class="section-number">9.1.4. </span>Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this heading">#</a></h2>
<p>The Mean Squared Error (MSE) is a fundamental metric used to quantify the goodness of fit of a regression model by measuring the average squared difference between the predicted values and the actual values of the dependent variable. It provides a measure of how well the model’s predictions align with the observed data <span id="id4">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>Mathematically, the MSE is calculated using the following formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c4c34182-bee7-4ada-adb7-64c2a0ff9da4">
<span class="eqno">(9.3)<a class="headerlink" href="#equation-c4c34182-bee7-4ada-adb7-64c2a0ff9da4" title="Permalink to this equation">#</a></span>\[\begin{equation}
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}\]</div>
<p>Here are the components in the formula:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> represents the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> signifies the actual value of the dependent variable for the <span class="math notranslate nohighlight">\( i \)</span>-th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> corresponds to the predicted value of the dependent variable for the <span class="math notranslate nohighlight">\( i \)</span>-th data point based on the regression model.</p></li>
</ul>
<p>Interpreting the MSE:</p>
<ul class="simple">
<li><p>The MSE measures the average squared difference between the predicted and actual values.</p></li>
<li><p>A lower MSE indicates that the model’s predictions are closer to the actual values, implying a better fit.</p></li>
<li><p>A higher MSE suggests that the model’s predictions deviate more from the actual values, indicating a poorer fit.</p></li>
</ul>
<p>MSE is commonly used as a loss function to assess the performance of regression models during training. It plays a crucial role in model selection and comparison, helping to identify the model that provides the best balance between predictive accuracy and generalization.</p>
<p>It’s important to note that while MSE is a valuable metric for evaluating model performance, it should be interpreted in conjunction with other metrics and diagnostic tools to ensure a comprehensive understanding of the model’s strengths and limitations.</p>
<p><font color='Blue'><b>Example:</b></font> Considering the data from the previous instance, we have</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                            <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;#e483b3&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;#a94375&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\dfrac{(y - \hat</span><span class="si">{y}</span><span class="s1">)^2}</span><span class="si">{n}</span><span class="s1">$&#39;</span><span class="p">,</span>
            <span class="n">yscale</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span>
            <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e45ee5b26cf6bf29cf59e9d4867689826e29cdddddd72a66694e3d4ce026c82e.png" src="../_images/e45ee5b26cf6bf29cf59e9d4867689826e29cdddddd72a66694e3d4ce026c82e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">MSE_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
    <span class="c1"># Sum of Squares Error</span>
    <span class="n">SSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SSE</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Calculate the Mean Squared Error to assess the goodness of fit</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">MSE_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.6836
</pre></div>
</div>
</div>
</div>
<p>Using sklearn’s built-in <code class="docutils literal notranslate"><span class="pre">mean_squared_error</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="c1"># Calculate the Mean Squared Error to assess the goodness of fit</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.6836
</pre></div>
</div>
</div>
</div>
</section>
<section id="mean-absolute-error-mae">
<h2><span class="section-number">9.1.5. </span>Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this heading">#</a></h2>
<p>The Mean Absolute Error (MAE) is a fundamental metric used in regression analysis to assess the accuracy of a predictive model by measuring the average absolute differences between the predicted values and the actual values of the dependent variable. It provides insight into how closely the model’s predictions align with the observed data <span id="id5">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>Mathematically, the MAE is calculated as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6dfe6e3f-ce86-43ec-9c41-f0f055f0700d">
<span class="eqno">(9.4)<a class="headerlink" href="#equation-6dfe6e3f-ce86-43ec-9c41-f0f055f0700d" title="Permalink to this equation">#</a></span>\[\begin{equation}
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}\]</div>
<p>Here are the components in the formula:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> represents the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> denotes the actual value of the dependent variable for the <span class="math notranslate nohighlight">\( i \)</span>-th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> represents the predicted value of the dependent variable for the <span class="math notranslate nohighlight">\( i \)</span>-th data point based on the regression model.</p></li>
</ul>
<p>Interpreting the MAE:</p>
<ul class="simple">
<li><p>The MAE measures the average absolute difference between the predicted and actual values.</p></li>
<li><p>A lower MAE indicates that the model’s predictions are closer to the actual values, suggesting a better fit.</p></li>
<li><p>A higher MAE implies that the model’s predictions have larger absolute differences from the actual values, indicating a poorer fit.</p></li>
</ul>
<p>MAE is often used in combination with other metrics to evaluate the performance of regression models, especially when dealing with outliers or when you want to penalize large prediction errors linearly. It provides valuable insights into the model’s predictive accuracy and is a useful tool for model selection and comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">markerline</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span>
                            <span class="n">linefmt</span><span class="o">=</span><span class="s1">&#39;#d45353&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">markerline</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;#7f1717&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span>
            <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\dfrac{|y - \hat</span><span class="si">{y}</span><span class="s1">|}</span><span class="si">{n}</span><span class="s1">$&#39;</span><span class="p">,</span>
            <span class="n">yscale</span><span class="o">=</span><span class="s1">&#39;log&#39;</span><span class="p">,</span>
            <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e68011b3f3afc98274886766e3d115a11fab5ab29590d8c17e71fdc853556e30.png" src="../_images/e68011b3f3afc98274886766e3d115a11fab5ab29590d8c17e71fdc853556e30.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">MAE_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
    <span class="c1"># Sum of Squares Error</span>
    <span class="n">SSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">SSE</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Calculate the Mean Absolute Error to assess the goodness of fit</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">MAE_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.6119
</pre></div>
</div>
</div>
</div>
<p>Using sklearn’s built-in <code class="docutils literal notranslate"><span class="pre">mean_absolute_error</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="c1"># Calculate the Mean Absolute Error to assess the goodness of fit</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;R-squared = </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared = 0.6119
</pre></div>
</div>
</div>
</div>
</section>
<section id="correlation-matrix">
<h2><span class="section-number">9.1.6. </span>Correlation Matrix<a class="headerlink" href="#correlation-matrix" title="Permalink to this heading">#</a></h2>
<p>The correlation matrix is a square matrix that displays the correlations between different variables (parameters) in a given dataset. Each element in the matrix represents the correlation coefficient between two variables. The correlation coefficient measures the strength and direction of the linear relationship between two variables. It ranges from -1 to +1, with -1 indicating a perfect negative linear relationship, +1 indicating a perfect positive linear relationship, and 0 indicating no linear relationship <span id="id6">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>The correlation coefficient between two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be calculated using the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0a2ec465-5df7-4230-8bb5-22ca32e7ef67">
<span class="eqno">(9.5)<a class="headerlink" href="#equation-0a2ec465-5df7-4230-8bb5-22ca32e7ef67" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{Cor}(X, Y) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
\end{align}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> are the values of the <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> variables, respectively, for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\bar{y}\)</span> are the mean values of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> variables, respectively.</p></li>
</ul>
<p>The correlation matrix provides a convenient way to visualize the relationships between multiple variables in a dataset. It helps identify patterns of association and potential multicollinearity (high correlation between predictor variables) in regression models. A positive correlation coefficient indicates that the variables tend to increase or decrease together, while a negative correlation coefficient indicates an inverse relationship. A correlation coefficient close to 0 suggests little or no linear relationship between the variables.</p>
<p><font color='Blue'><b>Example - Iris Flower Pair Plot:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the Iris dataset and format column titles and species names</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;iris&#39;</span><span class="p">)</span>
<span class="n">iris</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">iris</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">iris</span><span class="o">.</span><span class="n">Species</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">Species</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">title</span><span class="p">())</span>

<span class="c1"># Display the formatted dataset</span>
<span class="n">display</span><span class="p">(</span><span class="n">iris</span><span class="p">)</span>

<span class="c1"># Create a 2x2 grid of subplots with shared axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="c1"># Remove the last subplot</span>
<span class="n">fig</span><span class="o">.</span><span class="n">delaxes</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># List of species (Using only the first three species)</span>
<span class="n">species_list</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()[:</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># Calculate and display the correlation matrices for each species</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">species</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">species_list</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
    <span class="c1"># Extract data for the current species</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
    
    <span class="c1"># Calculate the correlation matrix for the subset</span>
    <span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
        
    <span class="c1"># Create a heatmap for the correlation matrix on the respective subplot</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.2f&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlGn&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
    
    <span class="c1"># Set title for the subplot with proper formatting</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Correlation Matrix for </span><span class="si">{</span><span class="n">species</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    
    <span class="c1"># Disable grid lines</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">visible</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Add a single color bar outside the subplots</span>
<span class="n">cax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>  <span class="c1"># Define the position and size of the color bar</span>
<span class="n">sm</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlGn&#39;</span><span class="p">)</span>
<span class="n">sm</span><span class="o">.</span><span class="n">set_array</span><span class="p">([])</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">sm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Correlation&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal Length</th>
      <th>Sepal Width</th>
      <th>Petal Length</th>
      <th>Petal Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div></div><img alt="../_images/c4c33aca392930b8b6789ca30ba2be946b5afc2b6467c36b554535a0ccf1f160.png" src="../_images/c4c33aca392930b8b6789ca30ba2be946b5afc2b6467c36b554535a0ccf1f160.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Create a 2x2 grid of subplots with shared axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Define species and corresponding variable pairs</span>
<span class="n">species</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Setosa&#39;</span><span class="p">,</span> <span class="s1">&#39;Versicolor&#39;</span><span class="p">,</span> <span class="s1">&#39;Versicolor&#39;</span><span class="p">,</span> <span class="s1">&#39;Virginica&#39;</span><span class="p">]</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Sepal Width&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Petal Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Sepal Length&#39;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;Petal Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Petal Width&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Petal Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Sepal Length&#39;</span><span class="p">)]</span>

<span class="c1"># Loop through species and variables</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">species</span><span class="p">):</span>
    <span class="c1"># Select subset of data for the current species</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">spec</span><span class="p">]</span>
    <span class="n">x_var</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">=</span> <span class="n">variables</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="c1"># Create a regression plot for the selected variables</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">subset</span><span class="p">[</span><span class="n">x_var</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">subset</span><span class="p">[</span><span class="n">y_var</span><span class="p">],</span> <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s1">&#39;fc&#39;</span> <span class="p">:</span> <span class="s1">&#39;#7fbdd4&#39;</span><span class="p">,</span> <span class="s1">&#39;ec&#39;</span><span class="p">:</span> <span class="s1">&#39;#335f8a&#39;</span><span class="p">},</span>
                <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">},</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="c1"># Calculate and display the Pearson correlation coefficient</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="n">x_var</span><span class="p">],</span> <span class="n">subset</span><span class="p">[</span><span class="n">y_var</span><span class="p">])</span><span class="o">.</span><span class="n">statistic</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">x_var</span><span class="si">}</span><span class="s1"> vs. </span><span class="si">{</span><span class="n">y_var</span><span class="si">}</span><span class="s1"> for </span><span class="si">{</span><span class="n">spec</span><span class="si">}</span><span class="s1">. Corr = </span><span class="si">{</span><span class="n">corr</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Adjust subplot layout for better presentation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dfa2ed75abbfc1ecd797891093933c400792516dd5e6461f17c6572b2722289c.png" src="../_images/dfa2ed75abbfc1ecd797891093933c400792516dd5e6461f17c6572b2722289c.png" />
</div>
</div>
<p>The relationship between the heatmap of linear correlation and the plot above lies in their shared objective of visualizing statistical relationships within a dataset. The plot presented here demonstrates scatterplots with regression lines, showcasing how two variables interact within different species of iris flowers. In contrast, a heatmap of linear correlation provides a comprehensive overview of the entire dataset, displaying the correlation coefficients between all pairs of variables in a matrix format. While the plot focuses on specific pairwise relationships within distinct species, the heatmap offers a broader perspective by highlighting the strength and direction of correlations across the entire dataset. Both visualization techniques are valuable tools in exploratory data analysis, with the scatterplot plot offering insights into individual relationships, and the heatmap providing a global view of correlation patterns within the data.</p>
</section>
<section id="confusion-matrix">
<h2><span class="section-number">9.1.7. </span>Confusion Matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this heading">#</a></h2>
<p>The confusion matrix is a fundamental tool used to evaluate the performance of classification algorithms by comparing their predictions against actual outcomes. It’s especially important for understanding the types of errors a model makes. To explain the math behind the confusion matrix, let’s consider a binary classification scenario, where there are two classes: “Positive” (P) and “Negative” (N). In this context, the confusion matrix is a 2x2 matrix with four entries <span id="id7">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<figure class="align-center" id="id25">
<a class="reference internal image-reference" href="../_images/CM.png"><img alt="../_images/CM.png" src="../_images/CM.png" style="width: 420px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9.3 </span><span class="caption-text">Visual representation of a Confusion Matrix.</span><a class="headerlink" href="#id25" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Here’s how each of these terms is defined:</p>
<ul class="simple">
<li><p><strong>True Positives (TP)</strong>: The number of instances that are actually positive (P) and are correctly predicted as positive by the classification algorithm.</p></li>
<li><p><strong>False Positives (FP)</strong>: The number of instances that are actually negative (N) but are incorrectly predicted as positive (P) by the algorithm.</p></li>
<li><p><strong>True Negatives (TN)</strong>: The number of instances that are actually negative (N) and are correctly predicted as negative by the algorithm.</p></li>
<li><p><strong>False Negatives (FN)</strong>: The number of instances that are actually positive (P) but are incorrectly predicted as negative (N) by the algorithm.</p></li>
</ul>
<p>These elements provide valuable information about the performance of a classification model. They can be used to calculate various metrics like accuracy, precision, recall (sensitivity), specificity, and F1-score, which provide deeper insights into the model’s effectiveness in different aspects.</p>
<p>Keep in mind that the terms TP, FP, TN, and FN can have different interpretations depending on the context of the problem. For instance, in medical diagnostics, a false negative (FN) might mean failing to identify a disease in a patient, which could have significant consequences. Understanding the confusion matrix helps in understanding the strengths and weaknesses of a classification model in real-world applications.</p>
<p><font color='Blue'><b>Confusion Matrix Example in Environmental Classification: Water Pollution Detection (Fictional):</b></font>
Consider the following fictional example of a confusion matrix in the context of environmental classification, specifically applied to the detection of pollution in water samples:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p></p></th>
<th class="head text-center"><p><strong>Actual Polluted Water</strong></p></th>
<th class="head text-center"><p><strong>Actual Clean Water</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><strong>Predicted Polluted Water</strong></p></td>
<td class="text-center"><p>85</p></td>
<td class="text-center"><p>10</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>Predicted Clean Water</strong></p></td>
<td class="text-center"><p>15</p></td>
<td class="text-center"><p>120</p></td>
</tr>
</tbody>
</table>
<p>In this confusion matrix:</p>
<ul class="simple">
<li><p>“Clean Water” represents water samples that are truly clean and free from pollution.</p></li>
<li><p>“Polluted Water” represents water samples that are contaminated or polluted.</p></li>
<li><p>“Predicted Clean” represents water samples that a pollution detection system has correctly classified as clean.</p></li>
<li><p>“Predicted Polluted” represents water samples that a pollution detection system has correctly classified as polluted.</p></li>
</ul>
<p>Now, let’s interpret the numbers:</p>
<ul class="simple">
<li><p>There are 120 water samples that are truly clean (Clean Water) and were correctly classified as clean (Predicted Clean).</p></li>
<li><p>There are 15 water samples that are polluted (Polluted Water) but were incorrectly classified as clean (Predicted Clean).</p></li>
<li><p>There are 10 water samples that are truly clean (Clean Water) but were incorrectly classified as polluted (Predicted Polluted).</p></li>
<li><p>There are 85 water samples that are polluted (Polluted Water) and were correctly classified as polluted (Predicted Polluted).</p></li>
</ul>
<p>In the context of the provided confusion matrix, which is used to evaluate the performance of a pollution detection system in water samples, we can interpret the terms TP (True Positives), FP (False Positives), TN (True Negatives), and FN (False Negatives) as follows:</p>
<ol class="arabic simple">
<li><p><strong>True Positives (TP):</strong></p>
<ul class="simple">
<li><p>TP represents the cases where the pollution detection system correctly predicted that water samples were polluted (Predicted Polluted) and indeed, they were polluted (Actual Polluted Water).</p></li>
<li><p>In this example, there are 85 instances where the system correctly identified polluted water samples.</p></li>
</ul>
</li>
<li><p><strong>False Positives (FP):</strong></p>
<ul class="simple">
<li><p>FP represents the cases where the pollution detection system incorrectly predicted that water samples were polluted (Predicted Polluted), but in reality, they were clean (Actual Clean Water).</p></li>
<li><p>There are 10 instances where the system incorrectly classified clean water samples as polluted.</p></li>
</ul>
</li>
<li><p><strong>True Negatives (TN):</strong></p>
<ul class="simple">
<li><p>TN represents the cases where the pollution detection system correctly predicted that water samples were clean (Predicted Clean) and indeed, they were clean (Actual Clean Water).</p></li>
<li><p>In this example, there are 120 instances where the system correctly identified clean water samples.</p></li>
</ul>
</li>
<li><p><strong>False Negatives (FN):</strong></p>
<ul class="simple">
<li><p>FN represents the cases where the pollution detection system incorrectly predicted that water samples were clean (Predicted Clean), but in reality, they were polluted (Actual Polluted Water).</p></li>
<li><p>There are 15 instances where the system failed to detect pollution in water samples that were actually polluted.</p></li>
</ul>
</li>
</ol>
</section>
<section id="accuracy-evaluating-correct-classifications">
<h2><span class="section-number">9.1.8. </span>Accuracy: Evaluating Correct Classifications<a class="headerlink" href="#accuracy-evaluating-correct-classifications" title="Permalink to this heading">#</a></h2>
<p>Accuracy is a metric that quantifies the ratio of correctly classified instances to the total predictions made by a model. It’s determined using the elements of the confusion matrix <span id="id8">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b2de3a38-c4ac-4945-b55b-6ac41343105b">
<span class="eqno">(9.6)<a class="headerlink" href="#equation-b2de3a38-c4ac-4945-b55b-6ac41343105b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}\]</div>
<p>While commonly employed, accuracy may lead to misinterpretations, particularly in cases of imbalanced datasets. In such scenarios, where one class dominates, high accuracy can be achieved even when the model struggles to discern the minority class. Therefore, while accuracy provides an overall view of model performance, it’s advisable to complement it with additional metrics, especially when dealing with imbalanced data distributions.</p>
<p><font color='Blue'><b>Example:</b></font>
In the context of the Water Pollution Detection example, accuracy is computed using the following formula:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation*}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>TP (True Positives) denotes instances where the pollution detection system correctly predicted “Polluted Water” (85).</p></li>
<li><p>TN (True Negatives) represents instances where the system correctly predicted “Clean Water” (120).</p></li>
<li><p>FP (False Positives) correspond to instances where the system incorrectly predicted “Polluted Water” when it was actually “Clean Water” (10).</p></li>
<li><p>FN (False Negatives) denote instances where the system incorrectly predicted “Clean Water” when it was actually “Polluted Water” (15).</p></li>
</ul>
<p>Utilizing these values in the formula, we compute the accuracy as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\text{Accuracy} = \frac{85 + 120}{85 + 120 + 10 + 15} = \frac{205}{230} \approx 0.8913
\end{equation*}\]</div>
<p>Thus, the accuracy of the pollution detection system in this example is approximately 0.8913 or 89.13%.</p>
</section>
<section id="precision">
<h2><span class="section-number">9.1.9. </span>Precision<a class="headerlink" href="#precision" title="Permalink to this heading">#</a></h2>
<p>Precision is a metric that measures the accuracy of positive predictions generated by a model, taking false positives into account. It’s derived from the confusion matrix elements <span id="id9">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-43b91dd7-968f-420e-823d-d4f1298603f5">
<span class="eqno">(9.7)<a class="headerlink" href="#equation-43b91dd7-968f-420e-823d-d4f1298603f5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}\]</div>
<p>Precision gains significance in situations where the implications of false positives are significant. In scenarios where mistakenly predicting a positive outcome has notable consequences, precision serves as a vital indicator of a model’s ability to avoid making erroneous positive predictions.</p>
<p><font color='Blue'><b>Example:</b></font> In the context of the Water Pollution Detection example,</p>
<ul class="simple">
<li><p><strong>Precision for Polluted Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Precision} = \frac{TP}{TP + FP}  = \frac{85}{85 + 10} = \frac{85}{95} \approx 0.8947 \end{equation*}\]</div>
<ul class="simple">
<li><p><strong>Precision for Clean Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Precision} = \frac{TN}{TN + FN} = \frac{120}{120 + 15} = \frac{120}{135} \approx 0.8889 \end{equation*}\]</div>
</section>
<section id="recall-sensitivity-or-true-positive-rate">
<h2><span class="section-number">9.1.10. </span>Recall (Sensitivity or True Positive Rate)<a class="headerlink" href="#recall-sensitivity-or-true-positive-rate" title="Permalink to this heading">#</a></h2>
<p>Recall, also known as sensitivity or the true positive rate, quantifies a model’s capacity to identify all positive instances, even when considering false negatives. This metric is calculated using the confusion matrix elements <span id="id10">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-65ee844c-6da6-4573-9218-c8728da6c707">
<span class="eqno">(9.8)<a class="headerlink" href="#equation-65ee844c-6da6-4573-9218-c8728da6c707" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}\]</div>
<p>Recall holds paramount importance in situations where the avoidance of false negatives is critical. When failing to detect positive cases can have significant repercussions, recall serves as a pivotal indicator of a model’s effectiveness in recognizing positive instances, ensuring minimal instances are overlooked.</p>
<p><font color='Blue'><b>Example:</b></font> In the context of the Water Pollution Detection example,</p>
<ul class="simple">
<li><p><strong>Recall for Polluted Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Recall} = \frac{TP}{TP + FN} =  \frac{85}{85 + 15} = \frac{85}{100} = 0.85 \end{equation*}\]</div>
<ul class="simple">
<li><p><strong>Recall for Clean Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Recall} = \frac{TN}{TN + FP} = \frac{120}{120 + 10} = \frac{120}{130} \approx 0.9231 \end{equation*}\]</div>
</section>
<section id="f1-score">
<h2><span class="section-number">9.1.11. </span>F1-Score<a class="headerlink" href="#f1-score" title="Permalink to this heading">#</a></h2>
<p>The F1-Score presents a harmonious equilibrium between precision and recall, while accounting for both false positives and false negatives. It’s calculated using the confusion matrix elements <span id="id11">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4cc5a29f-aff8-40e8-9b9d-fc3238ef4c84">
<span class="eqno">(9.9)<a class="headerlink" href="#equation-4cc5a29f-aff8-40e8-9b9d-fc3238ef4c84" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{F1-Score} = 2 \times  \frac{\text{Precision} \times \text{Recall}}{\text{Precision + Recall}}
\end{equation}\]</div>
<p>When a trade-off situation emerges between precision and recall, the F1-Score serves as a valuable indicator. In scenarios where achieving high precision might lower recall or vice versa, the F1-Score provides comprehensive insight into a model’s performance by considering the balance between these two vital aspects of prediction accuracy.</p>
<p><font color='Blue'><b>Example:</b></font>
In the context of the Water Pollution Detection example,</p>
<ul class="simple">
<li><p><strong>F1 Score for Polluted Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-b712add7-3e07-4c33-b4db-fa4148ee7d19">
<span class="eqno">(9.10)<a class="headerlink" href="#equation-b712add7-3e07-4c33-b4db-fa4148ee7d19" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}  = 2 \cdot \frac{0.8947 \cdot 0.85}{0.8947 + 0.85} \approx 0.8711 \end{equation}\]</div>
<ul class="simple">
<li><p><strong>F1 Score for Clean Water:</strong></p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-4fe7607d-efbf-47b2-8492-cdb3a6fc35ec">
<span class="eqno">(9.11)<a class="headerlink" href="#equation-4fe7607d-efbf-47b2-8492-cdb3a6fc35ec" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}  = 2 \cdot \frac{0.8889 \cdot 0.9231}{0.8889 + 0.9231} \approx 0.9056 \end{equation}\]</div>
</section>
<section id="balanced-accuracy">
<h2><span class="section-number">9.1.12. </span>Balanced Accuracy<a class="headerlink" href="#balanced-accuracy" title="Permalink to this heading">#</a></h2>
<p>Balanced Accuracy is a metric that addresses the challenges posed by imbalanced datasets, where one class significantly outweighs the others. It calculates the average accuracy for each class, providing a more reliable measure of overall model performance <span id="id12">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>The Balanced Accuracy is calculated as the average of the true positive rate (Recall) for each class:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4b04a2ee-729e-48ef-9abb-75e2a995d083">
<span class="eqno">(9.12)<a class="headerlink" href="#equation-4b04a2ee-729e-48ef-9abb-75e2a995d083" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Balanced Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \frac{TP_i}{TP_i + FN_i}
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(TP_i\)</span> is the true positive count for class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(FN_i\)</span> is the false negative count for class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>Balanced Accuracy takes into account the individual class performance, making it particularly useful for evaluating models in scenarios where class distribution is skewed. It offers a fairer assessment of how well a model performs across all classes, providing a more accurate representation of its effectiveness on imbalanced datasets.</p>
<p>In binary classification scenarios, Balanced Accuracy can be defined as the average of two key metrics: sensitivity (the true positive rate) and specificity (the true negative rate). Alternatively, it can be likened to the area under the Receiver Operating Characteristic (ROC) curve when using binary predictions instead of continuous scores. Mathematically, this can be expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-59dfb02a-ef94-4619-a970-01ef6ff76d3a">
<span class="eqno">(9.13)<a class="headerlink" href="#equation-59dfb02a-ef94-4619-a970-01ef6ff76d3a" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Balanced Accuracy} = \frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right) \end{equation}\]</div>
<p>This formula captures the balanced evaluation of a binary classification model’s ability to correctly identify both positive and negative instances while considering the trade-off between them.</p>
<p><font color='Blue'><b>Example:</b></font> In the context of the Water Pollution Detection example,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{Balanced Accuracy} &amp;= \frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right) \\
&amp;= \frac{\text{Recall for Clean Water} + \text{Recall for Polluted Water}}{2} \\
&amp;= \frac{1}{2} \left( \frac{0.9231 + 0.85}{2} \right) \approx 0.8866.
\end{align*}\]</div>
</section>
<section id="matthews-correlation-coefficient-mcc">
<h2><span class="section-number">9.1.13. </span>Matthews Correlation Coefficient (MCC)<a class="headerlink" href="#matthews-correlation-coefficient-mcc" title="Permalink to this heading">#</a></h2>
<p>The Matthews Correlation Coefficient (MCC) is a metric used to assess the quality of binary classification models. It takes into account true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) to provide a balanced evaluation of classification performance, especially when dealing with imbalanced datasets <span id="id13">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>MCC ranges from -1 to +1, where:</p>
<ul class="simple">
<li><p>+1 indicates perfect prediction.</p></li>
<li><p>0 indicates no better than random prediction.</p></li>
<li><p>-1 indicates perfect disagreement between predictions and actual classes.</p></li>
</ul>
<p>Mathematically, MCC is calculated as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e3b558e1-cf6c-4ad6-b20c-cb14290fe968">
<span class="eqno">(9.14)<a class="headerlink" href="#equation-e3b558e1-cf6c-4ad6-b20c-cb14290fe968" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{MCC} = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
\end{equation}\]</div>
<p>Here’s how the components of the formula contribute:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(TP \times TN\)</span> term rewards correct positive and negative predictions.</p></li>
<li><p><span class="math notranslate nohighlight">\(FP \times FN\)</span> term penalizes the count of false positive and false negative predictions.</p></li>
<li><p>The denominator normalizes the score, accounting for class distribution.</p></li>
</ul>
<p>Key points about MCC:</p>
<ol class="arabic simple">
<li><p><strong>Balanced Metric</strong>: MCC is suitable for imbalanced datasets because it considers TP, TN, FP, and FN, providing a balanced evaluation.</p></li>
<li><p><strong>Symmetry</strong>: MCC is symmetric, meaning that swapping the positive and negative classes doesn’t affect the score.</p></li>
<li><p><strong>Sensitive to Class Imbalance</strong>: Unlike accuracy, MCC isn’t easily misled by imbalanced data.</p></li>
<li><p><strong>Usefulness</strong>: MCC helps identify how well the model’s predictions align with the actual classes, considering all four possible outcomes of binary classification.</p></li>
</ol>
<p><font color='Blue'><b>Example:</b></font>
We also can calculate the Matthews Correlation Coefficient (MCC) for the Water Pollution Detection example as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">calculate_mcc</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="p">(</span><span class="n">tp</span> <span class="o">*</span> <span class="n">tn</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">fp</span> <span class="o">*</span> <span class="n">fn</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fn</span><span class="p">))</span>
    
    <span class="n">mcc</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span> <span class="k">if</span> <span class="n">denominator</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>
    
    <span class="k">return</span> <span class="n">mcc</span>

<span class="n">mcc</span> <span class="o">=</span> <span class="n">calculate_mcc</span><span class="p">(</span><span class="n">tp</span><span class="o">=</span><span class="mi">85</span><span class="p">,</span> <span class="n">tn</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">fp</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matthews Correlation Coefficient (MCC): </span><span class="si">{</span><span class="n">mcc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Matthews Correlation Coefficient (MCC): 0.7783
</pre></div>
</div>
</div>
</div>
</section>
<section id="cohens-kappa">
<h2><span class="section-number">9.1.14. </span>Cohen’s Kappa<a class="headerlink" href="#cohens-kappa" title="Permalink to this heading">#</a></h2>
<p>Cohen’s Kappa, often referred to simply as Kappa, is a statistic used to measure the level of agreement between two raters or evaluators, especially in situations involving categorical data or classification tasks. It’s particularly useful when assessing the reliability or consistency of human judgments or the agreement between human judgments and machine predictions <span id="id14">[<a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>Kappa takes into account the agreement that could occur by chance and provides a normalized score that indicates the extent to which the observed agreement between raters or evaluators is beyond what could be expected by chance alone.</p>
<p>The formula for Cohen’s Kappa is as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e624f4fa-366d-47c5-9d90-477b16656205">
<span class="eqno">(9.15)<a class="headerlink" href="#equation-e624f4fa-366d-47c5-9d90-477b16656205" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Kappa} = \frac{P_o - P_e}{1 - P_e}
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P_o \)</span> is the observed proportion of agreement between the raters or evaluators.</p></li>
<li><p><span class="math notranslate nohighlight">\( P_e \)</span> is the proportion of agreement expected by chance, calculated as the product of the marginal frequencies of the categories being rated.</p></li>
</ul>
<p>Key points about Cohen’s Kappa:</p>
<ol class="arabic simple">
<li><p><strong>Range of Values</strong>: Kappa values range from -1 to +1.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( +1 \)</span>: Perfect agreement between raters.</p></li>
<li><p><span class="math notranslate nohighlight">\( 0 \)</span>: Agreement equivalent to chance.</p></li>
<li><p><span class="math notranslate nohighlight">\( -1 \)</span>: Complete disagreement between raters.</p></li>
</ul>
</li>
<li><p><strong>Adjustment for Chance</strong>: Kappa adjusts for agreement that could occur by random chance.</p></li>
<li><p><strong>Interpretation</strong>: The interpretation of Kappa values varies, but typically:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( 0.2 \)</span> or less: Poor agreement.</p></li>
<li><p><span class="math notranslate nohighlight">\( 0.21 - 0.40 \)</span>: Fair agreement.</p></li>
<li><p><span class="math notranslate nohighlight">\( 0.41 - 0.60 \)</span>: Moderate agreement.</p></li>
<li><p><span class="math notranslate nohighlight">\( 0.61 - 0.80 \)</span>: Substantial agreement.</p></li>
<li><p><span class="math notranslate nohighlight">\( 0.81 - 1.00 \)</span>: Almost perfect agreement.</p></li>
</ul>
</li>
<li><p><strong>Considerations</strong>: Cohen’s Kappa is sensitive to the distribution of categories and can be affected by the prevalence of a particular category.</p></li>
</ol>
<p>Cohen’s Kappa is a valuable tool in fields such as inter-rater reliability studies, medical diagnoses, and social sciences, where assessing agreement among raters is crucial. It provides insight into the reliability of judgments beyond raw agreement percentages and helps ensure the consistency and quality of human or machine-based evaluations.</p>
<p>In the context of a binary classification confusion matrix used in machine learning and statistics, Cohen’s Kappa formula can be expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a1570a2d-2aa0-40f7-bd02-7eb3ef7d3207">
<span class="eqno">(9.16)<a class="headerlink" href="#equation-a1570a2d-2aa0-40f7-bd02-7eb3ef7d3207" title="Permalink to this equation">#</a></span>\[\begin{equation}
\kappa = \frac{2 \times (TP \times TN - FN \times FP)}{(TP + FP) \times (FP + TN) + (TP + FN) \times (FN + TN)}
\end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>TP represents the true positives.</p></li>
<li><p>FP represents the false positives.</p></li>
<li><p>TN represents the true negatives.</p></li>
<li><p>FN represents the false negatives.</p></li>
</ul>
<p>Cohen’s Kappa is a metric used to assess the agreement between observed and expected classification results in binary classification scenarios. It quantifies the level of agreement beyond what would be expected by chance. Notably, in this context, Cohen’s Kappa is equivalent to the Heidke skill score used in Meteorology. This measure has a historical origin, as it was first introduced by Myrick Haskell Doolittle in 1888.</p>
<p><font color='Blue'><b>Example:</b></font> In the Water Pollution Detection example, we can calculate Cohen’s Kappa as follows:</p>
<p><strong>Total Number of Samples (<span class="math notranslate nohighlight">\(N\)</span>):</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-b220180f-e6cc-407a-8def-ab57f27d8af9">
<span class="eqno">(9.17)<a class="headerlink" href="#equation-b220180f-e6cc-407a-8def-ab57f27d8af9" title="Permalink to this equation">#</a></span>\[\begin{equation}
N = \text{TP} + \text{TN} + \text{FP} + \text{FN} = 85 + 120 + 10 + 15 = 230
\end{equation}\]</div>
<p><strong>Observed Agreement (<span class="math notranslate nohighlight">\(po\)</span>):</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-23c7f827-8a52-4970-925a-ae036fa0a26f">
<span class="eqno">(9.18)<a class="headerlink" href="#equation-23c7f827-8a52-4970-925a-ae036fa0a26f" title="Permalink to this equation">#</a></span>\[\begin{equation}
po = \frac{\text{TP} + \text{TN}}{N} = \frac{85 + 120}{230} = \frac{205}{230}
\end{equation}\]</div>
<p><strong>Probabilities of Random Agreement (<span class="math notranslate nohighlight">\(pe_{\text{polluted}}, pe_{\text{clean}}\)</span>):</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-a1cd7770-f86b-4007-93a2-df326f7ac084">
<span class="eqno">(9.19)<a class="headerlink" href="#equation-a1cd7770-f86b-4007-93a2-df326f7ac084" title="Permalink to this equation">#</a></span>\[\begin{equation}
pe_{\text{polluted}} = \frac{(\text{TP} + \text{FP}) \times (\text{TP} + \text{FN})}{N^2} = \frac{(85 + 10) \times (85 + 15)}{230^2}
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-cc6e92c5-d228-4daf-a14d-9e97b56e1872">
<span class="eqno">(9.20)<a class="headerlink" href="#equation-cc6e92c5-d228-4daf-a14d-9e97b56e1872" title="Permalink to this equation">#</a></span>\[\begin{equation}
pe_{\text{clean}} = \frac{(\text{TN} + \text{FP}) \times (\text{TN} + \text{FN})}{N^2} = \frac{(120 + 10) \times (120 + 15)}{230^2}
\end{equation}\]</div>
<p><strong>Cohen’s Kappa (<span class="math notranslate nohighlight">\(\kappa\)</span>):</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-4fdee67c-20f7-4506-b568-6c861f731688">
<span class="eqno">(9.21)<a class="headerlink" href="#equation-4fdee67c-20f7-4506-b568-6c861f731688" title="Permalink to this equation">#</a></span>\[\begin{equation}
\kappa = \frac{po - (pe_{\text{polluted}} + pe_{\text{clean}})}{1 - (pe_{\text{polluted}} + pe_{\text{clean}})} \approx 0.7776
\end{equation}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_cohens_kappa</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="c1"># Calculate the total number of samples</span>
    <span class="n">total_samples</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">+</span> <span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span> <span class="o">+</span> <span class="n">fn</span>
    
    <span class="c1"># Calculate observed agreement (po)</span>
    <span class="n">po</span> <span class="o">=</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_samples</span>
    
    <span class="c1"># Calculate the probabilities of random agreement for each category</span>
    <span class="n">pe_polluted</span> <span class="o">=</span> <span class="p">((</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_samples</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_samples</span><span class="p">)</span>
    <span class="n">pe_clean</span> <span class="o">=</span> <span class="p">((</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_samples</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">tn</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_samples</span><span class="p">)</span>
    
    <span class="c1"># Calculate Cohen&#39;s Kappa</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="p">(</span><span class="n">po</span> <span class="o">-</span> <span class="p">(</span><span class="n">pe_polluted</span> <span class="o">+</span> <span class="n">pe_clean</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">pe_polluted</span> <span class="o">+</span> <span class="n">pe_clean</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">kappa</span>

<span class="c1"># Define the values (you can replace these with your specific values)</span>
<span class="n">tp</span> <span class="o">=</span> <span class="mi">85</span>
<span class="n">tn</span> <span class="o">=</span> <span class="mi">120</span>
<span class="n">fp</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">fn</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Calculate Cohen&#39;s Kappa using the function</span>
<span class="n">kappa</span> <span class="o">=</span> <span class="n">calculate_cohens_kappa</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cohen&#39;s Kappa: </span><span class="si">{</span><span class="n">kappa</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cohen&#39;s Kappa: 0.7776
</pre></div>
</div>
</div>
</div>
<p>Calculating Cohen’s Kappa for the Water Pollution Detection example using the binary formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2cc74fd2-03c3-448d-bd0f-244ff504c92e">
<span class="eqno">(9.22)<a class="headerlink" href="#equation-2cc74fd2-03c3-448d-bd0f-244ff504c92e" title="Permalink to this equation">#</a></span>\[\begin{align}
\kappa &amp;= \frac{2 \times (TP \times TN - FP \times FN)}{(TP + FP) \times (FP + TN) + (TP + FN) \times (FN + TN)} \\
&amp;= \frac{2 \times (85 \times 120 - 10 \times 15)}{(85 + 10) \times (10 + 120) + (85 + 15) \times (15 + 120)} \\
&amp;= \frac{2 \times (10200 - 150)}{(95) \times (130) + (100) \times (135)} \\
&amp;= \frac{2 \times 10050}{12350 + 13500} \\
&amp;= \frac{20100}{25850} \approx 0.7776
\end{align}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_cohens_kappa_alt</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="c1"># Calculate Cohen&#39;s Kappa using the alternative formula</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">tp</span> <span class="o">*</span> <span class="n">tn</span> <span class="o">-</span> <span class="n">fp</span> <span class="o">*</span> <span class="n">fn</span><span class="p">))</span> <span class="o">/</span> <span class="p">((</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">fp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">fn</span> <span class="o">+</span> <span class="n">tn</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">kappa</span>

<span class="c1"># Define the values (you can replace these with your specific values)</span>
<span class="n">tp</span> <span class="o">=</span> <span class="mi">85</span>
<span class="n">tn</span> <span class="o">=</span> <span class="mi">120</span>
<span class="n">fp</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">fn</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Calculate Cohen&#39;s Kappa using the alternative formula and the function</span>
<span class="n">kappa</span> <span class="o">=</span> <span class="n">calculate_cohens_kappa_alt</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cohen&#39;s Kappa: </span><span class="si">{</span><span class="n">kappa</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cohen&#39;s Kappa: 0.7776
</pre></div>
</div>
</div>
</div>
</section>
<section id="scikit-learns-metrics">
<h2><span class="section-number">9.1.15. </span>Scikit-learn’s metrics<a class="headerlink" href="#scikit-learns-metrics" title="Permalink to this heading">#</a></h2>
<p>Scikit-learn’s functions, such as <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score</span></code>, <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.precision_score</span></code>, <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.recall_score</span></code>, and <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.f1_score</span></code>, make use of these fundamental metrics for their computations. By interpreting these metrics within the context of the confusion matrix, practitioners can acquire a more profound insight into their model’s capabilities and limitations. This enhanced understanding empowers them to make informed decisions throughout the various stages of model development and deployment <span id="id15">[<a class="reference internal" href="../References.html#id160" title="F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id163" title="David Powers. Evaluation: from precision, recall and f-measure to ROC, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies, 2(1):37–63, 2011.">Powers, 2011</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Formula or Usage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">accuracy_score</span></code></p></td>
<td><p>Computes accuracy, the ratio of correctly predicted instances to the total instances.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">accuracy_score(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">balanced_accuracy_score</span></code></p></td>
<td><p>Calculates accuracy, considering imbalanced classes by averaging accuracy across classes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">balanced_accuracy_score(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">precision_score</span></code></p></td>
<td><p>Measures the proportion of true positive predictions among all positive predictions.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">precision_score(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">recall_score</span></code></p></td>
<td><p>Computes the proportion of true positive predictions among actual positive instances.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">recall_score(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">f1_score</span></code></p></td>
<td><p>Balances precision and recall, combining them into a single score.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">f1_score(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">specificity_score</span></code></p></td>
<td><p>Measures the proportion of true negative predictions among actual negative instances.</p></td>
<td><p>Calculate sensitivity, then apply <code class="docutils literal notranslate"><span class="pre">specificity_score</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">sensitivity</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">neg_pred_value</span></code></p></td>
<td><p>Calculates the likelihood of true negative predictions among all predicted negatives.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">neg_pred_value</span> <span class="pre">=</span> <span class="pre">TN</span> <span class="pre">/</span> <span class="pre">(TN</span> <span class="pre">+</span> <span class="pre">FN)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">false_positive_rate</span></code></p></td>
<td><p>Computes the proportion of false positive predictions among actual negative instances.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false_positive_rate</span> <span class="pre">=</span> <span class="pre">FP</span> <span class="pre">/</span> <span class="pre">(FP</span> <span class="pre">+</span> <span class="pre">TN)</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">false_negative_rate</span></code></p></td>
<td><p>Measures the rate of false negative predictions among actual positive instances.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false_negative_rate</span> <span class="pre">=</span> <span class="pre">FN</span> <span class="pre">/</span> <span class="pre">(FN</span> <span class="pre">+</span> <span class="pre">TP)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">matthews_corrcoef</span></code></p></td>
<td><p>Incorporates true positives, true negatives, false positives, and false negatives into one value.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">matthews_corrcoef(y_true,</span> <span class="pre">y_pred)</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">cohen_kappa_score</span></code></p></td>
<td><p>Measures agreement between raters or evaluators while considering chance.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cohen_kappa_score(y1,</span> <span class="pre">y2)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>Additional metrics such as AUC-ROC, AUC-PR, log loss, etc.</p></td>
<td><p>Various functions available in <code class="docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="additioal-information-for-statsmodels-api-optional-content">
<h2><span class="section-number">9.1.16. </span>Additioal Information for <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code> (Optional Content)<a class="headerlink" href="#additioal-information-for-statsmodels-api-optional-content" title="Permalink to this heading">#</a></h2>
<p>The terms you’ve mentioned are statistical measures often included in regression analysis output or summary tables. They provide information about various aspects of the model’s goodness of fit, normality of residuals, and potential issues. Let’s delve into each of these metrics:</p>
<section id="standard-error-and-coefficient-precision-in-statistics">
<h3><span class="section-number">9.1.16.1. </span>Standard Error and Coefficient Precision in Statistics<a class="headerlink" href="#standard-error-and-coefficient-precision-in-statistics" title="Permalink to this heading">#</a></h3>
<p>In statistics, the <strong>standard error (SE)</strong> is a way to measure how accurate an estimated value is, like the coefficients in a linear regression. It shows the average amount by which estimates can vary when we take multiple samples from the same group <span id="id16">[<a class="reference internal" href="../References.html#id22" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>]</span>.</p>
<p>For a coefficient estimate <span class="math notranslate nohighlight">\( \beta \)</span> in a linear regression, you can find the standard error using this formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ae2bd7d2-1d11-4e3f-ad9c-56bfc223d435">
<span class="eqno">(9.23)<a class="headerlink" href="#equation-ae2bd7d2-1d11-4e3f-ad9c-56bfc223d435" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{SE}(\beta) = \sqrt{\frac{S_e}{\sum_i (x_i - \bar{x})^2}}
\end{align}\]</div>
<p>Here’s what each part means:</p>
<ul>
<li><p>The standard error of estimate, denoted as <span class="math notranslate nohighlight">\( S_e \)</span>, is computed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5226177a-d656-40eb-8886-03ec99209f10">
<span class="eqno">(9.24)<a class="headerlink" href="#equation-5226177a-d656-40eb-8886-03ec99209f10" title="Permalink to this equation">#</a></span>\[\begin{equation}
    S_e = \sqrt{\text{MSE}} = \sqrt\frac{{\sum_{i=1}^n({Y_i}-\hat{Y})^2}}{{n-2}}
    \end{equation}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( S_e \)</span> is the standard error of estimate, representing the typical variability of data points around the regression line.</p></li>
<li><p>MSE stands for Mean Squared Error, which quantifies the average squared difference between observed values (<span class="math notranslate nohighlight">\( Y_i \)</span>) and predicted values (<span class="math notranslate nohighlight">\( \hat{Y} \)</span>).</p></li>
<li><p>The denominator <span class="math notranslate nohighlight">\( (n-2) \)</span> takes into account the loss of 2 degrees of freedom. This adjustment compensates for the estimation of both the intercept and the slope in the linear regression model, effectively reflecting the model’s complexity while calculating the standard error.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> is the value of the independent variable for the <span class="math notranslate nohighlight">\( i \)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{x} \)</span> is the average value of the independent variable.</p></li>
</ul>
<p>The standard error helps us understand how reliable our coefficient estimates are. Smaller standard errors mean more accurate coefficients that are less likely to be far off from the true population value.</p>
<p>Scientists often use the standard error to figure out <strong>confidence intervals</strong> for coefficient estimates. A confidence interval gives us a range of values where we can reasonably expect the true population value to be. This is how you usually calculate it:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ec48bbdd-a49a-4328-b5dc-f17e0202b67b">
<span class="eqno">(9.25)<a class="headerlink" href="#equation-ec48bbdd-a49a-4328-b5dc-f17e0202b67b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Confidence Interval} = \text{Coefficient Estimate} \pm \text{Critical Value} \times \text{SE}(\text{Coefficient})
\end{equation}\]</div>
<p>The critical value is decided based on how sure we want to be and the t-statistic distribution.</p>
</section>
<section id="t-value-t-and-p-value-p-t-in-regression-analysis">
<h3><span class="section-number">9.1.16.2. </span>t-value (<span class="math notranslate nohighlight">\(t\)</span>) and p-value (<span class="math notranslate nohighlight">\(P&gt;|t|\)</span>) in Regression Analysis<a class="headerlink" href="#t-value-t-and-p-value-p-t-in-regression-analysis" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><strong>t-value (<span class="math notranslate nohighlight">\(t\)</span>) - Assessing Coefficient Significance:</strong> The t-value, also known as the t-statistic, is a critical statistic in regression analysis that helps us understand the importance of a coefficient estimate. It indicates how many standard errors the estimated coefficient is away from zero. The main goal of the t-value is to examine the hypothesis that the coefficient has no effect, essentially testing whether it is equal to zero within the model <span id="id17">[<a class="reference internal" href="../References.html#id22" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>]</span>.</p>
<p>Mathematically, the t-value is calculated as the ratio of the coefficient estimate to the standard error of that coefficient:</p>
<div class="amsmath math notranslate nohighlight" id="equation-897f0f68-1eaf-4b7e-9d2a-7be7cf85540a">
<span class="eqno">(9.26)<a class="headerlink" href="#equation-897f0f68-1eaf-4b7e-9d2a-7be7cf85540a" title="Permalink to this equation">#</a></span>\[\begin{equation}
    t = \frac{\text{Coefficient Estimate}}{\text{Standard Error of the Coefficient}}
    \end{equation}\]</div>
<p>The t-value plays a key role in determining the statistical significance of a coefficient. A large t-value suggests that the estimated coefficient significantly differs from zero. Conversely, a small t-value indicates that the coefficient might not be statistically distinguishable from zero, suggesting that the variable associated with the coefficient might not have a meaningful impact on the outcome.</p>
<p>The distribution of t-values follows a t-distribution, where the degrees of freedom are determined by the sample size and the number of predictor variables in the regression. A higher t-value, relative to the t-distribution with appropriate degrees of freedom, boosts confidence in the significance of the coefficient estimate. This makes the t-value an essential tool for hypothesis testing in statistical modeling.</p>
</li>
<li><p><strong>p-value (<span class="math notranslate nohighlight">\(P&gt;|t|\)</span>) - Evaluating Statistical Significance</strong>: The p-value associated with the t-value is a critical indicator of the statistical significance of a coefficient in regression analysis. It answers the question: “Is the observed effect (coefficient) statistically significant, or is it likely due to random chance?” The p-value’s calculation is grounded in the principles of the t-distribution, representing the probability of observing a t-value as extreme as the computed one, assuming that the null hypothesis (which assumes a coefficient of zero, implying no effect) is true <span id="id18">[<a class="reference internal" href="../References.html#id22" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>, <a class="reference internal" href="../References.html#id186" title="David W Scott. Scott's rule. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4):497–502, 2010.">Scott, 2010</a>]</span>.</p>
<p>A small p-value, often set at a predetermined significance level (e.g., 0.05), suggests statistical significance. This implies that the variable associated with the coefficient likely has a genuine, non-zero impact on the dependent variable. The observed effect is unlikely to be solely a result of random chance. Conversely, a large p-value suggests that the observed effect could reasonably be attributed to random variability, rather than a real relationship between the variable and the dependent variable. In such cases, the effect is not considered statistically significant.</p>
<p>The p-value serves as a critical tool for hypothesis testing. Researchers and analysts frequently use the p-value to make informed decisions about whether to reject the null hypothesis, concluding coefficient significance, or to fail to reject it, considering the effect as non-significant. Selecting an appropriate significance level (alpha) beforehand helps establish the threshold for determining statistical significance, balancing the risk of Type I errors (incorrectly concluding significance) and Type II errors (incorrectly concluding non-significance). The p-value remains a valuable metric guiding these decisions in statistical modeling and hypothesis testing.</p>
</li>
</ul>
</section>
<section id="omnibus-test">
<h3><span class="section-number">9.1.16.3. </span>Omnibus Test<a class="headerlink" href="#omnibus-test" title="Permalink to this heading">#</a></h3>
<p>Omnibus test is a measure of the overall goodness of fit of the regression model. It assesses whether the model’s residuals (the differences between actual and predicted values) are normally distributed. A significant Omnibus test suggests that the residuals do not follow a normal distribution, which might indicate a violation of one of the assumptions of linear regression.</p>
<p>Here’s how the Omnibus test works <span id="id19">[<a class="reference internal" href="../References.html#id58" title="J. Doornik and H. Hansen. An Omnibus Test for Univariate and Multivariate Normality. SSRN, 2008. URL: https://books.google.ca/books?id=kJ3gzwEACAAJ.">Doornik and Hansen, 2008</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Objective:</strong>
The Omnibus test aims to determine whether the distribution of the residuals is symmetric and bell-shaped, as is the case with a normal distribution. If the residuals closely resemble a normal distribution, it indicates that the model’s assumptions are met and that the linear relationship between the independent variables and the dependent variable is a reasonable approximation.</p></li>
<li><p><strong>Calculation:</strong>
The test combines measures of skewness and kurtosis of the residuals to evaluate their departure from normality. Skewness assesses the symmetry of the distribution, while kurtosis examines the “tailedness” or peak of the distribution.</p></li>
<li><p><strong>Hypotheses:</strong>
The null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) of the Omnibus test is that the residuals are normally distributed. The alternative hypothesis (<span class="math notranslate nohighlight">\(H_1\)</span>) is that the residuals are not normally distributed.</p></li>
<li><p><strong>Test Statistic:</strong>
The Omnibus test statistic is often computed using the Chi-squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) distribution. It is calculated based on the skewness and kurtosis of the residuals and follows a Chi-squared distribution with degrees of freedom equal to 2.</p></li>
<li><p><strong>Interpretation:</strong>
The resulting test statistic is compared to a critical value from the Chi-squared distribution based on the chosen significance level (usually 0.05). If the calculated test statistic exceeds the critical value, the null hypothesis is rejected, indicating that the residuals do not follow a normal distribution. This suggests that the model might not be the best fit for the data, and further investigation or adjustments to the model might be necessary.</p></li>
</ol>
<p><strong>Prob(Omnibus)</strong></p>
<p>“Prob(Omnibus)” is a statistical metric that is often included in the output of regression analysis, particularly when using software packages like statsmodels in Python. It is associated with the Omnibus test, which is a test for the normality of the residuals in a regression model <span id="id20">[<a class="reference internal" href="../References.html#id105" title="R.S. Kenett, S. Zacks, and P. Gedeck. Industrial Statistics: A Computer-Based Approach with Python. Statistics for Industry, Technology, and Engineering. Springer International Publishing, 2023. ISBN 9783031284823. URL: https://books.google.ca/books?id=4M7FEAAAQBAJ.">Kenett <em>et al.</em>, 2023</a>]</span>.</p>
<p>The Omnibus test assesses whether the distribution of the residuals follows a normal distribution. The “Prob(Omnibus)” value, also known as the p-value for the Omnibus test, indicates the probability of observing the calculated Omnibus test statistic (or a more extreme value) if the residuals were drawn from a truly normal distribution. In other words, it quantifies the evidence against the null hypothesis that the residuals are normally distributed.</p>
<p>Here’s how to interpret the “Prob(Omnibus)” value:</p>
<ol class="arabic simple">
<li><p><strong>Low p-value (typically below 0.05):</strong>
A low “Prob(Omnibus)” value suggests that the residuals significantly deviate from a normal distribution. In this case, you might question the assumption of normality and consider potential issues with the model. A low p-value indicates that you have evidence to reject the null hypothesis of normality.</p></li>
<li><p><strong>High p-value (typically above 0.05):</strong>
A high “Prob(Omnibus)” value indicates that there is not enough evidence to reject the null hypothesis of normality. This suggests that the residuals are reasonably consistent with a normal distribution, and the model’s assumption of normality might be valid.</p></li>
</ol>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Perform the Omnibus Test for goodness of fit</span>
<span class="n">omnibus_test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">omni_normtest</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Display the test statistics and p-value</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Omnibus Test Statistics: </span><span class="si">{</span><span class="n">omnibus_test</span><span class="o">.</span><span class="n">statistic</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Omnibus Test p-value: </span><span class="si">{</span><span class="n">omnibus_test</span><span class="o">.</span><span class="n">pvalue</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Interpret the results</span>
<span class="k">if</span> <span class="n">omnibus_test</span><span class="o">.</span><span class="n">pvalue</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are not normally distributed, indicating potential model issues.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are normally distributed, suggesting a better model fit.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Omnibus Test Statistics: 19.2758
Omnibus Test p-value: 0.0001
The residuals are not normally distributed, indicating potential model issues.
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:52   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="durbin-watson-statistic">
<h3><span class="section-number">9.1.16.4. </span>Durbin-Watson Statistic<a class="headerlink" href="#durbin-watson-statistic" title="Permalink to this heading">#</a></h3>
<p>The Durbin-Watson statistic is a numerical measure used to detect the presence of autocorrelation (serial correlation) in the residuals of a regression model, particularly in time series or panel data analysis. Autocorrelation refers to the correlation between a variable and its lagged values. The Durbin-Watson statistic helps assess whether the residuals exhibit a pattern of dependence over time <span id="id21">[<a class="reference internal" href="../References.html#id61" title="J.M. Dufour and M.G. Dagenais. Durbin-Watson Tests for Serial Correlation in Regressions with Missing Observations. Cahier (Université de Montréal. Département de sciences économiques). Université de Montréal, 1983. URL: https://books.google.ca/books?id=Y2PuAAAAMAAJ.">Dufour and Dagenais, 1983</a>]</span>.</p>
<p>Here’s how the Durbin-Watson statistic works:</p>
<ol class="arabic">
<li><p><strong>Objective:</strong>
The primary purpose of the Durbin-Watson statistic is to determine whether there is a significant correlation between the residuals of consecutive observations in a time series or panel data context. Autocorrelation in residuals can indicate that the model has not captured all relevant information, leading to omitted variables or incorrect model specification.</p></li>
<li><p><strong>Calculation:</strong>
The Durbin-Watson statistic is calculated using the differences between consecutive residuals. Specifically, it’s computed as the ratio of the sum of squared differences between consecutive residuals to the sum of squared residuals. Mathematically, the formula for the Durbin-Watson statistic (<span class="math notranslate nohighlight">\(DW\)</span>) is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3aac6761-b691-44e6-aa9f-5b4b65b2584b">
<span class="eqno">(9.27)<a class="headerlink" href="#equation-3aac6761-b691-44e6-aa9f-5b4b65b2584b" title="Permalink to this equation">#</a></span>\[\begin{equation} DW = \frac{\sum_{t=2}^{n} (e_t - e_{t-1})^2}{\sum_{t=1}^{n} e_t^2} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( e_t \)</span> is the residual at time <span class="math notranslate nohighlight">\( t \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of observations in the time series.</p></li>
</ul>
</li>
<li><p><strong>Interpretation:</strong>
The Durbin-Watson statistic ranges from 0 to 4, with values closer to 2 indicating less autocorrelation. The statistic’s significance largely depends on its proximity to 2:</p>
<ul class="simple">
<li><p>A Durbin-Watson statistic close to 2 (around 2) suggests that there is little or no autocorrelation present in the residuals, implying that the model’s assumptions are met.</p></li>
<li><p>A Durbin-Watson statistic significantly less than 2 (closer to 0) indicates positive autocorrelation, meaning that residuals tend to be positively correlated with their lagged values.</p></li>
<li><p>A Durbin-Watson statistic significantly greater than 2 (closer to 4) suggests negative autocorrelation, where residuals tend to be negatively correlated with their lagged values.</p></li>
</ul>
</li>
<li><p><strong>Rule of Thumb:</strong>
Generally, a Durbin-Watson statistic value between 1.5 and 2.5 is often considered acceptable and indicative of minimal autocorrelation issues.</p></li>
</ol>
<p>In summary, the Durbin-Watson statistic helps analysts identify the presence and nature of autocorrelation in the residuals of a time series or panel data regression model. It guides researchers in assessing the validity of the model assumptions and determining whether further adjustments or exploration are necessary.</p>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.stattools</span> <span class="kn">import</span> <span class="n">durbin_watson</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Perform the Omnibus Test for goodness of fit</span>
<span class="n">omnibus_test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">omni_normtest</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Calculate the Durbin-Watson statistic</span>
<span class="n">dw_statistic</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Display the Durbin-Watson statistic</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Durbin-Watson Statistic: </span><span class="si">{</span><span class="n">dw_statistic</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Interpret the results</span>
<span class="k">if</span> <span class="n">omnibus_test</span><span class="o">.</span><span class="n">pvalue</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are not normally distributed, indicating potential model issues.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are normally distributed, suggesting a better model fit.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dw_statistic</span> <span class="o">&lt;</span> <span class="mf">1.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Positive autocorrelation may be present in the residuals.&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">dw_statistic</span> <span class="o">&gt;</span> <span class="mf">2.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Negative autocorrelation may be present in the residuals.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No significant autocorrelation is detected in the residuals.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Durbin-Watson Statistic: 2.0308
The residuals are not normally distributed, indicating potential model issues.
No significant autocorrelation is detected in the residuals.
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:52   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="jarque-bera-jb-test">
<h3><span class="section-number">9.1.16.5. </span>Jarque-Bera (JB) Test<a class="headerlink" href="#jarque-bera-jb-test" title="Permalink to this heading">#</a></h3>
<p>The Jarque-Bera (JB) test is a statistical test used to assess the normality of the residuals in a regression model. It evaluates whether the distribution of the residuals is symmetric and bell-shaped, resembling a normal distribution. The JB test is particularly important when dealing with linear regression models, as the validity of the normality assumption for the residuals is a key consideration <span id="id22">[<a class="reference internal" href="../References.html#id150" title="D. Nagakura. Normality Tests with Robust Scale Estimators. SSRN, 2022. URL: https://books.google.ca/books?id=-DDfzwEACAAJ.">Nagakura, 2022</a>]</span>.</p>
<p>Here’s how the Jarque-Bera test works:</p>
<ol class="arabic simple">
<li><p><strong>Objective:</strong>
The primary goal of the Jarque-Bera test is to determine whether the residuals of a regression model exhibit characteristics consistent with a normal distribution. A significant result indicates that the residuals deviate from normality, which can impact the reliability of statistical inference based on the model.</p></li>
<li><p><strong>Calculation:</strong>
The Jarque-Bera test relies on the skewness and kurtosis of the residuals. Skewness measures the symmetry of the distribution, while kurtosis gauges the “tailedness” of the distribution.</p></li>
<li><p><strong>Hypotheses:</strong>
The null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) of the Jarque-Bera test is that the residuals follow a normal distribution. The alternative hypothesis (<span class="math notranslate nohighlight">\(H_1\)</span>) is that the residuals do not follow a normal distribution.</p></li>
<li><p><strong>Test Statistic:</strong>
The test statistic for the Jarque-Bera test is calculated using the skewness and kurtosis of the residuals. It follows a Chi-squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) distribution with 2 degrees of freedom.</p></li>
<li><p><strong>Interpretation:</strong>
The calculated test statistic is compared to a critical value from the Chi-squared distribution, based on the chosen significance level (usually 0.05). If the calculated test statistic exceeds the critical value, the null hypothesis is rejected, indicating that the residuals do not follow a normal distribution. This suggests that the normality assumption for the residuals might not be met, which can affect the reliability of model inference.</p></li>
<li><p><strong>Prob(JB):</strong>
The “Prob(JB)” value, often reported alongside the JB test statistic, provides the probability associated with the JB test. It indicates the likelihood of observing the calculated JB test statistic (or a more extreme value) under the assumption of normality. A small “Prob(JB)” value indicates that the residuals deviate significantly from normality.</p></li>
</ol>
<p><strong>Prob(JB)</strong></p>
<p>“Prob(JB)” is a statistical metric often found in regression analysis output or summary tables, particularly in the context of assessing the normality of residuals using the Jarque-Bera (JB) test.</p>
<p>The Jarque-Bera test evaluates whether the distribution of the residuals of a regression model is consistent with a normal distribution. The “Prob(JB)” value, also known as the p-value for the Jarque-Bera test, provides important information about the significance of the test result.</p>
<p>Here’s how to interpret the “Prob(JB)” value:</p>
<ol class="arabic simple">
<li><p><strong>Low p-value (typically below 0.05):</strong>
A low “Prob(JB)” value suggests that the residuals significantly deviate from a normal distribution. In other words, there is strong evidence against the null hypothesis that the residuals are normally distributed. This indicates a departure from normality and raises concerns about the model’s assumption.</p></li>
<li><p><strong>High p-value (typically above 0.05):</strong>
A high “Prob(JB)” value indicates that there is not enough evidence to reject the null hypothesis of normality. In this case, the residuals are consistent with a normal distribution, and the model’s assumption of normality might be valid.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.stattools</span> <span class="kn">import</span> <span class="n">durbin_watson</span><span class="p">,</span> <span class="n">jarque_bera</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Perform the Omnibus Test for goodness of fit</span>
<span class="n">omnibus_test</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">omni_normtest</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Calculate the Durbin-Watson statistic</span>
<span class="n">dw_statistic</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Perform the Jarque-Bera (JB) test for normality</span>
<span class="n">jb_statistic</span><span class="p">,</span> <span class="n">jb_p_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jarque_bera</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Display the JB test results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jarque-Bera Statistic: </span><span class="si">{</span><span class="n">jb_statistic</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jarque-Bera p-value: </span><span class="si">{</span><span class="n">jb_p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Interpret the results</span>
<span class="k">if</span> <span class="n">omnibus_test</span><span class="o">.</span><span class="n">pvalue</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are not normally distributed, indicating potential model issues.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals are normally distributed, suggesting a better model fit.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">dw_statistic</span> <span class="o">&lt;</span> <span class="mf">1.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Positive autocorrelation may be present in the residuals.&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">dw_statistic</span> <span class="o">&gt;</span> <span class="mf">2.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Negative autocorrelation may be present in the residuals.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No significant autocorrelation is detected in the residuals.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">jb_p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals do not follow a normal distribution, suggesting non-normality.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The residuals follow a normal distribution, indicating normality.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jarque-Bera Statistic: 5.3540
Jarque-Bera p-value: 0.0688
The residuals are not normally distributed, indicating potential model issues.
No significant autocorrelation is detected in the residuals.
The residuals follow a normal distribution, indicating normality.
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:52   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="skewness">
<h3><span class="section-number">9.1.16.6. </span>Skewness<a class="headerlink" href="#skewness" title="Permalink to this heading">#</a></h3>
<p>Skewness is a statistical measure that quantifies the asymmetry of the probability distribution of a dataset or variable. It indicates the degree to which the data’s distribution is skewed or “lopsided.” In the context of regression analysis, skewness often refers to the skewness of the residuals of a model.</p>
<p>Here’s how skewness works:</p>
<ol class="arabic">
<li><p><strong>Symmetry and Skewness:</strong>
In a symmetric distribution, the left and right sides of the distribution are mirror images of each other. For a perfectly symmetric distribution, the mean, median, and mode coincide at the center of the distribution. Skewness measures the departure from this symmetry.</p></li>
<li><p><strong>Positive Skewness:</strong>
Positive skewness occurs when the distribution’s tail is longer on the right side (positively skewed). This means that the distribution is stretched out to the right, and the majority of data points are concentrated on the left side. The mean tends to be greater than the median in a positively skewed distribution.</p></li>
<li><p><strong>Negative Skewness:</strong>
Negative skewness occurs when the distribution’s tail is longer on the left side (negatively skewed). In this case, the distribution is stretched out to the left, and the majority of data points are concentrated on the right side. The mean tends to be less than the median in a negatively skewed distribution.</p></li>
<li><p><strong>Calculation of Skewness:</strong>
The skewness of a dataset <span class="math notranslate nohighlight">\(X\)</span> can be calculated using the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b4ae1eea-4c8d-401e-93a1-fb76390e925b">
<span class="eqno">(9.28)<a class="headerlink" href="#equation-b4ae1eea-4c8d-401e-93a1-fb76390e925b" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Skewness}(X) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^3}{n \cdot s^3} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> represents each data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{x} \)</span> is the mean of the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( s \)</span> is the standard deviation of the dataset.</p></li>
</ul>
</li>
<li><p><strong>Interpretation:</strong>
Skewness values can be positive, negative, or close to zero. A skewness of 0 indicates perfect symmetry. Larger positive or negative skewness values indicate increasing levels of skewness. Skewness values are dimensionless, making them suitable for comparing the skewness of different datasets.</p></li>
</ol>
<p>In the context of regression analysis, examining the skewness of the residuals is important because normally distributed residuals are a common assumption of linear regression models. Skewed residuals could suggest a lack of fit of the model to the data, leading to potential biases in the coefficient estimates and statistical inferences. Addressing skewness might involve transformations of the dependent or independent variables to achieve better model performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">skew</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>

<span class="c1"># Sample data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>

<span class="c1"># Calculate skewness</span>
<span class="n">skewness</span> <span class="o">=</span> <span class="n">skew</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Create figure and axis</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Create a KDE plot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Values&quot;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Density&quot;</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Skewness: </span><span class="si">{</span><span class="n">skewness</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2faf8ba193a8a60c445392e721ffae055e25ca0e2235330aca10600327e95e46.png" src="../_images/2faf8ba193a8a60c445392e721ffae055e25ca0e2235330aca10600327e95e46.png" />
</div>
</div>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">skew</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Calculate the skewness of residuals</span>
<span class="n">residual_skew</span> <span class="o">=</span> <span class="n">skew</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Display the skewness of residuals</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Skewness of Residuals: </span><span class="si">{</span><span class="n">residual_skew</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Skewness of Residuals: -0.1913
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:53   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="kurtosis">
<h3><span class="section-number">9.1.16.7. </span>Kurtosis<a class="headerlink" href="#kurtosis" title="Permalink to this heading">#</a></h3>
<p>Kurtosis is a statistical measure that quantifies the “tailedness” or the extent to which the tails of a probability distribution differ from those of a normal distribution. In the context of regression analysis or statistics in general, kurtosis provides information about the shape of the distribution of a dataset or variable <span id="id23">[<a class="reference internal" href="../References.html#id155" title="G.R. Norman and D.L. Streiner. Biostatistics: The Bare Essentials. B.C. Decker, 2008. ISBN 9781550094008. URL: https://books.google.ca/books?id=8rkqWafdpuoC.">Norman and Streiner, 2008</a>]</span>.</p>
<p>Here’s how kurtosis works:</p>
<ol class="arabic">
<li><p><strong>Kurtosis and Distribution Shape:</strong>
Kurtosis focuses on the tails of a distribution. A normal distribution has a kurtosis of 3, and distributions with higher kurtosis have heavier tails, indicating more extreme values than the tails of a normal distribution. Distributions with lower kurtosis have lighter tails, suggesting fewer extreme values.</p></li>
<li><p><strong>Leptokurtic Distribution:</strong>
A distribution with positive kurtosis (greater than 3) is called leptokurtic. Leptokurtic distributions have heavy tails and a peak that is higher and sharper than that of a normal distribution. This means that extreme values are more likely to occur in a leptokurtic distribution.</p></li>
<li><p><strong>Platykurtic Distribution:</strong>
A distribution with negative kurtosis (less than 3) is called platykurtic. Platykurtic distributions have lighter tails and a peak that is lower and flatter than that of a normal distribution. Extreme values are less likely to occur in a platykurtic distribution.</p></li>
<li><p><strong>Mesokurtic Distribution:</strong>
A distribution with kurtosis equal to 3 is called mesokurtic. A mesokurtic distribution has tails and a peak that are similar to those of a normal distribution.</p></li>
<li><p><strong>Calculation of Kurtosis:</strong>
The kurtosis of a dataset <span class="math notranslate nohighlight">\(X\)</span> can be calculated using the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-81e21123-29ab-4c88-8722-0c440e43f7c7">
<span class="eqno">(9.29)<a class="headerlink" href="#equation-81e21123-29ab-4c88-8722-0c440e43f7c7" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Kurtosis}(X) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^4}{n \cdot s^4} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of data points in the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> represents each data point.</p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{x} \)</span> is the mean of the dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\( s \)</span> is the standard deviation of the dataset.</p></li>
</ul>
</li>
<li><p><strong>Interpretation:</strong>
Positive kurtosis indicates heavier tails and a more peaked distribution, while negative kurtosis indicates lighter tails and a flatter distribution. The closer the kurtosis is to 3, the more similar the distribution is to a normal distribution.</p></li>
</ol>
<p>In the context of regression analysis, examining the kurtosis of the residuals can provide insights into the distributional characteristics of the model’s errors. Kurtosis, along with other diagnostic metrics, helps analysts evaluate the normality assumption and make informed decisions about potential model adjustments or transformations.</p>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">skew</span><span class="p">,</span> <span class="n">kurtosis</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Calculate the skewness and kurtosis of residuals</span>
<span class="n">residual_skew</span> <span class="o">=</span> <span class="n">skew</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>
<span class="n">residual_kurtosis</span> <span class="o">=</span> <span class="n">kurtosis</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>

<span class="c1"># Display the skewness and kurtosis of residuals</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Skewness of Residuals: </span><span class="si">{</span><span class="n">residual_skew</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kurtosis of Residuals: </span><span class="si">{</span><span class="n">residual_kurtosis</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Skewness of Residuals: -0.1913
Kurtosis of Residuals: -1.0671
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:53   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
<section id="condition-number-cond">
<h3><span class="section-number">9.1.16.8. </span>Condition Number (Cond)<a class="headerlink" href="#condition-number-cond" title="Permalink to this heading">#</a></h3>
<p>The condition number, often denoted as “Cond,” is a metric used to assess the multicollinearity (high correlation) among predictor variables in a regression model. It provides insight into the stability of the regression coefficients and the potential for large changes in the estimated coefficients due to small changes in the data.</p>
<p>Here’s how the condition number works:</p>
<ol class="arabic">
<li><p><strong>Objective:</strong>
The primary goal of examining the condition number is to understand whether the predictor variables in the model are highly correlated with each other. Multicollinearity can lead to unstable coefficient estimates and difficulties in interpreting the contributions of individual variables.</p></li>
<li><p><strong>Calculation:</strong>
The condition number is calculated based on the eigenvalues of the correlation matrix of the predictor variables. Specifically, it is the ratio of the largest eigenvalue to the smallest eigenvalue. Mathematically:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e815bd84-cddc-4b05-b3fe-5ea437121d33">
<span class="eqno">(9.30)<a class="headerlink" href="#equation-e815bd84-cddc-4b05-b3fe-5ea437121d33" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Condition Number (Cond)} = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \lambda_{\text{max}} \)</span> is the largest eigenvalue of the correlation matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_{\text{min}} \)</span> is the smallest eigenvalue of the correlation matrix.</p></li>
</ul>
</li>
<li><p><strong>Interpretation:</strong>
The condition number provides information about the severity of multicollinearity:</p>
<ul class="simple">
<li><p>A low condition number (close to 1) indicates minimal multicollinearity. The predictor variables are not strongly correlated with each other, and the model’s coefficients are stable.</p></li>
<li><p>A high condition number (much greater than 1) indicates strong multicollinearity. The predictor variables are highly correlated, potentially leading to instability in the coefficient estimates. A high condition number suggests that small changes in the data could result in large changes in the estimated coefficients.</p></li>
</ul>
</li>
<li><p><strong>Implications:</strong>
High multicollinearity can make it challenging to interpret the individual contributions of predictor variables and can result in inflated standard errors, making some coefficients appear statistically insignificant when they might not be. It’s important to address or mitigate multicollinearity to improve the model’s reliability.</p></li>
</ol>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Add a constant term (intercept) to the independent variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Create a linear regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Calculate the condition number of the design matrix</span>
<span class="n">condition_number</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Display the condition number</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Condition Number of Design Matrix: </span><span class="si">{</span><span class="n">condition_number</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Condition Number of Design Matrix: 3.5825
</pre></div>
</div>
</div>
</div>
<p>We can also obtain this information by using <code class="docutils literal notranslate"><span class="pre">print(model.summary())</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.974
Method:                 Least Squares   F-statistic:                     3766.
Date:                Tue, 14 Nov 2023   Prob (F-statistic):           5.22e-80
Time:                        09:20:53   Log-Likelihood:                -13.196
No. Observations:                 100   AIC:                             30.39
Df Residuals:                      98   BIC:                             35.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.5581      0.054     10.417      0.000       0.452       0.664
x1             2.9683      0.048     61.365      0.000       2.872       3.064
==============================================================================
Omnibus:                       19.276   Durbin-Watson:                   2.031
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.354
Skew:                          -0.191   Prob(JB):                       0.0688
Kurtosis:                       1.933   Cond. No.                         3.58
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ReadMe.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>An Introduction to Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C09S2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.2. </span>An Introduction to Linear Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-test-sets">9.1.1. Training, Validation, and Test sets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error">9.1.2. Standard Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared-statistic">9.1.3. R-squared Statistic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">9.1.4. Mean Squared Error (MSE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">9.1.5. Mean Absolute Error (MAE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-matrix">9.1.6. Correlation Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">9.1.7. Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-evaluating-correct-classifications">9.1.8. Accuracy: Evaluating Correct Classifications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#precision">9.1.9. Precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity-or-true-positive-rate">9.1.10. Recall (Sensitivity or True Positive Rate)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score">9.1.11. F1-Score</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#balanced-accuracy">9.1.12. Balanced Accuracy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matthews-correlation-coefficient-mcc">9.1.13. Matthews Correlation Coefficient (MCC)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cohens-kappa">9.1.14. Cohen’s Kappa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learns-metrics">9.1.15. Scikit-learn’s metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additioal-information-for-statsmodels-api-optional-content">9.1.16. Additioal Information for <code class="docutils literal notranslate"><span class="pre">statsmodels.api</span></code> (Optional Content)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error-and-coefficient-precision-in-statistics">9.1.16.1. Standard Error and Coefficient Precision in Statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-value-t-and-p-value-p-t-in-regression-analysis">9.1.16.2. t-value (<span class="math notranslate nohighlight">\(t\)</span>) and p-value (<span class="math notranslate nohighlight">\(P&gt;|t|\)</span>) in Regression Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-test">9.1.16.3. Omnibus Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#durbin-watson-statistic">9.1.16.4. Durbin-Watson Statistic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jarque-bera-jb-test">9.1.16.5. Jarque-Bera (JB) Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skewness">9.1.16.6. Skewness</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kurtosis">9.1.16.7. Kurtosis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-number-cond">9.1.16.8. Condition Number (Cond)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>