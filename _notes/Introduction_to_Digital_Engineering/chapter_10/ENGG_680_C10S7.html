

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.7. Gradient Boosting &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG_680_C10S7';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Dimensionality Reduction and Feature Selection" href="../chapter_11/ReadMe.html" />
    <link rel="prev" title="10.6. Random Forests" href="ENGG_680_C10S6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression  (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">10. Tree-Based Methods</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S2.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S3.html">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gradient Boosting</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-explanation">10.7.1. Mathematical Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-gradientboostingregressor">10.7.1.1. Regression (GradientBoostingRegressor)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-gradientboostingclassifier">10.7.1.2. Classification (GradientBoostingClassifier)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learns-gradient-boosting-algorithms">10.7.2. scikit-learn’s Gradient Boosting Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost-extreme-gradient-boosting">10.7.3. XGBoost (Extreme Gradient Boosting)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="gradient-boosting">
<h1><span class="section-number">10.7. </span>Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permalink to this headline">#</a></h1>
<p>Gradient Boosting is a versatile machine learning technique used for both classification and regression tasks. It constructs an ensemble of decision trees sequentially, enhancing predictive accuracy. Below, I’ll elaborate on the mathematical intricacies of the Gradient Boosting algorithm for classification and regression tasks, connecting it to the scikit-learn classes <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
<div class="section" id="mathematical-explanation">
<h2><span class="section-number">10.7.1. </span>Mathematical Explanation<a class="headerlink" href="#mathematical-explanation" title="Permalink to this headline">#</a></h2>
<p>At its core, Gradient Boosting iteratively crafts a series of decision trees, each aimed at rectifying errors made by its predecessors. It optimizes a loss function, usually the mean squared error for regression or the deviance (log loss) for classification. The algorithm operates as follows <span id="id1">[<a class="reference internal" href="../References.html#id49" title="E. Alpaydin. Introduction to Machine Learning, fourth edition. Adaptive Computation and Machine Learning series. MIT Press, 2020. ISBN 9780262043793. URL: https://books.google.ca/books?id=tZnSDwAAQBAJ.">Alpaydin, 2020</a>, <a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="section" id="regression-gradientboostingregressor">
<h3><span class="section-number">10.7.1.1. </span>Regression (GradientBoostingRegressor)<a class="headerlink" href="#regression-gradientboostingregressor" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Begin with a model set at a constant value, often the mean of the target variable, serving as the initial prediction:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1205ab38-2640-4184-860b-1c524ae05045">
<span class="eqno">(10.19)<a class="headerlink" href="#equation-1205ab38-2640-4184-860b-1c524ae05045" title="Permalink to this equation">#</a></span>\[\begin{equation}
   \hat{f}_0(x) = \underset{c}{\mathrm{argmin}} \sum_{i=1}^{n} L(y_i, c)
   \end{equation}\]</div>
</li>
<li><p>For each boosting iteration (b = 1 to B), follow these steps:</p>
<p>a. Compute the negative gradient (residuals) of the loss function concerning the current model’s predictions:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c6714fa6-8111-4ab2-82ce-ea3f25942065">
<span class="eqno">(10.20)<a class="headerlink" href="#equation-c6714fa6-8111-4ab2-82ce-ea3f25942065" title="Permalink to this equation">#</a></span>\[\begin{equation}
   - \left[\frac{\partial L(y_i, \hat{f}(x_i))}{\partial \hat{f}(x_i)}\right]_{\hat{f}(x)=\hat{f}_{b-1}(x)}
   \end{equation}\]</div>
<p>b. Fit a decision tree to the negative gradient (residuals) values as the new target variable. Typically, this tree is shallow and controlled by hyperparameters like <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>.</p>
<p>c. Update the model by incorporating the prediction from the new tree, scaled by a learning rate (<span class="math notranslate nohighlight">\(\alpha\)</span>):</p>
<div class="amsmath math notranslate nohighlight" id="equation-81895e22-2c97-4a4e-bf32-e5910214c313">
<span class="eqno">(10.21)<a class="headerlink" href="#equation-81895e22-2c97-4a4e-bf32-e5910214c313" title="Permalink to this equation">#</a></span>\[\begin{equation}
   \hat{f}_b(x) = \hat{f}_{b-1}(x) + \alpha \cdot \text{new_tree}(x)
   \end{equation}\]</div>
</li>
<li><p>The ultimate prediction is the sum of all individual trees:</p>
<div class="amsmath math notranslate nohighlight" id="equation-28cee4c6-a50a-40fb-a313-14cc378ace86">
<span class="eqno">(10.22)<a class="headerlink" href="#equation-28cee4c6-a50a-40fb-a313-14cc378ace86" title="Permalink to this equation">#</a></span>\[\begin{equation}
   \hat{f}(x) = \sum_{b=1}^{B} \hat{f}_b(x)
   \end{equation}\]</div>
</li>
</ol>
</div>
<div class="section" id="classification-gradientboostingclassifier">
<h3><span class="section-number">10.7.1.2. </span>Classification (GradientBoostingClassifier)<a class="headerlink" href="#classification-gradientboostingclassifier" title="Permalink to this headline">#</a></h3>
<p>The classification process parallels regression but employs a distinct loss function (typically deviance) and predicts class probabilities. Here’s the procedure:</p>
<ol class="arabic">
<li><p>Start with a model initialized with class probabilities, often derived from class frequencies.</p></li>
<li><p>For each boosting iteration (b = 1 to B), follow these steps:</p>
<p>a. Calculate the negative gradient of the log-loss (deviance) with respect to the current class probabilities:</p>
<div class="amsmath math notranslate nohighlight" id="equation-807f16a4-c812-49a4-8186-b5c55f4d160a">
<span class="eqno">(10.23)<a class="headerlink" href="#equation-807f16a4-c812-49a4-8186-b5c55f4d160a" title="Permalink to this equation">#</a></span>\[\begin{equation}
   - \left[\frac{\partial \text{Dev}(y_i, p_i)}{\partial p_{ik}}\right]_{p_{ik} = p_{ik}^{(b-1)}}
   \end{equation}\]</div>
<p>b. Construct a decision tree using the negative gradient values as the new target variable.</p>
<p>c. Update the model’s class probabilities by adding the prediction from the new tree.</p>
</li>
<li><p>The final prediction involves converting class probabilities into class labels.</p></li>
</ol>
<p><strong>Scikit-learn Implementation:</strong></p>
<p>Scikit-learn offers the <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> classes for regression and classification tasks. These classes allow customization of hyperparameters like learning rate, tree depth, and the number of boosting iterations (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>). Here’s how to use them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>

<span class="c1"># For regression</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># For classification</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Utilizing these classes simplifies the implementation of Gradient Boosting for regression and classification tasks within scikit-learn.</p>
</div>
</div>
<div class="section" id="scikit-learns-gradient-boosting-algorithms">
<h2><span class="section-number">10.7.2. </span>scikit-learn’s Gradient Boosting Algorithms<a class="headerlink" href="#scikit-learns-gradient-boosting-algorithms" title="Permalink to this headline">#</a></h2>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Initialize the model’s prediction as a constant value, often the mean of the target variable for regression or class probabilities for classification.</p></li>
<li><p>Compute the initial negative gradient (residuals) for regression or the negative gradient of the log-loss for classification.</p></li>
</ul>
</li>
<li><p><strong>Boosting Iterations:</strong></p>
<ul>
<li><p>For each boosting iteration (b = 1 to B), perform the following steps:</p>
<p>a. <strong>Construct a Decision Tree:</strong></p>
<ul class="simple">
<li><p>Fit a decision tree to the negative gradient (residuals) or the negative gradient of the log-loss values as the new target variable.</p></li>
<li><p>The decision tree typically has a limited depth (controlled by hyperparameters like <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>).</p></li>
</ul>
<p>b. <strong>Update the Model:</strong></p>
<ul>
<li><p>Compute the prediction from the new decision tree and scale it by a learning rate (α).</p></li>
<li><p>Update the model’s prediction by adding the scaled tree prediction:</p>
<div class="amsmath math notranslate nohighlight" id="equation-37f62798-a2ea-44f0-b6c7-fec590dc6ed8">
<span class="eqno">(10.24)<a class="headerlink" href="#equation-37f62798-a2ea-44f0-b6c7-fec590dc6ed8" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{f}_b(x) = \hat{f}_{b-1}(x) + \alpha \cdot \text{new_tree}(x) \end{equation}\]</div>
</li>
</ul>
<p>c. <strong>Update the Negative Gradient (Residuals):</strong></p>
<ul>
<li><p>Update the negative gradient (residuals) for regression or the negative gradient of the log-loss for classification by subtracting the contribution of the new tree’s prediction:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8fa32f36-8fec-44ca-aaeb-671ad72b91ef">
<span class="eqno">(10.25)<a class="headerlink" href="#equation-8fa32f36-8fec-44ca-aaeb-671ad72b91ef" title="Permalink to this equation">#</a></span>\[\begin{equation} r_i \leftarrow r_i - \alpha \cdot \text{new_tree}(x_i) \end{equation}\]</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Final Prediction:</strong></p>
<ul>
<li><p>The final prediction is the sum of predictions from all individual trees:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c9088a90-1e63-41bd-9498-e16400c60152">
<span class="eqno">(10.26)<a class="headerlink" href="#equation-c9088a90-1e63-41bd-9498-e16400c60152" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{f}(x) = \sum_{b=1}^{B} \hat{f}_b(x) \end{equation}\]</div>
</li>
</ul>
</li>
<li><p><strong>Classification Specifics:</strong></p>
<ul class="simple">
<li><p>For classification tasks, the class probabilities are often transformed into class labels using a threshold or argmax operation.</p></li>
</ul>
</li>
<li><p><strong>Hyperparameters:</strong></p>
<ul class="simple">
<li><p>The algorithm’s behavior is influenced by hyperparameters like the number of boosting iterations (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>), learning rate (<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>), maximum tree depth (<code class="docutils literal notranslate"><span class="pre">max_depth</span></code>), and more.</p></li>
</ul>
</li>
<li><p><strong>Regularization:</strong></p>
<ul class="simple">
<li><p>Regularization techniques, such as early stopping and subsampling, are commonly used to prevent overfitting and improve efficiency.</p></li>
</ul>
</li>
<li><p><strong>Prediction and Evaluation:</strong></p>
<ul class="simple">
<li><p>Use the trained model to make predictions on new data. For classification, the class with the highest probability (or predicted label) is assigned.</p></li>
<li><p>Evaluate the model’s performance using appropriate metrics for regression or classification tasks.</p></li>
</ul>
</li>
</ol>
<p>Remember that while this description provides an overview of the algorithm, the actual implementation in scikit-learn includes optimizations and additional features. For comprehensive details, you can refer to scikit-learn’s documentation on <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
</div>
<div class="section" id="xgboost-extreme-gradient-boosting">
<h2><span class="section-number">10.7.3. </span>XGBoost (Extreme Gradient Boosting)<a class="headerlink" href="#xgboost-extreme-gradient-boosting" title="Permalink to this headline">#</a></h2>
<p>XGBoost (Extreme Gradient Boosting) is a highly optimized and popular gradient boosting framework that excels in performance and predictive accuracy. Developed by Tianqi Chen, XGBoost has become one of the go-to choices for structured data in machine learning competitions and real-world applications. It extends the basic gradient boosting algorithm by introducing enhancements and fine-tuning to achieve superior results. Here’s an overview of the XGBoost algorithm <span id="id2">[<a class="reference internal" href="../References.html#id60" title="xgboost developers. Xgboost documentation. https://xgboost.readthedocs.io/en/stable/, 2023. [Online; accessed 01-August-2023].">xgboost developers, 2023</a>]</span>:</p>
<p><strong>Gradient Boosting Framework:</strong>
XGBoost shares the fundamental idea of gradient boosting. It iteratively builds an ensemble of weak learners, usually decision trees, to form a powerful predictive model. Each weak learner attempts to correct the errors made by its predecessors.</p>
<p><strong>Key Features of XGBoost:</strong>
XGBoost introduces several innovations that contribute to its remarkable performance:</p>
<ol class="arabic simple">
<li><p><strong>Regularization:</strong> XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in the optimization objective to control model complexity and prevent overfitting.</p></li>
<li><p><strong>Custom Loss Functions:</strong> XGBoost allows the use of custom loss functions, extending its applicability to various problem domains.</p></li>
<li><p><strong>Handling Missing Values:</strong> XGBoost can automatically handle missing values during tree construction.</p></li>
<li><p><strong>Feature Importance:</strong> XGBoost provides insights into feature importance, aiding in understanding the model’s decision-making process.</p></li>
<li><p><strong>Built-in Cross-Validation:</strong> XGBoost includes a built-in cross-validation function for model assessment and hyperparameter tuning.</p></li>
<li><p><strong>Parallel and Distributed Computing:</strong> XGBoost supports parallel and distributed computing to accelerate training on large datasets.</p></li>
<li><p><strong>Pruning:</strong> XGBoost employs a technique called “pruning” to remove splits that lead to negative gains during tree growth, improving efficiency.</p></li>
</ol>
<p><strong>XGBoost Algorithm:</strong>
The XGBoost algorithm, like gradient boosting, consists of iterative steps to build an ensemble of decision trees. The main steps are as follows:</p>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Start with a constant prediction (often the mean of the target variable for regression) for all instances.</p></li>
<li><p>Compute the initial gradient (negative gradient of the loss function) for each instance.</p></li>
</ul>
</li>
<li><p><strong>Boosting Iterations:</strong></p>
<ul>
<li><p>For each boosting iteration (b = 1 to B), perform these steps:</p>
<p>a. <strong>Construct a Decision Tree:</strong></p>
<ul class="simple">
<li><p>Fit a decision tree to the negative gradient values, incorporating regularization terms.</p></li>
<li><p>Prune the tree using depth and gain-based criteria.</p></li>
</ul>
<p>b. <strong>Update the Model:</strong></p>
<ul class="simple">
<li><p>Compute the prediction from the new tree and scale it by a learning rate (α).</p></li>
<li><p>Update the model’s prediction by adding the scaled tree prediction.</p></li>
</ul>
<p>c. <strong>Update the Gradient:</strong></p>
<ul class="simple">
<li><p>Compute the new gradient using the residuals (negative gradient) from the previous iteration.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Final Prediction:</strong></p>
<ul class="simple">
<li><p>The final prediction is the sum of predictions from all individual trees.</p></li>
</ul>
</li>
</ol>
<p><strong>XGBoost Implementation:</strong>
XGBoost is implemented as a Python package with wrappers for various programming languages. The Python API allows you to create instances of <code class="docutils literal notranslate"><span class="pre">xgboost.XGBRegressor</span></code> for regression tasks and <code class="docutils literal notranslate"><span class="pre">xgboost.XGBClassifier</span></code> for classification tasks. These classes provide a wide range of hyperparameters for fine-tuning, such as learning rate, maximum depth, and regularization terms.</p>
<p>XGBoost’s flexibility, scalability, and performance make it a popular choice for many machine learning projects, especially when working with structured data. Its advanced features and optimizations contribute to its effectiveness in achieving high predictive accuracy. For comprehensive details, you can refer to the official XGBoost documentation and resources.</p>
<p><font color='Blue'><b>Example</b></font> Let’s put these concepts into action by utilizing the Boston Housing dataset. You can access the Boston Housing dataset through the following link: <a class="reference external" href="http://lib.stat.cmu.edu/datasets/boston">http://lib.stat.cmu.edu/datasets/boston</a>. The Boston Housing dataset holds its significance in regression analysis, offering valuable insights into various attributes associated with housing prices across diverse neighborhoods in Boston. In our exploration, we’ll harness the potential of the GradientBoostingRegressor and also employ the xgboost.XGBRegressor to further enhance our analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">_url</span> <span class="o">=</span> <span class="s2">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CRIM&#39;</span><span class="p">,</span> <span class="s1">&#39;ZN&#39;</span><span class="p">,</span> <span class="s1">&#39;INDUS&#39;</span><span class="p">,</span> <span class="s1">&#39;CHAS&#39;</span><span class="p">,</span> <span class="s1">&#39;NOX&#39;</span><span class="p">,</span> <span class="s1">&#39;RM&#39;</span><span class="p">,</span> <span class="s1">&#39;AGE&#39;</span><span class="p">,</span> <span class="s1">&#39;DIS&#39;</span><span class="p">,</span> <span class="s1">&#39;RAD&#39;</span><span class="p">,</span> <span class="s1">&#39;TAX&#39;</span><span class="p">,</span> <span class="s1">&#39;PTRATIO&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;LSTAT&#39;</span><span class="p">,</span> <span class="s1">&#39;MEDV&#39;</span><span class="p">]</span>

<span class="n">Boston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="o">=</span> <span class="n">_url</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span>
                 <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1">#Flatten all the values into a single long list and remove the nulls</span>
<span class="n">values_w_nulls</span> <span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">all_values</span> <span class="o">=</span> <span class="n">values_w_nulls</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">values_w_nulls</span><span class="p">)]</span>

<span class="c1">#Reshape the values to have 14 columns and make a new df out of them</span>
<span class="n">Boston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">all_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)),</span>
                  <span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span><span class="p">)</span>
<span class="c1"># Display the first 10 rows</span>
<span class="n">display</span><span class="p">(</span><span class="n">Boston</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.02985</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.430</td>
      <td>58.7</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.12</td>
      <td>5.21</td>
      <td>28.7</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.08829</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.012</td>
      <td>66.6</td>
      <td>5.5605</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>395.60</td>
      <td>12.43</td>
      <td>22.9</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.14455</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.172</td>
      <td>96.1</td>
      <td>5.9505</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>396.90</td>
      <td>19.15</td>
      <td>27.1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.21124</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>5.631</td>
      <td>100.0</td>
      <td>6.0821</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>386.63</td>
      <td>29.93</td>
      <td>16.5</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.17004</td>
      <td>12.5</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>0.524</td>
      <td>6.004</td>
      <td>85.9</td>
      <td>6.5921</td>
      <td>5.0</td>
      <td>311.0</td>
      <td>15.2</td>
      <td>386.71</td>
      <td>17.10</td>
      <td>18.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MEDV&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Boston</span><span class="o">.</span><span class="n">MEDV</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Create a figure and subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">feature_set_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Using sklearn GradientBoostingRegressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Using xgboost.XGBRegressor&#39;</span><span class="p">]</span>

<span class="c1"># Loop through different regressors</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">regressor_class</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">]):</span>
    <span class="c1"># Create a regressor</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">regressor_class</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Create scatter plot and a diagonal reference line</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;medv&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;SkyBlue&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;--k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    
    <span class="c1"># Set title and labels for the current subplot</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">feature_set_labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Predicted Values&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Actual Values&#39;</span><span class="p">)</span>
    
    <span class="c1"># Calculate and display Mean Squared Error (MSE) with a background color</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.63</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                      <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
                      <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;Whitesmoke&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">))</span>  <span class="c1"># Add background color</span>
    
    <span class="c1"># Set an equal aspect ratio for the subplots</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="c1"># Adjust the layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/21d61c62fa4af89951dd655d7612311b1bf07c202819c62e6117ebb4348feb1b.png" src="../_images/21d61c62fa4af89951dd655d7612311b1bf07c202819c62e6117ebb4348feb1b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Initialize and train the models</span>
<span class="c1"># Initialize the GradientBoostingRegressor model</span>
<span class="n">reg_gb</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reg_gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Initialize the XGBRegressor model</span>
<span class="n">reg_xgb</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reg_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Create DataFrames with feature importances</span>
<span class="c1"># Create a DataFrame for feature importances using GradientBoostingRegressor</span>
<span class="n">importance_gb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">reg_gb</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="o">*</span> <span class="mi">100</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Create a DataFrame for feature importances using XGBRegressor</span>
<span class="n">importance_xgb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">reg_xgb</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="o">*</span> <span class="mi">100</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Create subplots</span>
<span class="c1"># Create a figure with two vertically stacked subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot feature importance for GradientBoostingRegressor</span>
<span class="c1"># Create a bar plot for feature importance in the first subplot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">importance_gb</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">importance_gb</span><span class="o">.</span><span class="n">Importance</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;DarkGreen&#39;</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s2">&quot;///&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature Importance in Gradient Boosting Regressor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DarkSlateGray&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">])</span>

<span class="c1"># Plot feature importance for XGBRegressor</span>
<span class="c1"># Create a bar plot for feature importance in the second subplot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">importance_xgb</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">importance_xgb</span><span class="o">.</span><span class="n">Importance</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;DarkRed&#39;</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\\\\</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature Importance in XGBoost Regressor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DarkSlateGray&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">])</span>

<span class="c1"># Common settings for both subplots</span>
<span class="c1"># Iterate through the axes and apply common settings</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Variable Importance&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/845c7fb641e187bb1de68073a721cc5568fd2d9587cee3d08e84d881bbbbd9ec.png" src="../_images/845c7fb641e187bb1de68073a721cc5568fd2d9587cee3d08e84d881bbbbd9ec.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Merge importance_gb and importance_xgb DataFrames</span>
<span class="n">merged_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">importance_gb</span><span class="p">,</span> <span class="n">importance_xgb</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">suffixes</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;_GB&#39;</span><span class="p">,</span> <span class="s1">&#39;_XGB&#39;</span><span class="p">))</span>

<span class="c1"># Display the merged DataFrame with background gradient</span>
<span class="n">display</span><span class="p">(</span><span class="n">merged_importance</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlGnBu&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_d4439_row0_col0 {
  background-color: #f8fcc9;
  color: #000000;
}
#T_d4439_row0_col1, #T_d4439_row2_col1, #T_d4439_row6_col0 {
  background-color: #fdfed4;
  color: #000000;
}
#T_d4439_row1_col0, #T_d4439_row3_col0 {
  background-color: #ffffd9;
  color: #000000;
}
#T_d4439_row1_col1, #T_d4439_row3_col1, #T_d4439_row8_col1 {
  background-color: #feffd8;
  color: #000000;
}
#T_d4439_row2_col0, #T_d4439_row4_col1, #T_d4439_row6_col1, #T_d4439_row8_col0, #T_d4439_row11_col0, #T_d4439_row11_col1 {
  background-color: #feffd6;
  color: #000000;
}
#T_d4439_row4_col0, #T_d4439_row9_col0 {
  background-color: #fdfed5;
  color: #000000;
}
#T_d4439_row5_col0 {
  background-color: #3fb4c4;
  color: #f1f1f1;
}
#T_d4439_row5_col1 {
  background-color: #a7dcb7;
  color: #000000;
}
#T_d4439_row7_col0 {
  background-color: #f0f9b8;
  color: #000000;
}
#T_d4439_row7_col1 {
  background-color: #f1faba;
  color: #000000;
}
#T_d4439_row9_col1 {
  background-color: #f6fbc5;
  color: #000000;
}
#T_d4439_row10_col0 {
  background-color: #f4fbc0;
  color: #000000;
}
#T_d4439_row10_col1 {
  background-color: #e1f3b2;
  color: #000000;
}
#T_d4439_row12_col0 {
  background-color: #1d2e83;
  color: #f1f1f1;
}
#T_d4439_row12_col1 {
  background-color: #081d58;
  color: #f1f1f1;
}
</style>
<table id="T_d4439">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_d4439_level0_col0" class="col_heading level0 col0" >Importance_GB</th>
      <th id="T_d4439_level0_col1" class="col_heading level0 col1" >Importance_XGB</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_d4439_level0_row0" class="row_heading level0 row0" >CRIM</th>
      <td id="T_d4439_row0_col0" class="data row0 col0" >3.142650</td>
      <td id="T_d4439_row0_col1" class="data row0 col1" >0.998187</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row1" class="row_heading level0 row1" >ZN</th>
      <td id="T_d4439_row1_col0" class="data row1 col0" >0.000000</td>
      <td id="T_d4439_row1_col1" class="data row1 col1" >0.413653</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row2" class="row_heading level0 row2" >INDUS</th>
      <td id="T_d4439_row2_col0" class="data row2 col0" >0.454974</td>
      <td id="T_d4439_row2_col1" class="data row2 col1" >0.990447</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row3" class="row_heading level0 row3" >CHAS</th>
      <td id="T_d4439_row3_col0" class="data row3 col0" >0.106613</td>
      <td id="T_d4439_row3_col1" class="data row3 col1" >0.424462</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row4" class="row_heading level0 row4" >NOX</th>
      <td id="T_d4439_row4_col0" class="data row4 col0" >0.864238</td>
      <td id="T_d4439_row4_col1" class="data row4 col1" >0.532492</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row5" class="row_heading level0 row5" >RM</th>
      <td id="T_d4439_row5_col0" class="data row5 col0" >29.282518</td>
      <td id="T_d4439_row5_col1" class="data row5 col1" >17.847019</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row6" class="row_heading level0 row6" >AGE</th>
      <td id="T_d4439_row6_col0" class="data row6 col0" >0.971014</td>
      <td id="T_d4439_row6_col1" class="data row6 col1" >0.569709</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row7" class="row_heading level0 row7" >DIS</th>
      <td id="T_d4439_row7_col0" class="data row7 col0" >6.032818</td>
      <td id="T_d4439_row7_col1" class="data row7 col1" >5.766480</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row8" class="row_heading level0 row8" >RAD</th>
      <td id="T_d4439_row8_col0" class="data row8 col0" >0.504099</td>
      <td id="T_d4439_row8_col1" class="data row8 col1" >0.418214</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row9" class="row_heading level0 row9" >TAX</th>
      <td id="T_d4439_row9_col0" class="data row9 col0" >0.892471</td>
      <td id="T_d4439_row9_col1" class="data row9 col1" >3.817075</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row10" class="row_heading level0 row10" >PTRATIO</th>
      <td id="T_d4439_row10_col0" class="data row10 col0" >4.545715</td>
      <td id="T_d4439_row10_col1" class="data row10 col1" >9.623649</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row11" class="row_heading level0 row11" >B</th>
      <td id="T_d4439_row11_col0" class="data row11 col0" >0.559243</td>
      <td id="T_d4439_row11_col1" class="data row11 col1" >0.565723</td>
    </tr>
    <tr>
      <th id="T_d4439_level0_row12" class="row_heading level0 row12" >LSTAT</th>
      <td id="T_d4439_row12_col0" class="data row12 col0" >52.643646</td>
      <td id="T_d4439_row12_col1" class="data row12 col1" >58.032887</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>The table represents the feature importance scores for two different regression models: Gradient Boosting (GB) and XGBoost (XGB). These scores provide insights into the relative influence of each feature on the models’ predictions. Here’s an interpretation of the important features based on their feature importance scores:</p>
<ol class="arabic simple">
<li><p><strong>RM (Average number of rooms per dwelling):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 29.28</p></li>
<li><p>Importance_XGB: 17.85</p></li>
<li><p>Interpretation: Both the Gradient Boosting and XGBoost models underscore the importance of the average number of rooms per dwelling (RM) in forecasting housing prices. A higher RM value is correlated with higher property prices.</p></li>
</ul>
</li>
<li><p><strong>LSTAT (Percentage of lower status of the population):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 52.64</p></li>
<li><p>Importance_XGB: 58.03</p></li>
<li><p>Interpretation: Both models prioritize the percentage of lower-status population (LSTAT) as a crucial predictor of housing prices. Regions with a higher percentage of economically disadvantaged residents tend to have lower property values.</p></li>
</ul>
</li>
<li><p><strong>DIS (Weighted distances to five Boston employment centers):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 6.03</p></li>
<li><p>Importance_XGB: 5.77</p></li>
<li><p>Interpretation: Both models recognize the significance of distances to employment centers (DIS) in forecasting housing prices. Proximity to employment hubs typically corresponds to higher property values.</p></li>
</ul>
</li>
<li><p><strong>PTRATIO (Pupil-teacher ratio in public schools):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 4.55</p></li>
<li><p>Importance_XGB: 9.62</p></li>
<li><p>Interpretation: Both models highlight the influence of the pupil-teacher ratio (PTRATIO) on housing prices. Lower PTRATIO values (indicating smaller class sizes) are associated with higher property values.</p></li>
</ul>
</li>
<li><p><strong>NOX (Nitric oxides concentration):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 0.86</p></li>
<li><p>Importance_XGB: 0.53</p></li>
<li><p>Interpretation: Both models acknowledge the impact of nitric oxides concentration (NOX) on housing prices. Elevated NOX levels are often linked to regions with lower property values.</p></li>
</ul>
</li>
<li><p><strong>TAX (Property tax rate):</strong></p>
<ul class="simple">
<li><p>Importance_GB: 0.89</p></li>
<li><p>Importance_XGB: 3.82</p></li>
<li><p>Interpretation: Both models attribute some importance to the property tax rate (TAX) as a predictor of housing prices. Higher tax rates can contribute to decreased property values.</p></li>
</ul>
</li>
</ol>
<p>These interpretations emphasize the consistent ranking of feature importance between the Gradient Boosting and XGBoost models, indicating that certain features, such as the number of rooms, the percentage of lower-status population, and distances to employment centers, consistently emerge as robust predictors of housing prices. The variations in importance scores across the two models reflect their nuanced perspectives while aligning on the significance of key features in shaping housing price predictions.</p>
<p>The differences in feature importance between the two models, particularly in parameters such as LSTAT and RM, can be attributed to the inherent variations in how Gradient Boosting and XGBoost algorithms work, as well as the differences in their hyperparameters and training procedures. Let’s delve into the potential reasons for these discrepancies and how they can be interpreted:</p>
<ol class="arabic simple">
<li><p><strong>Algorithm Differences:</strong></p>
<ul class="simple">
<li><p><strong>Gradient Boosting (GB):</strong> GB builds an ensemble of decision trees sequentially, with each subsequent tree aiming to correct the errors made by its predecessors. The algorithm might place more emphasis on certain features to rectify the model’s mistakes from earlier trees.</p></li>
<li><p><strong>XGBoost (XGB):</strong> XGB is an optimized gradient boosting algorithm that employs a more advanced optimization technique, which could lead to different feature importance calculations due to differences in how the algorithm constructs trees and evaluates feature contributions.</p></li>
</ul>
</li>
<li><p><strong>Hyperparameters:</strong></p>
<ul class="simple">
<li><p><strong>Tree Depth and Complexity:</strong> Hyperparameters such as the maximum depth of trees, learning rate, and regularization terms can influence how much attention the models pay to certain features. Different default hyperparameters or parameter tuning strategies in GB and XGB can lead to varying interpretations of feature importance.</p></li>
</ul>
</li>
<li><p><strong>Data Representation:</strong></p>
<ul class="simple">
<li><p><strong>Internal Data Structures:</strong> GB and XGB might use different internal data representations and processing methods, leading to nuanced differences in how they weigh and evaluate features’ contributions to the target variable.</p></li>
</ul>
</li>
<li><p><strong>Feature Interaction:</strong></p>
<ul class="simple">
<li><p><strong>Interactions with Other Features:</strong> Certain features might interact differently with each other in the context of the algorithms. The importance of LSTAT, for example, could be influenced by how it interacts with other features such as RM, and these interactions can be captured differently by each algorithm.</p></li>
</ul>
</li>
</ol>
<p>Interpreting the Differences:
In the context of LSTAT and RM, their different importance rankings highlight the fact that these features are sensitive to the modeling approach and algorithmic differences. Here’s how you might interpret these discrepancies:</p>
<ol class="arabic simple">
<li><p><strong>LSTAT:</strong></p>
<ul class="simple">
<li><p>Higher Importance in XGB: XGB assigns more importance to LSTAT, indicating that its algorithm might have identified stronger relationships between the percentage of lower-status population and housing prices. XGB might capture nonlinear relationships more effectively, leading to a higher ranking for this feature.</p></li>
</ul>
</li>
<li><p><strong>RM:</strong></p>
<ul class="simple">
<li><p>Higher Importance in GB: GB places more emphasis on RM, suggesting that it found this feature to be a stronger predictor of housing prices within its sequential boosting process. GB might have taken into account specific errors made by previous trees and emphasized RM to address those errors.</p></li>
</ul>
</li>
</ol>
<p><font color='Blue'><b>Example:</b></font> The digits dataset comprises a collection of 8x8 pixel images depicting various numerical digits. Within the dataset, the ‘images’ attribute holds 8x8 arrays representing grayscale values corresponding to each image. For illustrative purposes, we will leverage these arrays to visualize the initial four images. Notably, the ‘target’ attribute in the dataset retains information about the numerical digit portrayed by each image. This informative detail is seamlessly incorporated into the titles of the four plots showcased below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">num2words</span> <span class="kn">import</span> <span class="n">num2words</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.font_manager</span> <span class="kn">import</span> <span class="n">FontProperties</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xticks&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;yticks&#39;</span><span class="p">:</span> <span class="p">[]})</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Set custom font for titles</span>
<span class="n">font</span> <span class="o">=</span> <span class="n">FontProperties</span><span class="p">()</span>
<span class="n">font</span><span class="o">.</span><span class="n">set_family</span><span class="p">(</span><span class="s1">&#39;fantasy&#39;</span><span class="p">)</span>

<span class="c1"># Load the digits dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Iterate over images and labels</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">num2words</span><span class="p">(</span><span class="n">label</span><span class="p">),</span> <span class="n">fontproperties</span><span class="o">=</span><span class="n">font</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Normalize images and set target labels</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span> <span class="o">/</span> <span class="mf">256.0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Create a dictionary to map labels to their word representations</span>
<span class="n">Labels_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="p">[</span><span class="n">num2words</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)]))</span>
<span class="c1"># Adjust layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0a528215dcc6941ddde5d1f99e6b1d8d714229e124a62d029357174c6c47a536.png" src="../_images/0a528215dcc6941ddde5d1f99e6b1d8d714229e124a62d029357174c6c47a536.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>

<span class="c1"># Create the figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>

<span class="c1"># Create the pie chart</span>
<span class="n">wedges</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">pie</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Count&#39;</span><span class="p">],</span>
                                  <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="n">Labels_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">],</span>
                                  <span class="n">autopct</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%1.1f%%</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">startangle</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span>
                                  <span class="n">colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">tab20c</span><span class="o">.</span><span class="n">colors</span><span class="p">,</span>
                                  <span class="n">explode</span><span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                  <span class="n">shadow</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">wedgeprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span> <span class="s1">&#39;whitesmoke&#39;</span><span class="p">})</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of Categories&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>  <span class="c1"># Equal aspect ratio ensures that pie is drawn as a circle.</span>

<span class="c1"># Highlight the labels with annotations</span>
<span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">autotext</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span><span class="p">):</span>
    <span class="n">text</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">text</span><span class="o">.</span><span class="n">set_fontweight</span><span class="p">(</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">autotext</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">autotext</span><span class="o">.</span><span class="n">set_fontweight</span><span class="p">(</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4be5b27c26503bbd7872ec22904ab4ca81007a04e9545a10eff3b5676d6ae49b.png" src="../_images/4be5b27c26503bbd7872ec22904ab4ca81007a04e9545a10eff3b5676d6ae49b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># ------------------------------</span>
<span class="c1"># Gradient Boosting Classifier</span>
<span class="c1"># ------------------------------</span>

<span class="c1"># Create a GradientBoostingClassifier</span>
<span class="n">gb_classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">gb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate the classifier on the test data</span>
<span class="n">gb_accuracy</span> <span class="o">=</span> <span class="n">gb_classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient Boosting Classifier Accuracy: </span><span class="si">{</span><span class="n">gb_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ------------------------------</span>
<span class="c1"># XGBoost Classifier</span>
<span class="c1"># ------------------------------</span>

<span class="c1"># Create an XGBClassifier</span>
<span class="n">xgb_classifier</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">xgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict using the classifier</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>

<span class="c1"># Calculate accuracy using accuracy_score</span>
<span class="n">xgb_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;XGB Classifier Accuracy: </span><span class="si">{</span><span class="n">xgb_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient Boosting Classifier Accuracy: 0.97
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>XGB Classifier Accuracy: 0.96
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Update the Matplotlib settings using the dictionary</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;axes.grid.axis&#39;</span><span class="p">:</span> <span class="s1">&#39;y&#39;</span><span class="p">})</span>

<span class="c1"># Choose indices from the test set to visualize</span>
<span class="n">test_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Create subplots grid</span>
<span class="n">num_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_indices</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_rows</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_indices</span><span class="p">):</span>
    <span class="n">selected_sample</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">true_label</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

    <span class="c1"># Predict probabilities using the GradientBoostingClassifier</span>
    <span class="n">probs_gb</span> <span class="o">=</span> <span class="n">gb_classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">selected_sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Predict probabilities using the XGBClassifier</span>
    <span class="n">probs_xgb</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">selected_sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Visualize the selected sample on the left side</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">selected_sample</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True Label: </span><span class="si">{</span><span class="n">true_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>

    <span class="c1"># Plot probabilities on the right side</span>
    <span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># Adjust this value for spacing</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">-</span> <span class="n">bar_width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">probs_gb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;RoyalBlue&#39;</span><span class="p">,</span>
                  <span class="n">width</span><span class="o">=</span><span class="n">bar_width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Boosting&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">+</span> <span class="n">bar_width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">probs_xgb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;OrangeRed&#39;</span><span class="p">,</span>
                  <span class="n">width</span><span class="o">=</span><span class="n">bar_width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;XGBoost&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)],</span>
                  <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Class Probabilities&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.55</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2509fcc596ee9ae1ae572f14ff2a26d94b981b36c0bdb0c0465cc809b758ce11.png" src="../_images/2509fcc596ee9ae1ae572f14ff2a26d94b981b36c0bdb0c0465cc809b758ce11.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C10S6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10.6. </span>Random Forests</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_11/ReadMe.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Dimensionality Reduction and Feature Selection</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-explanation">10.7.1. Mathematical Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-gradientboostingregressor">10.7.1.1. Regression (GradientBoostingRegressor)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-gradientboostingclassifier">10.7.1.2. Classification (GradientBoostingClassifier)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learns-gradient-boosting-algorithms">10.7.2. scikit-learn’s Gradient Boosting Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost-extreme-gradient-boosting">10.7.3. XGBoost (Extreme Gradient Boosting)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>