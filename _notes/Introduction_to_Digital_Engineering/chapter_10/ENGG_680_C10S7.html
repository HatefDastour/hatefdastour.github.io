

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>10.7. Gradient Boosting &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG_680_C10S7';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Dimensionality Reduction and Feature Selection" href="../chapter_11/ReadMe.html" />
    <link rel="prev" title="10.6. Random Forests" href="ENGG_680_C10S6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">10. Tree-Based Methods</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">13.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gradient Boosting</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting-regressor-and-classifier">10.7.1. Gradient Boosting Regressor and Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-implementation">10.7.2. Scikit-learn Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost-extreme-gradient-boosting">10.7.3. XGBoost (Extreme Gradient Boosting)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gradient-boosting">
<h1><span class="section-number">10.7. </span>Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permalink to this heading">#</a></h1>
<p>Gradient Boosting Algorithm (GBA) is a powerful machine learning technique used to build predictive models. It creates an ensemble of weak learners, meaning that it combines several smaller, simpler models to obtain a more accurate prediction than what an individual model would produce. Gradient boosting works by iteratively training the weak learners on gradient-based functions and incorporating them into the model as “boosted” participants. At its core, gradient boosting works by combining multiple gradient steps to build up a strong predicting model from weak estimators residing in a gradient function space with additional weak learners joining the gradient function space after each iteration of gradient boosting. At each step, gradient descent is used to identify which small components help the function most and are thus added to the overall gradient model. This allows for complicated problems such as data analysis, text processing, and image recognition to be solved with greater accuracy and enhanced performance in <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="../_images/Gradient_Boosting_Trees.jpg"><img alt="../_images/Gradient_Boosting_Trees.jpg" src="../_images/Gradient_Boosting_Trees.jpg" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.11 </span><span class="caption-text">A simplified representation of Gradient Boosting Algorithm (GBA).</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The gradient boosting algorithm is an effective machine learning method used to solve supervised learning problems, such as classification and regression. It is becoming increasingly popular among data scientists due to its ability to increase the accuracy of predictions. The algorithm works by iteratively constructing a series of decision trees, each aimed at rectifying errors made by its predecessors. The algorithm optimizes a loss function, usually the mean squared error for regression or the deviance (log loss) for classification <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
<p>At its core, Gradient Boosting iteratively crafts a series of decision trees, each aimed at rectifying errors made by its predecessors. It optimizes a loss function, usually the mean squared error for regression or the deviance (log loss) for classification. The algorithm operates as follows <span id="id1">[<a class="reference internal" href="../References.html#id5" title="E. Alpaydin. Introduction to Machine Learning, fourth edition. Adaptive Computation and Machine Learning series. MIT Press, 2020. ISBN 9780262043793. URL: https://books.google.ca/books?id=tZnSDwAAQBAJ.">Alpaydin, 2020</a>, <a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<section id="gradient-boosting-regressor-and-classifier">
<h2><span class="section-number">10.7.1. </span>Gradient Boosting Regressor and Classifier<a class="headerlink" href="#gradient-boosting-regressor-and-classifier" title="Permalink to this heading">#</a></h2>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Begin with a model set at a constant value, typically the mean of the target variable, serving as the initial prediction:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-a089ba63-7b44-40a3-bfe5-299fefa08da5">
<span class="eqno">(10.33)<a class="headerlink" href="#equation-a089ba63-7b44-40a3-bfe5-299fefa08da5" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{f}_0(x) = \underset{c}{\mathrm{argmin}} \sum_{i=1}^{n} L(y_i, c)
    \end{equation}\]</div>
<p>The term <span class="math notranslate nohighlight">\( L(y_i, c) \)</span> represents the loss function, quantifying the difference between true target values (<span class="math notranslate nohighlight">\(y_i\)</span>) and predicted values (<span class="math notranslate nohighlight">\(c\)</span>). The algorithm aims to minimize this loss function during training. The model starts with a constant value (<span class="math notranslate nohighlight">\(c\)</span>), often set to the mean of the target variable, seeking the optimal constant value minimizing the overall loss across all data points. The specific form of the loss function (<span class="math notranslate nohighlight">\(L\)</span>) varies based on the problem type:</p>
<ul class="simple">
<li><p>For regression problems, the Mean Squared Error (MSE) is commonly used:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-3b02116c-b65c-4c85-8bb7-5a616f6562c5">
<span class="eqno">(10.34)<a class="headerlink" href="#equation-3b02116c-b65c-4c85-8bb7-5a616f6562c5" title="Permalink to this equation">#</a></span>\[\begin{equation}
    L(y_i, c) = (y_i - c)^2
    \end{equation}\]</div>
<ul class="simple">
<li><p>For classification problems, different loss functions, such as the deviance (log loss), may be employed.</p></li>
</ul>
</li>
<li><p><strong>Boosting Iterations (b = 1 to B):</strong>
a. <strong>Compute Negative Gradient:</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-47f58bd8-58ec-4eaf-b1c4-05152331ce1f">
<span class="eqno">(10.35)<a class="headerlink" href="#equation-47f58bd8-58ec-4eaf-b1c4-05152331ce1f" title="Permalink to this equation">#</a></span>\[\begin{equation}
    r_i = - \left[\frac{\partial L(y_i, \hat{f}(x_i))}{\partial \hat{f}(x_i)}\right]_{\hat{f}(x)=\hat{f}_{b-1}(x)}
    \end{equation}\]</div>
<p>This expression signifies the negative gradient of the loss function (<span class="math notranslate nohighlight">\(L\)</span>) concerning the current model’s predictions (<span class="math notranslate nohighlight">\(\hat{f}(x)\)</span>) at each data point (<span class="math notranslate nohighlight">\(x_i\)</span>). The partial derivative quantifies the sensitivity of the loss to changes in the model’s prediction and is evaluated at the previous iteration’s predictions (<span class="math notranslate nohighlight">\(\hat{f}_{b-1}(x)\)</span>). The negative gradient serves as a pseudo-residual in the training process.</p>
<p>b. <strong>Fit Decision Tree:</strong> Fit a decision tree to the negative gradient (residuals) values (<span class="math notranslate nohighlight">\(r_i\)</span>) as the new target variable. This tree is typically shallow and controlled by hyperparameters like <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>.</p>
<p>c. <strong>Update Model:</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-a5592b94-36db-49ef-abf5-1cb7d0fa42eb">
<span class="eqno">(10.36)<a class="headerlink" href="#equation-a5592b94-36db-49ef-abf5-1cb7d0fa42eb" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{f}_b(x) = \hat{f}_{b-1}(x) + \alpha \cdot \text{new_tree}(x)
    \end{equation}\]</div>
<p>Update the model by incorporating the prediction from the new tree, scaled by a learning rate (<span class="math notranslate nohighlight">\(\alpha\)</span>).</p>
</li>
<li><p><strong>Final Prediction:</strong></p>
<ul class="simple">
<li><p>The ultimate prediction is the sum of all individual trees:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-1cfe8591-4a03-4c33-a14c-6bd9b963380d">
<span class="eqno">(10.37)<a class="headerlink" href="#equation-1cfe8591-4a03-4c33-a14c-6bd9b963380d" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{f}(x) = \sum_{b=1}^{B} \hat{f}_b(x)
    \end{equation}\]</div>
</li>
</ol>
<p>The Gradient Boosting algorithm optimizes a loss function, typically mean squared error for regression or deviance (log loss) for classification <span id="id2">[<a class="reference internal" href="../References.html#id5" title="E. Alpaydin. Introduction to Machine Learning, fourth edition. Adaptive Computation and Machine Learning series. MIT Press, 2020. ISBN 9780262043793. URL: https://books.google.ca/books?id=tZnSDwAAQBAJ.">Alpaydin, 2020</a>, <a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="../_images/Gradient_Boosting_Flowchart.jpg"><img alt="../_images/Gradient_Boosting_Flowchart.jpg" src="../_images/Gradient_Boosting_Flowchart.jpg" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.12 </span><span class="caption-text">A visual representation of the above description.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="scikit-learn-implementation">
<h2><span class="section-number">10.7.2. </span>Scikit-learn Implementation<a class="headerlink" href="#scikit-learn-implementation" title="Permalink to this heading">#</a></h2>
<p>Scikit-learn offers the <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> classes for regression and classification tasks. These classes allow customization of hyperparameters like learning rate, tree depth, and the number of boosting iterations (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>). Here’s how to use them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>

<span class="c1"># For regression</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># For classification</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Utilizing these classes simplifies the implementation of Gradient Boosting for regression and classification tasks within scikit-learn.</p>
<div class="tip admonition">
<p class="admonition-title">scikit-learn’s Gradient Boosting Algorithms</p>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Initialize the model’s prediction as a constant value, often the mean of the target variable for regression or class probabilities for classification.</p></li>
<li><p>Compute the initial negative gradient (residuals) for regression or the negative gradient of the log-loss for classification.</p></li>
</ul>
</li>
<li><p><strong>Boosting Iterations:</strong></p>
<ul>
<li><p>For each boosting iteration (b = 1 to B), perform the following steps:</p>
<p>a. <strong>Construct a Decision Tree:</strong></p>
<ul class="simple">
<li><p>Fit a decision tree to the negative gradient (residuals) or the negative gradient of the log-loss values as the new target variable.</p></li>
<li><p>The decision tree typically has a limited depth (controlled by hyperparameters like <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>).</p></li>
</ul>
<p>b. <strong>Update the Model:</strong></p>
<ul>
<li><p>Compute the prediction from the new decision tree and scale it by a learning rate (α).</p></li>
<li><p>Update the model’s prediction by adding the scaled tree prediction:</p>
<div class="amsmath math notranslate nohighlight" id="equation-abfc7b69-c20f-456a-8f57-00d224b84935">
<span class="eqno">(10.38)<a class="headerlink" href="#equation-abfc7b69-c20f-456a-8f57-00d224b84935" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{f}_b(x) = \hat{f}_{b-1}(x) + \alpha \cdot \text{new_tree}(x) \end{equation}\]</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Final Prediction:</strong></p>
<ul>
<li><p>The final prediction is the sum of predictions from all individual trees:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f8080086-33ab-4903-9e18-cd24beda93a5">
<span class="eqno">(10.39)<a class="headerlink" href="#equation-f8080086-33ab-4903-9e18-cd24beda93a5" title="Permalink to this equation">#</a></span>\[\begin{equation} \hat{f}(x) = \sum_{b=1}^{B} \hat{f}_b(x) \end{equation}\]</div>
</li>
</ul>
</li>
<li><p><strong>Classification Specifics:</strong></p>
<ul class="simple">
<li><p>For classification tasks, the class probabilities are often transformed into class labels using a threshold or argmax operation.</p></li>
</ul>
</li>
<li><p><strong>Hyperparameters:</strong></p>
<ul class="simple">
<li><p>The algorithm’s behavior is influenced by hyperparameters like the number of boosting iterations (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>), learning rate (<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>), maximum tree depth (<code class="docutils literal notranslate"><span class="pre">max_depth</span></code>), and more.</p></li>
</ul>
</li>
<li><p><strong>Regularization:</strong></p>
<ul class="simple">
<li><p>Regularization techniques, such as early stopping and subsampling, are commonly used to prevent overfitting and improve efficiency.</p></li>
</ul>
</li>
<li><p><strong>Prediction and Evaluation:</strong></p>
<ul class="simple">
<li><p>Use the trained model to make predictions on new data. For classification, the class with the highest probability (or predicted label) is assigned.</p></li>
<li><p>Evaluate the model’s performance using appropriate metrics for regression or classification tasks.</p></li>
</ul>
</li>
</ol>
<p>Remember that while this description provides an overview of the algorithm, the actual implementation in scikit-learn includes optimizations and additional features. For comprehensive details, you can refer to scikit-learn’s documentation on <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
</div>
</section>
<section id="xgboost-extreme-gradient-boosting">
<h2><span class="section-number">10.7.3. </span>XGBoost (Extreme Gradient Boosting)<a class="headerlink" href="#xgboost-extreme-gradient-boosting" title="Permalink to this heading">#</a></h2>
<p>XGBoost (Extreme Gradient Boosting) is a highly optimized and popular gradient boosting framework that excels in performance and predictive accuracy. Developed by Tianqi Chen, XGBoost has become one of the go-to choices for structured data in machine learning competitions and real-world applications. It extends the basic gradient boosting algorithm by introducing enhancements and fine-tuning to achieve superior results. Here’s an overview of the XGBoost algorithm <span id="id3">[<a class="reference internal" href="../References.html#id209" title="xgboost developers. Xgboost documentation. https://xgboost.readthedocs.io/en/stable/, 2023. [Online; accessed 01-August-2023].">xgboost developers, 2023</a>]</span>:</p>
<p><strong>Gradient Boosting Framework:</strong>
XGBoost shares the fundamental idea of gradient boosting. It iteratively builds an ensemble of weak learners, usually decision trees, to form a powerful predictive model. Each weak learner attempts to correct the errors made by its predecessors.</p>
<p><strong>Key Features of XGBoost:</strong>
XGBoost introduces several innovations that contribute to its remarkable performance:</p>
<ol class="arabic simple">
<li><p><strong>Regularization:</strong> XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in the optimization objective to control model complexity and prevent overfitting.</p></li>
<li><p><strong>Custom Loss Functions:</strong> XGBoost allows the use of custom loss functions, extending its applicability to various problem domains.</p></li>
<li><p><strong>Handling Missing Values:</strong> XGBoost can automatically handle missing values during tree construction.</p></li>
<li><p><strong>Feature Importance:</strong> XGBoost provides insights into feature importance, aiding in understanding the model’s decision-making process.</p></li>
<li><p><strong>Built-in Cross-Validation:</strong> XGBoost includes a built-in cross-validation function for model assessment and hyperparameter tuning.</p></li>
<li><p><strong>Parallel and Distributed Computing:</strong> XGBoost supports parallel and distributed computing to accelerate training on large datasets.</p></li>
<li><p><strong>Pruning:</strong> XGBoost employs a technique called “pruning” to remove splits that lead to negative gains during tree growth, improving efficiency.</p></li>
</ol>
<p><strong>XGBoost Algorithm:</strong>
The XGBoost algorithm, like gradient boosting, consists of iterative steps to build an ensemble of decision trees. The main steps are as follows:</p>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Start with a constant prediction (often the mean of the target variable for regression) for all instances.</p></li>
<li><p>Compute the initial gradient (negative gradient of the loss function) for each instance.</p></li>
</ul>
</li>
<li><p><strong>Boosting Iterations:</strong></p>
<ul>
<li><p>For each boosting iteration (b = 1 to B), perform these steps:</p>
<p>a. <strong>Construct a Decision Tree:</strong></p>
<ul class="simple">
<li><p>Fit a decision tree to the negative gradient values, incorporating regularization terms.</p></li>
<li><p>Prune the tree using depth and gain-based criteria.</p></li>
</ul>
<p>b. <strong>Update the Model:</strong></p>
<ul class="simple">
<li><p>Compute the prediction from the new tree and scale it by a learning rate (α).</p></li>
<li><p>Update the model’s prediction by adding the scaled tree prediction.</p></li>
</ul>
<p>c. <strong>Update the Gradient:</strong></p>
<ul class="simple">
<li><p>Compute the new gradient using the residuals (negative gradient) from the previous iteration.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Final Prediction:</strong></p>
<ul class="simple">
<li><p>The final prediction is the sum of predictions from all individual trees.</p></li>
</ul>
</li>
</ol>
<p><strong>XGBoost Implementation:</strong>
XGBoost is implemented as a Python package with wrappers for various programming languages. The Python API allows you to create instances of <code class="docutils literal notranslate"><span class="pre">xgboost.XGBRegressor</span></code> for regression tasks and <code class="docutils literal notranslate"><span class="pre">xgboost.XGBClassifier</span></code> for classification tasks. These classes provide a wide range of hyperparameters for fine-tuning, such as learning rate, maximum depth, and regularization terms.</p>
<p>XGBoost’s flexibility, scalability, and performance make it a popular choice for many machine learning projects, especially when working with structured data. Its advanced features and optimizations contribute to its effectiveness in achieving high predictive accuracy. For comprehensive details, you can refer to the official XGBoost documentation and resources.</p>
<p><font color='Blue'><b>Example</b></font>. Consider the Auto MPG dataset retrieved from the <a class="reference external" href="http://archive.ics.uci.edu/dataset/9/auto+mpg">UCI Machine Learning Repository</a>. We will now apply these concepts by actively engaging with this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip</span>

<span class="c1"># Define column names based on the dataset&#39;s description</span>
<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">,</span> <span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span> <span class="s1">&#39;Displacement&#39;</span><span class="p">,</span> <span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Acceleration&#39;</span><span class="p">,</span> <span class="s1">&#39;Model_Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Origin&#39;</span><span class="p">,</span> <span class="s1">&#39;Car_Name&#39;</span><span class="p">]</span>

<span class="c1"># Read the dataset with column names, treating &#39;?&#39; as missing values, and remove rows with missing values</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;auto-mpg.data&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
                          <span class="n">na_values</span><span class="o">=</span><span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Remove the &#39;Car_Name&#39; column from the DataFrame</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Car_Name&#39;</span><span class="p">])</span>

<span class="c1"># Display the resulting DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MPG</th>
      <th>Cylinders</th>
      <th>Displacement</th>
      <th>Horsepower</th>
      <th>Weight</th>
      <th>Acceleration</th>
      <th>Model_Year</th>
      <th>Origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504.0</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693.0</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436.0</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433.0</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449.0</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>387</th>
      <td>27.0</td>
      <td>4</td>
      <td>140.0</td>
      <td>86.0</td>
      <td>2790.0</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>388</th>
      <td>44.0</td>
      <td>4</td>
      <td>97.0</td>
      <td>52.0</td>
      <td>2130.0</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
    </tr>
    <tr>
      <th>389</th>
      <td>32.0</td>
      <td>4</td>
      <td>135.0</td>
      <td>84.0</td>
      <td>2295.0</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>390</th>
      <td>28.0</td>
      <td>4</td>
      <td>120.0</td>
      <td>79.0</td>
      <td>2625.0</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>391</th>
      <td>31.0</td>
      <td>4</td>
      <td>119.0</td>
      <td>82.0</td>
      <td>2720.0</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>392 rows × 8 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="c1"># Extract the features (X) and target variable (y)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">MPG</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>  <span class="c1"># Take the natural logarithm of the MPG values</span>

<span class="c1"># Split the dataset into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">set_size_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)]},</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="s1">&#39;Test&#39;</span><span class="p">])</span>
<span class="n">display</span><span class="p">(</span><span class="n">set_size_df</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Train</th>
      <th>Test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Size</th>
      <td>294</td>
      <td>98</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">sklearnex</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">patch_sklearn</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Create a figure and subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">feature_set_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Using sklearn GradientBoostingRegressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Using xgboost.XGBRegressor&#39;</span><span class="p">]</span>

<span class="c1"># Loop through different regressors</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">regressor_class</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">]):</span>
    <span class="c1"># Create a regressor</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">regressor_class</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">)</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Create scatter plot and a diagonal reference line</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;medv&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;SkyBlue&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;--k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    
    <span class="c1"># Set title and labels for the current subplot</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">feature_set_labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Predicted Values&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Actual Values&#39;</span><span class="p">)</span>   
    
    <span class="c1"># Calculate and display Mean Squared Error (MSE) with a background color</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">feature_set_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">: MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.74</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                      <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
                      <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;Whitesmoke&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">))</span>  <span class="c1"># Add background color</span>
    
    <span class="c1"># Set an equal aspect ratio for the subplots</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="c1"># Adjust the layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">unpatch_sklearn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using sklearn GradientBoostingRegressor: MSE = 0.0140
Using xgboost.XGBRegressor: MSE = 0.0162
</pre></div>
</div>
<img alt="../_images/0cbdc2c4aa8be9e6063ecbc4604fd8bf6fc5910891c4d60b9fec48dab28b36b7.png" src="../_images/0cbdc2c4aa8be9e6063ecbc4604fd8bf6fc5910891c4d60b9fec48dab28b36b7.png" />
</div>
</div>
<p>Two models using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">xgboost</span></code>’s <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code> with default parameters. Both models were trained on the same dataset, which includes the following features: <code class="docutils literal notranslate"><span class="pre">Cylinders</span></code>, <code class="docutils literal notranslate"><span class="pre">Displacement</span></code>, <code class="docutils literal notranslate"><span class="pre">Horsepower</span></code>, <code class="docutils literal notranslate"><span class="pre">Weight</span></code>, <code class="docutils literal notranslate"><span class="pre">Acceleration</span></code>, <code class="docutils literal notranslate"><span class="pre">Model_Year</span></code>, and <code class="docutils literal notranslate"><span class="pre">Origin</span></code>. The target variable is <code class="docutils literal notranslate"><span class="pre">np.log(MPG)</span></code>.</p>
<p>The <strong>mean squared error (MSE)</strong> is a measure of how well the model fits the data. A lower MSE indicates a better fit. The MSE values you provided are as follows:</p>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>: <strong>MSE = 0.0162</strong></p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">xgboost</span></code>’s <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code>: <strong>MSE = 0.0140</strong></p></li>
</ul>
<p>Both models seem to have performed well, with the <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code> model having a slightly lower MSE than the <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> model. However, it’s important to note that the performance of a model depends on various factors such as the quality of the data, the choice of features, and the hyperparameters used for training the model. Therefore, it’s recommended to perform a thorough analysis of the models before drawing any conclusions.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>“Scikit-learn extensions” or “sklearnex” refers to additional modules or libraries that build upon the scikit-learn library, which is a machine learning library for Python. These extensions typically provide extra functionality, new algorithms, or improved features to enhance the capabilities of scikit-learn in various ways. The term “sklearnex” may encompass a range of third-party contributions aimed at extending and complementing the existing scikit-learn ecosystem.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">sklearnex.patch_sklearn()</span></code> is a method within the scikit-learn extensions framework. Its primary purpose is to patch or modify the behavior of the scikit-learn library by incorporating additional functionalities or improvements provided by sklearnex.</p>
<p>This function is typically employed to seamlessly integrate the extensions into the scikit-learn library, ensuring that the enhanced features or modifications become part of the standard scikit-learn functionality. By invoking <code class="docutils literal notranslate"><span class="pre">sklearnex.patch_sklearn()</span></code>, users can apply the necessary adjustments to the scikit-learn library, enabling the utilization of extended capabilities offered by the sklearnex framework within their machine learning workflows. Additional information can be found by referring to the documentation available <a class="reference external" href="https://github.com/intel/scikit-learn-intelex">here</a>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearnex</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">patch_sklearn</span><span class="p">()</span>

<span class="c1"># Initialize and train the models</span>
<span class="c1"># Initialize the GradientBoostingRegressor model</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">reg_gb</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span> <span class="n">random_state</span><span class="p">)</span>
<span class="n">reg_gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Initialize the XGBRegressor model</span>
<span class="n">reg_xgb</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">)</span>
<span class="n">reg_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Create DataFrames with feature importances</span>
<span class="c1"># Create a DataFrame for feature importances using GradientBoostingRegressor</span>
<span class="n">importance_gb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">reg_gb</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="o">*</span> <span class="mi">100</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Create a DataFrame for feature importances using XGBRegressor</span>
<span class="n">importance_xgb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Importance&#39;</span><span class="p">:</span> <span class="n">reg_xgb</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="o">*</span> <span class="mi">100</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Create subplots</span>
<span class="c1"># Create a figure with two vertically stacked subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Plot feature importance for GradientBoostingRegressor</span>
<span class="c1"># Create a bar plot for feature importance in the first subplot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">importance_gb</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">importance_gb</span><span class="o">.</span><span class="n">Importance</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#99f599&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;#006400&#39;</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s2">&quot;///&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature Importance:</span><span class="se">\n</span><span class="s1">Gradient Boosting Regressor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DarkSlateGray&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>

<span class="c1"># Plot feature importance for XGBRegressor</span>
<span class="c1"># Create a bar plot for feature importance in the second subplot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">importance_xgb</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">importance_xgb</span><span class="o">.</span><span class="n">Importance</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e9aaaa&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;#8B0000&#39;</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\\\\</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature Importance:</span><span class="se">\n</span><span class="s1">XGBoost Regressor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DarkSlateGray&#39;</span><span class="p">)</span>

<span class="c1"># Common settings for both subplots</span>
<span class="c1"># Iterate through the axes and apply common settings</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;DimGray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Importance (%)&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;MidnightBlue&#39;</span><span class="p">)</span>

<span class="c1"># Remove the ylabel for the right plot</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">unpatch_sklearn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/17947b6a5916486ffeb300d7f1e27badc69bb0338a71a334e7e9972ff845a312.png" src="../_images/17947b6a5916486ffeb300d7f1e27badc69bb0338a71a334e7e9972ff845a312.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Merge importance_gb and importance_xgb DataFrames</span>
<span class="n">merged_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">importance_gb</span><span class="p">,</span> <span class="n">importance_xgb</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">suffixes</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;_GB&#39;</span><span class="p">,</span> <span class="s1">&#39;_XGB&#39;</span><span class="p">))</span>

<span class="c1"># Display the merged DataFrame with background gradient</span>
<span class="n">display</span><span class="p">(</span><span class="n">merged_importance</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlGnBu&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_bdea0_row0_col0 {
  background-color: #3cb1c3;
  color: #f1f1f1;
}
#T_bdea0_row0_col1 {
  background-color: #f4fbc0;
  color: #000000;
}
#T_bdea0_row1_col0 {
  background-color: #97d6b9;
  color: #000000;
}
#T_bdea0_row1_col1 {
  background-color: #2262aa;
  color: #f1f1f1;
}
#T_bdea0_row2_col0 {
  background-color: #b4e2b6;
  color: #000000;
}
#T_bdea0_row2_col1 {
  background-color: #42b6c4;
  color: #f1f1f1;
}
#T_bdea0_row3_col0 {
  background-color: #081d58;
  color: #f1f1f1;
}
#T_bdea0_row3_col1 {
  background-color: #61c2bf;
  color: #000000;
}
#T_bdea0_row4_col0 {
  background-color: #fcfed1;
  color: #000000;
}
#T_bdea0_row4_col1 {
  background-color: #f8fcca;
  color: #000000;
}
#T_bdea0_row5_col0 {
  background-color: #c6e9b4;
  color: #000000;
}
#T_bdea0_row5_col1 {
  background-color: #2da2c2;
  color: #f1f1f1;
}
#T_bdea0_row6_col0 {
  background-color: #ffffd9;
  color: #000000;
}
#T_bdea0_row6_col1 {
  background-color: #f8fcc9;
  color: #000000;
}
</style>
<table id="T_bdea0">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_bdea0_level0_col0" class="col_heading level0 col0" >Importance_GB</th>
      <th id="T_bdea0_level0_col1" class="col_heading level0 col1" >Importance_XGB</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_bdea0_level0_row0" class="row_heading level0 row0" >Cylinders</th>
      <td id="T_bdea0_row0_col0" class="data row0 col0" >21.36</td>
      <td id="T_bdea0_row0_col1" class="data row0 col1" >3.36</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row1" class="row_heading level0 row1" >Displacement</th>
      <td id="T_bdea0_row1_col0" class="data row1 col0" >13.90</td>
      <td id="T_bdea0_row1_col1" class="data row1 col1" >30.40</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row2" class="row_heading level0 row2" >Horsepower</th>
      <td id="T_bdea0_row2_col0" class="data row2 col0" >11.79</td>
      <td id="T_bdea0_row2_col1" class="data row2 col1" >20.48</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row3" class="row_heading level0 row3" >Weight</th>
      <td id="T_bdea0_row3_col0" class="data row3 col0" >41.09</td>
      <td id="T_bdea0_row3_col1" class="data row3 col1" >17.92</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row4" class="row_heading level0 row4" >Acceleration</th>
      <td id="T_bdea0_row4_col0" class="data row4 col0" >1.21</td>
      <td id="T_bdea0_row4_col1" class="data row4 col1" >2.09</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row5" class="row_heading level0 row5" >Model_Year</th>
      <td id="T_bdea0_row5_col0" class="data row5 col0" >10.49</td>
      <td id="T_bdea0_row5_col1" class="data row5 col1" >23.39</td>
    </tr>
    <tr>
      <th id="T_bdea0_level0_row6" class="row_heading level0 row6" >Origin</th>
      <td id="T_bdea0_row6_col0" class="data row6 col0" >0.16</td>
      <td id="T_bdea0_row6_col1" class="data row6 col1" >2.35</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p><strong>Feature Importance Comparison Report</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>: The feature importance values are calculated based on the <strong>mean decrease impurity (MDI)</strong> method. The MDI method computes the total reduction of the criterion brought by a feature to all the trees in the ensemble. The higher the value, the more important the feature is in predicting the target variable. In this case, the most important feature is <code class="docutils literal notranslate"><span class="pre">Weight</span></code> with an importance value of 41.09, followed by <code class="docutils literal notranslate"><span class="pre">Cylinders</span></code> with an importance value of 21.36. The <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> estimator fits a sequence of regression trees on the negative gradient of the loss function. The <code class="docutils literal notranslate"><span class="pre">loss</span></code> parameter specifies the loss function to be optimized, and the <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> parameter shrinks the contribution of each tree by <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. The <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> parameter specifies the number of boosting stages to perform. The <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> parameter controls the maximum depth of the individual regression trees. The <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> parameter specifies the minimum number of samples required to split an internal node, and the <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> parameter specifies the minimum number of samples required to be at a leaf node <span id="id4">[<a class="reference internal" href="../References.html#id209" title="xgboost developers. Xgboost documentation. https://xgboost.readthedocs.io/en/stable/, 2023. [Online; accessed 01-August-2023].">xgboost developers, 2023</a>]</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code>: The feature importance values are calculated based on the <strong>gain</strong> method. The gain method computes the average gain of each feature when it is used in splits. The higher the value, the more important the feature is in predicting the target variable. In this case, the most important feature is <code class="docutils literal notranslate"><span class="pre">Displacement</span></code> with an importance value of 30.40, followed by <code class="docutils literal notranslate"><span class="pre">Model_Year</span></code> with an importance value of 23.39. The <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code> estimator fits a sequence of regression trees on the negative gradient of the loss function. The <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> parameter shrinks the contribution of each tree by <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, and the <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> parameter specifies the number of boosting stages to perform. The <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> parameter controls the maximum depth of the individual regression trees. The <code class="docutils literal notranslate"><span class="pre">min_child_weight</span></code> parameter specifies the minimum sum of instance weight (hessian) needed in a child. The <code class="docutils literal notranslate"><span class="pre">gamma</span></code> parameter specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. The <code class="docutils literal notranslate"><span class="pre">subsample</span></code> parameter specifies the fraction of samples to be used for fitting the individual base learners. The <code class="docutils literal notranslate"><span class="pre">colsample_bytree</span></code> parameter specifies the fraction of columns to be randomly sampled for each tree. The <code class="docutils literal notranslate"><span class="pre">reg_alpha</span></code> parameter specifies L1 regularization term on weights, and the <code class="docutils literal notranslate"><span class="pre">reg_lambda</span></code> parameter specifies L2 regularization term on weights <span id="id5">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>It’s important to note that feature importance does not necessarily imply causation or directionality. Instead, it indicates the relative importance of each feature in predicting the target variable. The feature importance values can be used to identify the most relevant features for the task at hand and to gain insights into the underlying data. However, it’s important to keep in mind that the feature importance values are based on the specific model and dataset used, and may not generalize to other models or datasets. Therefore, it’s recommended to perform a thorough analysis of the models and data before drawing any conclusions based on the feature importance values <span id="id6">[<a class="reference internal" href="../References.html#id77" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>]</span>.</p>
</div>
<p><font color='Blue'><b>Example:</b></font> The <a class="reference external" href="https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits">digits dataset</a> comprises a collection of 8x8 pixel images depicting various numerical digits. Within the dataset, the ‘images’ attribute holds 8x8 arrays representing grayscale values corresponding to each image. For illustrative purposes, we will leverage these arrays to visualize the initial four images. Notably, the ‘target’ attribute in the dataset retains information about the numerical digit portrayed by each image. This informative detail is seamlessly incorporated into the titles of the four plots showcased below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.font_manager</span> <span class="kn">import</span> <span class="n">FontProperties</span>

<span class="c1"># Create a dictionary to map labels to their word representations</span>
<span class="n">Labels_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="p">[</span><span class="s1">&#39;Zero&#39;</span><span class="p">,</span><span class="s1">&#39;One&#39;</span><span class="p">,</span><span class="s1">&#39;Two&#39;</span><span class="p">,</span><span class="s1">&#39;Three&#39;</span><span class="p">,</span><span class="s1">&#39;Four&#39;</span><span class="p">,</span><span class="s1">&#39;Five&#39;</span><span class="p">,</span><span class="s1">&#39;Six&#39;</span><span class="p">,</span><span class="s1">&#39;Seven&#39;</span><span class="p">,</span><span class="s1">&#39;Eight&#39;</span><span class="p">,</span><span class="s1">&#39;Nine&#39;</span><span class="p">]))</span>


<span class="c1"># Create subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xticks&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;yticks&#39;</span><span class="p">:</span> <span class="p">[]})</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Set custom font for titles</span>
<span class="n">font</span> <span class="o">=</span> <span class="n">FontProperties</span><span class="p">()</span>
<span class="n">font</span><span class="o">.</span><span class="n">set_family</span><span class="p">(</span><span class="s1">&#39;fantasy&#39;</span><span class="p">)</span>

<span class="c1"># Load the digits dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Iterate over images and labels</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">Labels_dict</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontproperties</span><span class="o">=</span><span class="n">font</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Normalize images and set target labels</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span> <span class="o">/</span> <span class="mf">256.0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Adjust layout and display the plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1fc60ab600d5ad786ee0b55f0c44849c8d091332d2a156e53dca5efc23b8f96c.png" src="../_images/1fc60ab600d5ad786ee0b55f0c44849c8d091332d2a156e53dca5efc23b8f96c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">_dist_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Labels_dict</span><span class="o">=</span><span class="n">Labels_dict</span><span class="p">,</span> <span class="n">CM</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">tab20c</span><span class="o">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a pie chart illustrating the distribution of categories.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - ax: Axes object to plot on.</span>
<span class="sd">    - y: Input data for which the distribution is to be visualized.</span>
<span class="sd">    - Labels_dict: Dictionary mapping category indices to labels.</span>
<span class="sd">    - CM: Color map for the pie chart.</span>
<span class="sd">    - title: Title for the plot. Set to False to omit.</span>

<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Prepare data for the pie chart</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>

    <span class="c1"># Create the pie chart</span>
    <span class="n">wedges</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">pie</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Count&#39;</span><span class="p">],</span>
                                      <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="n">Labels_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">],</span>
                                      <span class="n">autopct</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%1.1f%%</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">startangle</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span>
                                      <span class="n">colors</span><span class="o">=</span><span class="n">CM</span><span class="p">,</span>
                                      <span class="n">explode</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                      <span class="n">shadow</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">wedgeprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span><span class="p">:</span> <span class="s1">&#39;whitesmoke&#39;</span><span class="p">})</span>
    <span class="c1"># Set title and ensure equal aspect ratio for a circular pie chart</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

    <span class="c1"># Highlight the labels with annotations</span>
    <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">autotext</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">autotexts</span><span class="p">):</span>
        <span class="n">text</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">text</span><span class="o">.</span><span class="n">set_fontweight</span><span class="p">(</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
        <span class="n">autotext</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">autotext</span><span class="o">.</span><span class="n">set_fontweight</span><span class="p">(</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Create the figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">_dist_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Distribution of Categories&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7cdabbd6c864a798cafcd90443dfd8f53415de82ba839ac656c81ef21434dc79.png" src="../_images/7cdabbd6c864a798cafcd90443dfd8f53415de82ba839ac656c81ef21434dc79.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Create the figure and axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">_dist_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">CM</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Pastel1</span><span class="o">.</span><span class="n">colors</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Train Set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Green&#39;</span><span class="p">)</span>
<span class="n">_dist_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">CM</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Pastel2</span><span class="o">.</span><span class="n">colors</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Test Set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Blue&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Distribution of Categories&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Adjust layout and display the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/81a4696a2df6d5b940f369380b394ebf73c750f1108b1543da55db1f524dc700.png" src="../_images/81a4696a2df6d5b940f369380b394ebf73c750f1108b1543da55db1f524dc700.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">sklearnex</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">patch_sklearn</span><span class="p">()</span>
<span class="c1"># ------------------------------</span>
<span class="c1"># Gradient Boosting Classifier</span>
<span class="c1"># ------------------------------</span>

<span class="c1"># Create a GradientBoostingClassifier</span>
<span class="n">gb_classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">gb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate the classifier on the test data</span>
<span class="n">gb_accuracy</span> <span class="o">=</span> <span class="n">gb_classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient Boosting Classifier Accuracy: </span><span class="si">{</span><span class="n">gb_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ------------------------------</span>
<span class="c1"># XGBoost Classifier</span>
<span class="c1"># ------------------------------</span>

<span class="c1"># Create an XGBClassifier</span>
<span class="n">xgb_classifier</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fit the classifier to the training data</span>
<span class="n">xgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict using the classifier</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>

<span class="c1"># Calculate accuracy using accuracy_score</span>
<span class="n">xgb_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;XGB Classifier Accuracy: </span><span class="si">{</span><span class="n">xgb_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">sklearnex</span><span class="o">.</span><span class="n">unpatch_sklearn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient Boosting Classifier Accuracy: 0.9583
XGB Classifier Accuracy: 0.9611
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Update the Matplotlib settings using the dictionary</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;axes.grid.axis&#39;</span><span class="p">:</span> <span class="s1">&#39;y&#39;</span><span class="p">})</span>

<span class="c1"># Choose indices from the test set to visualize</span>
<span class="n">test_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Create subplots grid</span>
<span class="n">num_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_indices</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_rows</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_indices</span><span class="p">):</span>
    <span class="n">selected_sample</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">true_label</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

    <span class="c1"># Predict probabilities using the GradientBoostingClassifier</span>
    <span class="n">probs_gb</span> <span class="o">=</span> <span class="n">gb_classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">selected_sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Predict probabilities using the XGBClassifier</span>
    <span class="n">probs_xgb</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">selected_sample</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Visualize the selected sample on the left side</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">selected_sample</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True Label: </span><span class="si">{</span><span class="n">true_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>

    <span class="c1"># Plot probabilities on the right side</span>
    <span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># Adjust this value for spacing</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">-</span> <span class="n">bar_width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">probs_gb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;RoyalBlue&#39;</span><span class="p">,</span>
                  <span class="n">width</span><span class="o">=</span><span class="n">bar_width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Boosting&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">+</span> <span class="n">bar_width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">probs_xgb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;OrangeRed&#39;</span><span class="p">,</span>
                  <span class="n">width</span><span class="o">=</span><span class="n">bar_width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;XGBoost&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)],</span>
                  <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Class Probabilities&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.55</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7f85d4a0fefdf62a32dd3a62908572d12bb04dac3c45fd9436bcab5558ae51af.png" src="../_images/7f85d4a0fefdf62a32dd3a62908572d12bb04dac3c45fd9436bcab5558ae51af.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C10S6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10.6. </span>Random Forests</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_11/ReadMe.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Dimensionality Reduction and Feature Selection</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting-regressor-and-classifier">10.7.1. Gradient Boosting Regressor and Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-implementation">10.7.2. Scikit-learn Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost-extreme-gradient-boosting">10.7.3. XGBoost (Extreme Gradient Boosting)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>