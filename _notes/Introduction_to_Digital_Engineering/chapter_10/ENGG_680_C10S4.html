

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>10.4. Regression Trees and Linear Models (Optional Section) &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG_680_C10S4';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)" href="ENGG_680_C10S5.html" />
    <link rel="prev" title="10.3. Classification Trees" href="ENGG_680_C10S3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">10. Tree-Based Methods</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">13.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression Trees and Linear Models (Optional Section)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">10.4.1. Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">10.4.2. Regression Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-distinctions">10.4.3. Key Distinctions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-material-strength-prediction">10.4.4. Example: Material Strength Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-binary-classification">10.4.5. Example - Binary Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">10.4.5.1. Comparison</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-multiclass-classification">10.4.6. Example - Multiclass Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-between-regression-trees-and-linear-models">10.4.7. Choosing Between Regression Trees and Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">10.4.7.1. Regression Trees:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models">10.4.7.2. Linear Models:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-between-them">10.4.7.3. Choosing Between Them:</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression-trees-and-linear-models-optional-section">
<h1><span class="section-number">10.4. </span>Regression Trees and Linear Models (Optional Section)<a class="headerlink" href="#regression-trees-and-linear-models-optional-section" title="Permalink to this heading">#</a></h1>
<p>When we compare regression and classification trees to traditional methods, the differences become quite pronounced, especially when we contrast them with linear models, particularly linear regression <span id="id1">[<a class="reference internal" href="../References.html#id98" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<section id="linear-regression">
<h2><span class="section-number">10.4.1. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear regression is a statistical modeling technique grounded in a linear relationship between the predictor variables (<span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_p\)</span>) and the predicted outcome (<span class="math notranslate nohighlight">\(f(X)\)</span>). The model is expressed by the equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc79835a-5c65-4323-ad88-7d160480cc42">
<span class="eqno">(10.18)<a class="headerlink" href="#equation-bc79835a-5c65-4323-ad88-7d160480cc42" title="Permalink to this equation">#</a></span>\[\begin{equation} f(X) = \beta_0 + \sum_{j=1}^{p}X_j \beta_j \end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept term, and <span class="math notranslate nohighlight">\(\beta_j\)</span> are the coefficients associated with each predictor variable. The predicted outcome <span class="math notranslate nohighlight">\(f(X)\)</span> is a linear combination of these variables and their respective coefficients. This linear approach is particularly effective when the underlying relationships in the data can be accurately represented by a straight line.</p>
</section>
<section id="regression-trees">
<h2><span class="section-number">10.4.2. </span>Regression Trees<a class="headerlink" href="#regression-trees" title="Permalink to this heading">#</a></h2>
<p>In contrast, regression trees adopt a distinctive modeling structure. The predictive function <span class="math notranslate nohighlight">\(f(X)\)</span> for regression trees is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-169740c2-94ae-45fc-8170-406bbf2ffdb8">
<span class="eqno">(10.19)<a class="headerlink" href="#equation-169740c2-94ae-45fc-8170-406bbf2ffdb8" title="Permalink to this equation">#</a></span>\[\begin{equation} f(X) = \sum_{m=1}^{M}c_m \cdot \mathbb{I}_{X\in R_m} \end{equation}\]</div>
<p>In this equation, <span class="math notranslate nohighlight">\(M\)</span> represents the number of terminal nodes or regions in the tree, <span class="math notranslate nohighlight">\(c_m\)</span> denotes the constant associated with the <span class="math notranslate nohighlight">\(m\)</span>-th region, and <span class="math notranslate nohighlight">\(\mathbb{I}_{X\in R_m}\)</span> is an indicator function that equals 1 if <span class="math notranslate nohighlight">\(X\)</span> falls within region <span class="math notranslate nohighlight">\(R_m\)</span> and 0 otherwise. Unlike linear regression, regression trees accommodate non-linear relationships in the data by partitioning the feature space into distinct regions, each associated with a unique constant.</p>
</section>
<section id="key-distinctions">
<h2><span class="section-number">10.4.3. </span>Key Distinctions<a class="headerlink" href="#key-distinctions" title="Permalink to this heading">#</a></h2>
<p>The fundamental distinction lies in the modeling approach. Linear regression assumes a linear relationship between the predictors and the response, making it suitable for linearly structured data. On the other hand, regression trees offer flexibility in capturing non-linear patterns by segmenting the feature space. The choice between these methods depends on the nature of the data and the underlying relationships one aims to capture. While linear regression is straightforward and interpretable, regression trees excel in capturing complex, non-linear dependencies within the data.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>The indicator function, denoted as <span class="math notranslate nohighlight">\( \mathbb{I}_A \)</span> or <span class="math notranslate nohighlight">\( 1_A \)</span>, is defined for a set <span class="math notranslate nohighlight">\( A \)</span> and takes values in the real numbers. Its primary function is to serve as a binary indicator, representing the membership or non-membership of an element in the set <span class="math notranslate nohighlight">\( A \)</span>.</p>
<p>Mathematically, the indicator function is expressed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dd7df51e-5591-4e89-9eab-c55e28f48e01">
<span class="eqno">(10.20)<a class="headerlink" href="#equation-dd7df51e-5591-4e89-9eab-c55e28f48e01" title="Permalink to this equation">#</a></span>\[\begin{equation} \mathbb{I}_A(x) = \begin{cases} 1 &amp; \text{if } x \in A \\ 0 &amp; \text{if } x \notin A \end{cases} \end{equation}\]</div>
<p>In this formulation, the function evaluates to 1 when the input <span class="math notranslate nohighlight">\( x \)</span> is an element of the set <span class="math notranslate nohighlight">\( A \)</span>, and it evaluates to 0 otherwise. This binary nature makes the indicator function a valuable tool in defining and manipulating sets within various mathematical contexts.</p>
</div>
<!-- # Regression Trees and Linear Models

When we compare regression and classification trees to traditional methods, the differences become quite pronounced, especially when we contrast them with linear models, particularly linear regression {cite:p}`james2023introduction`.

## Linear Regression
Linear regression is built on the following model formulation {cite:p}`james2023introduction`:

```{math}
:label: eq8.8
f(X) = \beta_0 + \sum_{j=1}^{p}X_j \beta_j
```

In this equation, the predicted outcome, represented by $f(X)$, is the result of a linear combination of input features ($X_1, X_2, \ldots, X_p$), each multiplied by a corresponding coefficient ($\beta_j$). This approach is well-suited for capturing relationships between variables in a linear fashion, making it particularly useful when the data adheres to a linear structure.

## Regression Trees
In contrast, regression trees follow a completely different model structure {cite:p}`james2023introduction`:

```{math}
:label: eq8.9
f(X) = \sum_{m=1}^{M}c_m \cdot \mathbb{I}_{X\in R_m}
```

In this equation, the predictive function $f(X)$ is constructed as a sum of constants ($c_m$), with each constant associated with a specific region ($R_m$) within the feature space.


`````{admonition} Note
:class: warning

The indicator function, denoted as $ \mathbb{I}_A $ or $ 1_A $, is defined for a set $ A $ and takes values in the real numbers. Its primary function is to serve as a binary indicator, representing the membership or non-membership of an element in the set $ A $.

Mathematically, the indicator function is expressed as follows:

\begin{equation} \mathbb{I}_A(x) = \begin{cases} 1 & \text{if } x \in A \\ 0 & \text{if } x \notin A \end{cases} \end{equation}

In this formulation, the function evaluates to 1 when the input $ x $ is an element of the set $ A $, and it evaluates to 0 otherwise. This binary nature makes the indicator function a valuable tool in defining and manipulating sets within various mathematical contexts.
`````

## Selecting the Right Model
The choice between these distinct modeling strategies depends on the nature of the problem at hand. Key factors to consider include:

1. **Linearity of Relationships:** When the connection between input features and the response can be reasonably approximated using a linear model, as shown in Equation {eq}`eq8.8`, linear regression is a strong candidate. Linear regression excels at capturing linear relationships and is expected to perform better than tree-based methods in such cases {cite:p}`james2023introduction`.

2. **Complexity and Nonlinearity:** On the other hand, when the relationship between features and the response is notably nonlinear, intricate, or lacks a clear linear structure, as exemplified by Equation {eq}`eq8.9`, regression trees may be more suitable. Regression trees are well-equipped to handle complex, nonlinear patterns by adaptively partitioning the feature space {cite:p}`james2023introduction`.

The ultimate decision about the most suitable model should be grounded in the inherent characteristics of the data under examination. It's important to consider the complexity and linearity of relationships present in the dataset. To objectively evaluate the relative effectiveness of tree-based and traditional models, techniques like cross-validation or the validation set approach can be employed to estimate test error. These methods provide valuable insights into how well the models can generalize to new, unseen data, aiding in making informed decisions based on the unique attributes of the dataset. --></section>
<section id="example-material-strength-prediction">
<h2><span class="section-number">10.4.4. </span>Example: Material Strength Prediction<a class="headerlink" href="#example-material-strength-prediction" title="Permalink to this heading">#</a></h2>
<p><font color='Blue'><b> Example:</b></font> Suppose we have a fictional dataset that relates the temperature (<span class="math notranslate nohighlight">\(X\)</span>) to the strength of a material (<span class="math notranslate nohighlight">\(y\)</span>). We want to predict the material strength under different temperatures using linear regression and decision tree regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Set the style for a more visually appealing plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">strength</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">temperature</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>

<span class="c1"># Create a scatter plot with enhanced style</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>

<span class="c1"># Scatter plot with color-coded points based on material strength</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">strength</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">strength</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
                     <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Set axis labels and plot title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Temperature (°C)&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Material Strength&#39;</span><span class="p">)</span>

<span class="c1"># Set a bold title with a larger font size</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Material Strength vs. Temperature&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Ensure a tight layout for better presentation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e39a85dcc2407a0f5ef52dbaa162ca78914457c8d79a2591cbf9a0b7a48e5759.png" src="../_images/e39a85dcc2407a0f5ef52dbaa162ca78914457c8d79a2591cbf9a0b7a48e5759.png" />
</div>
</div>
<p>In this example, we simulate a scenario where material strength has a linear relationship with temperature, but there is some variability due to other factors, represented by the random noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Set the style for a more visually appealing plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">strength</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">temperature</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>

<span class="c1"># Create a scatter plot with enhanced style</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>

<span class="c1"># Scatter plot with color-coded points based on material strength</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">strength</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">strength</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
               <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual Data&#39;</span><span class="p">)</span>

<span class="c1"># Fit and plot the linear regression line</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">strength</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">temperature</span><span class="p">),</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;OrangeRed&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear Regression Line&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Train a decision tree regression model</span>
<span class="n">tree_reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">tree_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">strength</span><span class="p">)</span>

<span class="c1"># Plot the decision tree regression prediction</span>
<span class="n">temperature_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">50.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">strength_pred</span> <span class="o">=</span> <span class="n">tree_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">temperature_test</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temperature_test</span><span class="p">,</span> <span class="n">strength_pred</span><span class="p">,</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;Purple&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Tree Prediction&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Set axis labels and plot title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Temperature (°C)&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Material Strength&#39;</span><span class="p">)</span>

<span class="c1"># Add legend</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Set a bold title with a larger font size</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Material Strength Prediction Comparison&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Ensure a tight layout for better presentation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0229650e6c21bc085f01681d3ef7e44ac3611d0fc48b3b4c7c0c2838e253badb.png" src="../_images/0229650e6c21bc085f01681d3ef7e44ac3611d0fc48b3b4c7c0c2838e253badb.png" />
</div>
</div>
<p><strong>Comparison:</strong></p>
<ul class="simple">
<li><p>Linear regression assumes a linear relationship and provides a straightforward interpretation. In our example, the linear regression line captures the overall trend of increasing material strength with temperature.</p></li>
<li><p>On the other hand, the decision tree regression accommodates non-linear patterns. In this example, it may capture abrupt changes in material strength at specific temperature thresholds.</p></li>
</ul>
<p>The choice between linear regression and decision tree regression depends on the underlying characteristics of the data and the complexity of the relationship between variables. Linear regression is suitable for capturing linear trends, while decision tree regression can handle non-linear relationships and capture more intricate patterns in the data.</p>
</section>
<section id="example-binary-classification">
<h2><span class="section-number">10.4.5. </span>Example - Binary Classification<a class="headerlink" href="#example-binary-classification" title="Permalink to this heading">#</a></h2>
<p><font color='Blue'><b>Example - Binary Classification:</b></font> Consider a scenario wherein a dataset comprises data points inherently categorized into two distinct classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># Display the equation using LaTeX</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Latex</span><span class="p">,</span> <span class="n">display</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">31</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Use a custom style for plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Define colors and colormap for the plot</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Red&quot;</span><span class="p">,</span> <span class="s2">&quot;Blue&quot;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;MistyRose&quot;</span><span class="p">,</span> <span class="s2">&quot;LightBlue&quot;</span><span class="p">])</span>

<span class="c1"># Generate synthetic data using make_blobs</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">centers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">cluster_std</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="n">cluster_std</span><span class="p">)</span>

<span class="c1"># Create subplots for decision boundary visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Define parameters for classifiers</span>
<span class="n">dts_parms</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;max_leaf_nodes&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
<span class="n">lr_parms</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;max_iter&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">}</span>

<span class="c1"># Create classifier instances</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">dts_parms</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">lr_parms</span><span class="p">)]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Decision Tree Classifier&#39;</span><span class="p">,</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">]</span>
<span class="n">model_alphs</span> <span class="o">=</span> <span class="s1">&#39;ab&#39;</span>

<span class="c1"># Iterate through classifiers and plot decision boundaries</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">code</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">model_alphs</span><span class="p">):</span>
    <span class="c1"># Fit the classifier to the data</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Plot decision boundary using DecisionBoundaryDisplay</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">grid_resolution</span><span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    
    <span class="c1"># Scatter plot for data points</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">zorder</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Set legend and title for the subplot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">code</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="c1"># For DecisionTreeClassifier</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">):</span>
        <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Decision Tree Classifier:&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span>
                               <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]))</span>
        
    <span class="c1"># For Logistic Regression, get the coefficients and intercept</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">):</span>
        <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Logistic Regression:&#39;</span><span class="p">)</span>
        <span class="n">coef</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="c1"># Plot the decision boundary as a dashed line</span>
        <span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">y_vals</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">x_vals</span> <span class="o">-</span> <span class="p">(</span><span class="n">intercept</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="s1">&#39;--k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Decision Boundary (Logistic Regression)&#39;</span><span class="p">,</span> <span class="n">zorder</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">equation</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;$Feature_2 = </span><span class="si">{</span><span class="p">(</span><span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1"> * Feature_1 </span><span class="si">{</span><span class="p">(</span><span class="n">intercept</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s1">+.3f</span><span class="si">}</span><span class="s1">$&#39;</span>
        <span class="n">display</span><span class="p">(</span><span class="n">Latex</span><span class="p">(</span><span class="n">equation</span><span class="p">))</span>

<span class="c1"># Adjust layout for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Decision Tree Classifier:</span>
|--- Feature_2 &lt;= 2.85
|   |--- Feature_1 &lt;= 0.36
|   |   |--- class: 0
|   |--- Feature_1 &gt;  0.36
|   |   |--- class: 1
|--- Feature_2 &gt;  2.85
|   |--- class: 0

<span class=" -Color -Color-Bold -Color-Bold-Red">Logistic Regression:</span>
</pre></div>
</div>
<div class="output text_latex math notranslate nohighlight">
\[Feature_2 = +0.178 * Feature_1 -2.440\]</div>
<img alt="../_images/030127a594f32704dd3dc92cd74a79b62d4168235f81e45148f5b57e2c29b474.png" src="../_images/030127a594f32704dd3dc92cd74a79b62d4168235f81e45148f5b57e2c29b474.png" />
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>The decision boundary line in logistic regression is derived from the logistic function. In logistic regression with two variables (features), the decision boundary can be expressed as a linear combination of the features. The logistic function is used to transform this linear combination into a probability.</p>
<p>The logistic regression hypothesis function is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cc0266b1-a896-43ac-b43f-0d7e8f771bea">
<span class="eqno">(10.21)<a class="headerlink" href="#equation-cc0266b1-a896-43ac-b43f-0d7e8f771bea" title="Permalink to this equation">#</a></span>\[\begin{equation} P(X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}} \end{equation}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X)\)</span> is the predicted probability that the example <span class="math notranslate nohighlight">\(x\)</span> belongs to the positive class.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0, \beta_1, \beta_2\)</span> are the parameters (intercept and coefficients).</p></li>
<li><p><span class="math notranslate nohighlight">\(X = (x_1, x_2)\)</span> are the input features.</p></li>
</ul>
<p>To find the decision boundary, we set <span class="math notranslate nohighlight">\(h_{\beta}(x)\)</span> to 0.5 (as this is the threshold for classification in binary logistic regression). The logistic function maps any real-valued number to the range [0, 1]. Therefore:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6506da20-930c-438c-a85a-2166262daa05">
<span class="eqno">(10.22)<a class="headerlink" href="#equation-6506da20-930c-438c-a85a-2166262daa05" title="Permalink to this equation">#</a></span>\[\begin{equation} \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}} = 0.5 \end{equation}\]</div>
<p>Solving this equation for <span class="math notranslate nohighlight">\(x_2\)</span> gives the equation of the decision boundary:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f91314eb-7377-4cc7-b787-4e7eb40183df">
<span class="eqno">(10.23)<a class="headerlink" href="#equation-f91314eb-7377-4cc7-b787-4e7eb40183df" title="Permalink to this equation">#</a></span>\[\begin{equation} \beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0 \end{equation}\]</div>
<p>This is the equation of a straight line in a two-dimensional space. If you solve for <span class="math notranslate nohighlight">\(x_2\)</span>, you get:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2035c35d-5fe3-40a8-8941-044d2279dcc9">
<span class="eqno">(10.24)<a class="headerlink" href="#equation-2035c35d-5fe3-40a8-8941-044d2279dcc9" title="Permalink to this equation">#</a></span>\[\begin{equation} x_2 = -\frac{\beta_0 + \beta_1 x_1}{\beta_2} = -\frac{\beta_0}{\beta_2} + \frac{\beta_1}{\beta_2}x_1 \end{equation}\]</div>
<p>This is the equation of the decision boundary line. In the provided code, <span class="math notranslate nohighlight">\(x_1\)</span> is represented by <code class="docutils literal notranslate"><span class="pre">x_vals</span></code>, and <span class="math notranslate nohighlight">\(x_2\)</span> is represented by <code class="docutils literal notranslate"><span class="pre">y_vals</span></code>. The coefficients and intercept from the logistic regression model are used to calculate these values for plotting the decision boundary.</p>
</div>
<section id="comparison">
<h3><span class="section-number">10.4.5.1. </span>Comparison<a class="headerlink" href="#comparison" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Function to print a line of underscores for separation</span>
<span class="k">def</span> <span class="nf">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">60</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">31</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Sample data X and y are assumed to be defined here</span>

<span class="c1"># Models to be evaluated</span>
<span class="n">model_alphs</span> <span class="o">=</span> <span class="s1">&#39;ab&#39;</span>

<span class="c1"># Loop through each model</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">model_alphs</span><span class="p">):</span>
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">34</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>

    <span class="c1"># Initialize KFold cross-validator</span>
    <span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="c1"># The splitt would be 80-20!</span>

    <span class="c1"># Lists to store train and test scores for each fold</span>
    <span class="n">train_acc_scores</span><span class="p">,</span> <span class="n">test_acc_scores</span><span class="p">,</span> <span class="n">train_f1_scores</span><span class="p">,</span> <span class="n">test_f1_scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">train_class_proportions</span><span class="p">,</span> <span class="n">test_class_proportions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="c1"># DataFrames to store classification reports</span>
    <span class="n">reports_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">reports_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Perform Cross-Validation</span>
    <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Calculate class proportions for train and test sets</span>
        <span class="n">train_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>
        <span class="n">test_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

        <span class="c1"># train</span>
        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">train_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">))</span>
        <span class="n">train_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

        <span class="c1"># test</span>
        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>
        <span class="n">test_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="c1">#  Print the Train and Test Scores for each fold</span>
    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
        <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">:&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Class Proportions: </span><span class="si">{</span><span class="n">train_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Test Class Proportions: </span><span class="si">{</span><span class="n">test_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Accuracy Score = </span><span class="si">{</span><span class="n">train_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Accuracy Score = </span><span class="si">{</span><span class="n">test_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train F1 Score (weighted) = </span><span class="si">{</span><span class="n">train_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test F1 Score (weighted)= </span><span class="si">{</span><span class="n">test_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Accuracy Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Train Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Test Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;F1 Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Blue">(a) Decision Tree Classifier</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 1:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 1.0000, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 1.0000, Test F1 Score (weighted)= 0.9499
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 2:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9875, Test Accuracy Score = 1.0000
	Train F1 Score (weighted) = 0.9875, Test F1 Score (weighted)= 1.0000
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 3:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9875, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 0.9875, Test F1 Score (weighted)= 0.9499
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 4:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9875, Test Accuracy Score = 1.0000
	Train F1 Score (weighted) = 0.9875, Test F1 Score (weighted)= 1.0000
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 5:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9875, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 0.9875, Test F1 Score (weighted)= 0.9499
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9900 ± 0.0050
	Mean Test Accuracy Score: 0.9700 ± 0.0245
<span class=" -Color -Color-Bold -Color-Bold-Red">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9900 ± 0.0050
	Mean F1 Accuracy Score (weighted): 0.9699 ± 0.0246
____________________________________________________________
================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Blue">(b) Logistic Regression</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 1:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9375, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 0.9375, Test F1 Score (weighted)= 0.9499
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 2:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9250, Test Accuracy Score = 1.0000
	Train F1 Score (weighted) = 0.9250, Test F1 Score (weighted)= 1.0000
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 3:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9250, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 0.9250, Test F1 Score (weighted)= 0.9499
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 4:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9125, Test Accuracy Score = 0.9500
	Train F1 Score (weighted) = 0.9125, Test F1 Score (weighted)= 0.9499
<span class=" -Color -Color-Bold -Color-Bold-Red">Fold 5:</span>
	Train Class Proportions: [0.5, 0.5]*80
	Test Class Proportions: [0.5, 0.5]*20
	Train Accuracy Score = 0.9625, Test Accuracy Score = 0.8500
	Train F1 Score (weighted) = 0.9625, Test F1 Score (weighted)= 0.8496
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Red">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9325 ± 0.0170
	Mean Test Accuracy Score: 0.9400 ± 0.0490
<span class=" -Color -Color-Bold -Color-Bold-Red">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9325 ± 0.0170
	Mean F1 Accuracy Score (weighted): 0.9398 ± 0.0491
____________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>The outcomes present an evaluation of two models: a decision tree classifier and a logistic regression. Both models employ accuracy and F1 score as performance indicators. Accuracy reflects the proportion of correctly classified instances, while the F1 score serves as the harmonic mean of precision and recall, both metrics ranging from 0 to 1, with higher values denoting superior performance.</p>
<p>These results stem from a five-fold stratified cross-validation, ensuring random data division into five subsets, each maintaining class proportions akin to the original dataset. For each fold, one subset acts as the test set, with the remaining four constituting the training set. Model performance is then averaged across these five folds.</p>
<p>Key observations for result interpretation include:</p>
<ul class="simple">
<li><p>The decision tree classifier outperforms the logistic regression, exhibiting higher mean accuracy and F1 score on both training and test sets. This suggests the decision tree classifier’s suitability for the given data.</p></li>
<li><p>The decision tree classifier demonstrates minimal standard deviations for both metrics, indicating consistent performance across diverse folds. Conversely, the logistic regression manifests higher standard deviations, particularly for the test set, signifying more variable performance across folds.</p></li>
<li><p>The decision tree classifier achieves a notably high mean accuracy and F1 score on the training set, approaching 1. This signifies adept fitting to the training data but raises concerns of potential overfitting. In contrast, the logistic regression, while less prone to overfitting, displays lower mean accuracy and F1 score on the training set, implying potential underfitting and a lesser grasp of data complexity.</p></li>
<li><p>As anticipated, the decision tree classifier exhibits a slightly lower mean accuracy and F1 score on the test set compared to the training set, indicative of overfitting. Conversely, the logistic regression displays a somewhat higher mean accuracy and F1 score on the test set, an uncommon occurrence that may be attributed to random variation or sampling error.</p></li>
</ul>
<p>The findings suggest the decision tree classifier surpasses the logistic regression on this data, albeit with potential overfitting. The logistic regression, while less prone to overfitting, might face underfitting challenges. To enhance both models, exploring different hyperparameters, feature selection, or regularization techniques is advised. Additionally, considering alternative metrics, such as ROC curve or confusion matrix, could provide a more comprehensive model comparison.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">metrics.accuracy_score</span></code> is a function commonly used in the context of classification tasks within the field of machine learning. It is part of the scikit-learn library in Python. This function is utilized to quantify the accuracy of a classification model by comparing the predicted labels against the true labels of a dataset.</p>
<p>The accuracy score is calculated by dividing the number of correctly classified instances by the total number of instances. Mathematically, it can be expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f74a46d8-d223-489b-9b19-bec2173e8a89">
<span class="eqno">(10.25)<a class="headerlink" href="#equation-f74a46d8-d223-489b-9b19-bec2173e8a89" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Accuracy} = \frac{\text{Number of Correctly Classified Instances}}{\text{Total Number of Instances}} \end{equation}\]</div>
<p>In Python, using the <code class="docutils literal notranslate"><span class="pre">metrics.accuracy_score</span></code> function involves providing the true labels and the predicted labels as arguments. The function then returns a numerical value representing the accuracy of the classification model.</p>
<p>It is important to note that while accuracy is a straightforward metric, it may not be sufficient in scenarios with imbalanced class distributions. In such cases, additional metrics like precision, recall, and F1-score might be more informative for evaluating the performance of a classification model.</p>
</li>
<li><p>The F1 Score, specifically in its weighted form, is a metric commonly employed in the evaluation of classification models. It provides a balance between precision and recall, offering a single numerical value that summarizes the model’s performance across multiple classes in a weighted manner.</p>
<p>The weighted F1 Score is calculated by considering both precision (<span class="math notranslate nohighlight">\(P\)</span>) and recall (<span class="math notranslate nohighlight">\(R\)</span>) for each class and then computing the harmonic mean of these values. The weighted aspect accounts for the imbalance in class sizes. Mathematically, it is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ac918af0-2bee-492c-aec0-84b71ec26912">
<span class="eqno">(10.26)<a class="headerlink" href="#equation-ac918af0-2bee-492c-aec0-84b71ec26912" title="Permalink to this equation">#</a></span>\[\begin{equation} F1_{\text{weighted}} = \frac{\sum_{i=1}^{C} w_i \cdot F1_i}{\sum_{i=1}^{C} w_i} \end{equation}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( C \)</span> is the number of classes.</p></li>
<li><p><span class="math notranslate nohighlight">\( F1_i \)</span> is the F1 Score for class <span class="math notranslate nohighlight">\( i \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( w_i \)</span> is the weight assigned to class <span class="math notranslate nohighlight">\( i \)</span>, typically proportional to the number of instances in that class.</p></li>
</ul>
<p>In Python, scikit-learn’s <code class="docutils literal notranslate"><span class="pre">metrics.f1_score</span></code> function can be utilized to compute the F1 Score. When employing the ‘weighted’ parameter, it calculates the average F1 Score, considering the number of instances in each class as weights.</p>
<p>This weighted F1 Score is particularly useful when dealing with imbalanced datasets, where certain classes may have significantly fewer instances than others. It provides a more nuanced evaluation of the model’s ability to perform well across all classes, accounting for the influence of class size on the overall metric.</p>
<p><font color='Blue'><b>Example:</b></font> Let’s consider a scenario where we have a classification model dealing with a dataset that includes three classes: A, B, and C. The dataset is imbalanced, meaning that the number of instances in each class is different. We want to compute the weighted F1 Score for this model.</p>
<p>Here’s a hypothetical example with class counts and F1 Scores for each class:</p>
<ul class="simple">
<li><p>Class A: True Positives (TP) = 150, False Positives (FP) = 20, False Negatives (FN) = 10</p></li>
<li><p>Class B: TP = 80, FP = 5, FN = 30</p></li>
<li><p>Class C: TP = 40, FP = 10, FN = 5</p></li>
</ul>
<p>Class weights (<span class="math notranslate nohighlight">\(w_i\)</span>) can be determined based on the number of instances in each class. For simplicity, let’s assume the weights are proportional to the number of instances in each class:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{\text{A}} = 300\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(w_{\text{B}} = 115\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(w_{\text{C}} = 55\)</span></p></li>
</ul>
<p>Now, we can compute the F1 Score for each class using the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a55a1415-a53b-4e8c-8e09-a4f67eb34350">
<span class="eqno">(10.27)<a class="headerlink" href="#equation-a55a1415-a53b-4e8c-8e09-a4f67eb34350" title="Permalink to this equation">#</a></span>\[\begin{equation} F1_i = \frac{2 \cdot \text{TP}_i}{2 \cdot \text{TP}_i + \text{FP}_i + \text{FN}_i} \end{equation}\]</div>
<p>After calculating <span class="math notranslate nohighlight">\(F1_i\)</span> for each class, we can then compute the weighted F1 Score using the formula mentioned earlier:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c8784f35-02d1-4bf2-a7fa-9a3415d974da">
<span class="eqno">(10.28)<a class="headerlink" href="#equation-c8784f35-02d1-4bf2-a7fa-9a3415d974da" title="Permalink to this equation">#</a></span>\[\begin{equation} F1_{\text{weighted}} = \frac{w_{\text{A}} \cdot F1_{\text{A}} + w_{\text{B}} \cdot F1_{\text{B}} + w_{\text{C}} \cdot F1_{\text{C}}}{w_{\text{A}} + w_{\text{B}} + w_{\text{C}}} \end{equation}\]</div>
<p>This weighted F1 Score provides a comprehensive evaluation of the model’s performance, giving more importance to classes with a larger number of instances.</p>
</li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span>
                   <span class="n">filled</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">node_ids</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">,</span>
                   <span class="n">proportion</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7eea716daf712d63613f0ec06646a080905bacff4d0624cf73a31c054ccc8fa5.png" src="../_images/7eea716daf712d63613f0ec06646a080905bacff4d0624cf73a31c054ccc8fa5.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- Feature_2 &lt;= 2.86
|   |--- Feature_1 &lt;= 0.36
|   |   |--- class: 0
|   |--- Feature_1 &gt;  0.36
|   |   |--- class: 1
|--- Feature_2 &gt;  2.86
|   |--- class: 0
</pre></div>
</div>
</div>
</div>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/dts_fig03.png"><img alt="../_images/dts_fig03.png" src="../_images/dts_fig03.png" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.8 </span><span class="caption-text">Visual representation of the above Decision Tree Classifier.</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- 
import dtreeviz

viz_model = dtreeviz.model(dts, X, y, feature_names = feature_names, target_name= 'Outcome')
v = viz_model.view(colors={'classes':[None, None, colors]})
v.show()
--></section>
</section>
<section id="example-multiclass-classification">
<h2><span class="section-number">10.4.6. </span>Example - Multiclass Classification<a class="headerlink" href="#example-multiclass-classification" title="Permalink to this heading">#</a></h2>
<p><font color='Blue'><b>Example - Multiclass Classification:</b></font>  Unlike our earlier binary classification focus, where we concentrated on just two classes, our current journey involves untangling the complexities of categorizing across a multitude of distinct classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="c1"># Define colors and colormap for the plot</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Red&quot;</span><span class="p">,</span> <span class="s2">&quot;Blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Green&quot;</span><span class="p">,</span> <span class="s1">&#39;DimGray&#39;</span><span class="p">]</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;MistyRose&quot;</span><span class="p">,</span> <span class="s2">&quot;LightBlue&quot;</span><span class="p">,</span> <span class="s2">&quot;HoneyDew&quot;</span><span class="p">,</span> <span class="s2">&quot;LightGray&quot;</span><span class="p">])</span>

<span class="c1"># Generate synthetic data using make_blobs</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">centers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">cluster_std</span> <span class="o">=</span> <span class="mf">1.25</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="n">cluster_std</span><span class="p">)</span>

<span class="c1"># Create subplots for decision boundary visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Define classifier models, names, and codes</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;max_leaf_nodes&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}),</span>
          <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Decision Tree Classifier&#39;</span><span class="p">,</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">]</span>
<span class="n">model_codes</span> <span class="o">=</span> <span class="s1">&#39;abcd&#39;</span>

<span class="c1"># Loop through each classifier</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">code</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">model_codes</span><span class="p">):</span>
    <span class="c1"># Fit the classifier to the data</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Plot decision boundary using DecisionBoundaryDisplay</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                           <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                           <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                           <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    
    <span class="c1"># Scatter plot for data points</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y</span> <span class="o">==</span> <span class="n">num</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">num</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    
    <span class="c1"># Set title and remove grid lines</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">code</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Adjust layout for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d12021016f4cea9dd8b821a64bd5af965b0995ac714c7f7df629e01fbcea3f7f.png" src="../_images/d12021016f4cea9dd8b821a64bd5af965b0995ac714c7f7df629e01fbcea3f7f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Function to print a line of underscores for separation</span>
<span class="k">def</span> <span class="nf">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">60</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">32</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
<span class="c1"># Sample data X and y are assumed to be defined here</span>

<span class="c1"># Models to be evaluated</span>
<span class="n">model_alphs</span> <span class="o">=</span> <span class="s1">&#39;ab&#39;</span>

<span class="c1"># Loop through each model</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">model_names</span><span class="p">,</span> <span class="n">model_alphs</span><span class="p">):</span>
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">35</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;=&#39;</span><span class="p">)</span>

    <span class="c1"># Initialize KFold cross-validator</span>
    <span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="c1"># The splitt would be 80-20!</span>

    <span class="c1"># Lists to store train and test scores for each fold</span>
    <span class="n">train_acc_scores</span><span class="p">,</span> <span class="n">test_acc_scores</span><span class="p">,</span> <span class="n">train_f1_scores</span><span class="p">,</span> <span class="n">test_f1_scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">train_class_proportions</span><span class="p">,</span> <span class="n">test_class_proportions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="c1"># DataFrames to store classification reports</span>
    <span class="n">reports_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">reports_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="c1"># Perform Cross-Validation</span>
    <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Calculate class proportions for train and test sets</span>
        <span class="n">train_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>
        <span class="n">test_class_proportions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

        <span class="c1"># train</span>
        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">train_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">))</span>
        <span class="n">train_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

        <span class="c1"># test</span>
        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_acc_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>
        <span class="n">test_f1_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s1">&#39;weighted&#39;</span><span class="p">))</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="c1">#  Print the Train and Test Scores for each fold</span>
    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
        <span class="n">print_bold</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">:&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Class Proportions: </span><span class="si">{</span><span class="n">train_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Test Class Proportions: </span><span class="si">{</span><span class="n">test_class_proportions</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">}</span><span class="s2">*</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train Accuracy Score = </span><span class="si">{</span><span class="n">train_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Accuracy Score = </span><span class="si">{</span><span class="n">test_acc_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Train F1 Score (weighted) = </span><span class="si">{</span><span class="n">train_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test F1 Score (weighted)= </span><span class="si">{</span><span class="n">test_f1_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">_Line</span><span class="p">()</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Accuracy Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Train Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean Test Accuracy Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_acc_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;F1 Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">Mean F1 Accuracy Score (weighted): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_f1_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">_Line</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Magenta">(a) Decision Tree Classifier</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 1:</span>
	Train Class Proportions: [0.255, 0.25, 0.245, 0.25]*200
	Test Class Proportions: [0.24, 0.26, 0.26, 0.24]*50
	Train Accuracy Score = 0.9550, Test Accuracy Score = 0.8400
	Train F1 Score (weighted) = 0.9553, Test F1 Score (weighted)= 0.8382
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 2:</span>
	Train Class Proportions: [0.255, 0.25, 0.245, 0.25]*200
	Test Class Proportions: [0.24, 0.26, 0.26, 0.24]*50
	Train Accuracy Score = 0.9300, Test Accuracy Score = 0.8800
	Train F1 Score (weighted) = 0.9310, Test F1 Score (weighted)= 0.8859
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 3:</span>
	Train Class Proportions: [0.25, 0.255, 0.25, 0.245]*200
	Test Class Proportions: [0.26, 0.24, 0.24, 0.26]*50
	Train Accuracy Score = 0.9600, Test Accuracy Score = 0.8000
	Train F1 Score (weighted) = 0.9600, Test F1 Score (weighted)= 0.8022
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 4:</span>
	Train Class Proportions: [0.25, 0.255, 0.25, 0.245]*200
	Test Class Proportions: [0.26, 0.24, 0.24, 0.26]*50
	Train Accuracy Score = 0.9400, Test Accuracy Score = 0.9000
	Train F1 Score (weighted) = 0.9394, Test F1 Score (weighted)= 0.8968
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 5:</span>
	Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*200
	Test Class Proportions: [0.26, 0.26, 0.24, 0.24]*50
	Train Accuracy Score = 0.9250, Test Accuracy Score = 0.9200
	Train F1 Score (weighted) = 0.9252, Test F1 Score (weighted)= 0.9199
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Green">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9420 ± 0.0136
	Mean Test Accuracy Score: 0.8680 ± 0.0431
<span class=" -Color -Color-Bold -Color-Bold-Green">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9422 ± 0.0135
	Mean F1 Accuracy Score (weighted): 0.8686 ± 0.0426
____________________________________________________________
================================================================================
<span class=" -Color -Color-Bold -Color-Bold-Magenta">(b) Logistic Regression</span>
================================================================================
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 1:</span>
	Train Class Proportions: [0.255, 0.25, 0.245, 0.25]*200
	Test Class Proportions: [0.24, 0.26, 0.26, 0.24]*50
	Train Accuracy Score = 0.9150, Test Accuracy Score = 0.8600
	Train F1 Score (weighted) = 0.9149, Test F1 Score (weighted)= 0.8614
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 2:</span>
	Train Class Proportions: [0.255, 0.25, 0.245, 0.25]*200
	Test Class Proportions: [0.24, 0.26, 0.26, 0.24]*50
	Train Accuracy Score = 0.9050, Test Accuracy Score = 0.9000
	Train F1 Score (weighted) = 0.9041, Test F1 Score (weighted)= 0.9010
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 3:</span>
	Train Class Proportions: [0.25, 0.255, 0.25, 0.245]*200
	Test Class Proportions: [0.26, 0.24, 0.24, 0.26]*50
	Train Accuracy Score = 0.9200, Test Accuracy Score = 0.8200
	Train F1 Score (weighted) = 0.9201, Test F1 Score (weighted)= 0.8143
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 4:</span>
	Train Class Proportions: [0.25, 0.255, 0.25, 0.245]*200
	Test Class Proportions: [0.26, 0.24, 0.24, 0.26]*50
	Train Accuracy Score = 0.8950, Test Accuracy Score = 0.9200
	Train F1 Score (weighted) = 0.8943, Test F1 Score (weighted)= 0.9207
<span class=" -Color -Color-Bold -Color-Bold-Green">Fold 5:</span>
	Train Class Proportions: [0.25, 0.25, 0.25, 0.25]*200
	Test Class Proportions: [0.26, 0.26, 0.24, 0.24]*50
	Train Accuracy Score = 0.8850, Test Accuracy Score = 0.9400
	Train F1 Score (weighted) = 0.8844, Test F1 Score (weighted)= 0.9397
____________________________________________________________
<span class=" -Color -Color-Bold -Color-Bold-Green">Accuracy Score:</span>
	Mean Train Accuracy Score: 0.9040 ± 0.0128
	Mean Test Accuracy Score: 0.8880 ± 0.0431
<span class=" -Color -Color-Bold -Color-Bold-Green">F1 Score:</span>
	Mean F1 Accuracy Score (weighted): 0.9036 ± 0.0131
	Mean F1 Accuracy Score (weighted): 0.8874 ± 0.0449
____________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Models:&#39;</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
<span class="n">dts</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">16.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span>
                   <span class="n">filled</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">node_ids</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)],</span>
                   <span class="n">proportion</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Models: [DecisionTreeClassifier(max_features=3, max_leaf_nodes=10), LogisticRegression()]
</pre></div>
</div>
<img alt="../_images/84eea5935baa63939e5fca71e6000137b6ac048248787ffc0a139fff196046ba.png" src="../_images/84eea5935baa63939e5fca71e6000137b6ac048248787ffc0a139fff196046ba.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- Feature_2 &lt;= 5.78
|   |--- Feature_1 &lt;= -0.18
|   |   |--- Feature_2 &lt;= 4.86
|   |   |   |--- class: 2
|   |   |--- Feature_2 &gt;  4.86
|   |   |   |--- Feature_1 &lt;= -1.39
|   |   |   |   |--- class: 2
|   |   |   |--- Feature_1 &gt;  -1.39
|   |   |   |   |--- class: 0
|   |--- Feature_1 &gt;  -0.18
|   |   |--- Feature_2 &lt;= 2.34
|   |   |   |--- Feature_1 &lt;= 0.02
|   |   |   |   |--- class: 0
|   |   |   |--- Feature_1 &gt;  0.02
|   |   |   |   |--- class: 1
|   |   |--- Feature_2 &gt;  2.34
|   |   |   |--- Feature_2 &lt;= 3.34
|   |   |   |   |--- class: 0
|   |   |   |--- Feature_2 &gt;  3.34
|   |   |   |   |--- Feature_1 &lt;= -0.16
|   |   |   |   |   |--- class: 3
|   |   |   |   |--- Feature_1 &gt;  -0.16
|   |   |   |   |   |--- class: 0
|--- Feature_2 &gt;  5.78
|   |--- Feature_1 &lt;= 0.96
|   |   |--- class: 3
|   |--- Feature_1 &gt;  0.96
|   |   |--- class: 0
</pre></div>
</div>
</div>
</div>
</section>
<section id="choosing-between-regression-trees-and-linear-models">
<h2><span class="section-number">10.4.7. </span>Choosing Between Regression Trees and Linear Models<a class="headerlink" href="#choosing-between-regression-trees-and-linear-models" title="Permalink to this heading">#</a></h2>
<p>The decision to use either Regression Trees or Linear Models depends on various factors related to your data’s characteristics, the underlying relationships you intend to model, and your overall objectives. Both Regression Trees and Linear Models possess distinct strengths and limitations, making the optimal choice contingent on your specific use case <span id="id2">[<a class="reference internal" href="../References.html#id28" title="Emilio Carrizosa, Cristina Molero-Río, and Dolores Romero Morales. Mathematical optimization in classification and regression trees. TOP, 29(1):5-33, Apr 2021. doi:10.1007/s11750-021-00594-1.">Carrizosa <em>et al.</em>, 2021</a>, <a class="reference internal" href="../References.html#id64" title="B. Efron and T. Hastie. Computer Age Statistical Inference, Student Edition: Algorithms, Evidence, and Data Science. IMS monographs. Cambridge University Press, 2021. ISBN 9781108823418. URL: https://books.google.ca/books?id=q1ctEAAAQBAJ.">Efron and Hastie, 2021</a>, <a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<section id="id3">
<h3><span class="section-number">10.4.7.1. </span>Regression Trees:<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p><strong>Strengths:</strong></p>
<ul class="simple">
<li><p><strong>Handling Non-Linearity:</strong> Regression Trees excel at capturing intricate, non-linear relationships in your data. They can uncover patterns that Linear Models may struggle to identify.</p></li>
<li><p><strong>Automatic Feature Selection:</strong> Trees can handle a mix of numerical and categorical features, autonomously selecting relevant ones without extensive preprocessing efforts.</p></li>
<li><p><strong>Interpretability:</strong> Trees are visually intuitive and offer transparent interpretations, aiding in explaining the decision-making process to stakeholders.</p></li>
<li><p><strong>Outlier Robustness:</strong> Regression Trees tend to be less affected by outliers compared to certain linear models.</p></li>
</ul>
<p><strong>Weaknesses:</strong></p>
<ul class="simple">
<li><p><strong>Overfitting Risk:</strong> Due to their potential to adapt closely to training data, Regression Trees can overfit, yielding suboptimal generalization on unseen data.</p></li>
<li><p><strong>Sensitivity to Data Fluctuations:</strong> Minor changes in input data can lead to substantially different tree structures, making them sensitive to data variations.</p></li>
<li><p><strong>Extrapolation Limitation:</strong> Regression Trees may not reliably predict values outside the range of training data.</p></li>
</ul>
</section>
<section id="linear-models">
<h3><span class="section-number">10.4.7.2. </span>Linear Models:<a class="headerlink" href="#linear-models" title="Permalink to this heading">#</a></h3>
<p><strong>Strengths:</strong></p>
<ul class="simple">
<li><p><strong>Simplicity and Interpretability:</strong> Linear Models offer straightforward interpretations, particularly when relationships between variables are nearly linear.</p></li>
<li><p><strong>Stability and Generalization:</strong> In scenarios involving small datasets or concerns about overfitting, Linear Models often yield more stable and better generalizable results.</p></li>
<li><p><strong>Extrapolation Capability:</strong> Linear Models can predict values beyond the training data’s range, assuming the linear relationship persists.</p></li>
</ul>
<p><strong>Weaknesses:</strong></p>
<ul class="simple">
<li><p><strong>Limited Non-Linearity:</strong> Linear Models might struggle to accurately model complex non-linear relationships inherent in the data.</p></li>
<li><p><strong>Feature Engineering Demand:</strong> Capturing interactions and non-linearities in Linear Models may necessitate meticulous feature engineering.</p></li>
<li><p><strong>Assumption of Linearity:</strong> Linear Models assume the relationship between predictors and the target variable is linear, which may not hold universally.</p></li>
</ul>
</section>
<section id="choosing-between-them">
<h3><span class="section-number">10.4.7.3. </span>Choosing Between Them:<a class="headerlink" href="#choosing-between-them" title="Permalink to this heading">#</a></h3>
<p>Selecting the appropriate model hinges on your specific requirements:</p>
<ul class="simple">
<li><p><strong>Relationship Complexity:</strong> Opt for Regression Trees if non-linear relationships are suspected and you possess ample data.</p></li>
<li><p><strong>Interpretation Priority:</strong> When clarity in explaining the model’s decisions is paramount, Regression Trees offer transparent insights.</p></li>
<li><p><strong>Generalization Concerns:</strong> For scenarios involving concerns about overfitting or small datasets, Linear Models generally yield superior generalization.</p></li>
<li><p><strong>Data Preparation:</strong> If extensive feature engineering is tolerable, Linear Models can be fine-tuned to handle interactions and non-linearities.</p></li>
</ul>
<p>In practice, experimenting with both methods and evaluating their performance using techniques like cross-validation can be beneficial. Ensemble methods, such as Random Forests or Gradient Boosting Trees, merge the advantages of both Regression Trees and Linear Models, potentially delivering enhanced predictive outcomes. Ultimately, the model choice should be influenced by your unique data characteristics, problem domain, and overarching goals.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C10S3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10.3. </span>Classification Trees</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C10S5.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10.5. </span>Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">10.4.1. Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-trees">10.4.2. Regression Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-distinctions">10.4.3. Key Distinctions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-material-strength-prediction">10.4.4. Example: Material Strength Prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-binary-classification">10.4.5. Example - Binary Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">10.4.5.1. Comparison</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-multiclass-classification">10.4.6. Example - Multiclass Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-between-regression-trees-and-linear-models">10.4.7. Choosing Between Regression Trees and Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">10.4.7.1. Regression Trees:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models">10.4.7.2. Linear Models:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-between-them">10.4.7.3. Choosing Between Them:</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>