

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.3. Classification Trees &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG_680_C10S3';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.4. Regression Trees and Linear Models" href="ENGG_680_C10S4.html" />
    <link rel="prev" title="10.2. Regression Trees" href="ENGG_680_C10S2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">10. Tree-Based Methods</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S2.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S3.html">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">13.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification Trees</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decisiontreeclassifier-algorithm-in-scikit-learn">10.3.1. DecisionTreeClassifier algorithm in scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">10.3.2. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">10.3.3. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-decisiontreeclassifier-parameters">10.3.4. Optimizing DecisionTreeClassifier Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gridsearchcv">10.3.4.1. GridSearchCV</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="classification-trees">
<h1><span class="section-number">10.3. </span>Classification Trees<a class="headerlink" href="#classification-trees" title="Permalink to this headline">#</a></h1>
<p>A classification tree, similar to a regression tree, is a predictive model used to handle qualitative responses rather than quantitative ones. In a regression tree, the predicted response for an observation is the mean response of the training observations belonging to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most frequently occurring class among the training observations within its corresponding region. When interpreting the results of a classification tree, we’re not only interested in the predicted class for a specific terminal node region but also in the proportions of classes among the training observations falling into that region <span id="id1">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>The process of developing a classification tree closely mirrors the approach used for constructing a regression tree. We utilize a technique called recursive binary splitting to build the classification tree. However, when dealing with classification tasks, the conventional criterion used in regression trees, known as the Residual Sum of Squares (RSS), is unsuitable for guiding binary split decisions. Instead, a more appropriate option is the classification error rate. The fundamental goal is to assign an observation within a specific region to the class that appears most frequently among the training observations in that same region. The classification error rate quantifies the proportion of training observations within that region which do not align with the most prevalent class <span id="id2">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>Mathematically, the classification error rate (<span class="math notranslate nohighlight">\(E\)</span>) is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ed4eb1e6-0c5a-480e-8e15-48ccf2f708a4">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-ed4eb1e6-0c5a-480e-8e15-48ccf2f708a4" title="Permalink to this equation">#</a></span>\[\begin{equation}
E = 1 − \max_k (\hat{p}_{mk}),
\end{equation}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\)</span> stands for the classification error rate.</p></li>
<li><p><span class="math notranslate nohighlight">\(\max_k\)</span> signifies the maximum value taken over different classes (k).</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> represents the fraction of training observations in the mth region that belong to the kth class.</p></li>
</ul>
<p>However, it’s worth noting that the classification error rate is found to lack the desired level of sensitivity for effective tree growth. As a result, practical experience has shown that two alternative measures are more advantageous in the tree-building process.</p>
<p>The Gini index is formulated as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-74f72ae7-e21a-4bf9-94f5-6c7841a96238">
<span class="eqno">(10.5)<a class="headerlink" href="#equation-74f72ae7-e21a-4bf9-94f5-6c7841a96238" title="Permalink to this equation">#</a></span>\[\begin{equation}
G = \sum_{k = 1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk}).
\end{equation}\]</div>
<p>This index serves as a metric to gauge the overall variance among the K classes. Its calculation involves considering the proportions (<span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span>) of training observations belonging to the kth class in a particular region. A crucial point to note is that the Gini index assumes lower values when the <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> values are closer to either zero or one. This characteristic lends the Gini index its characterization as a measure of node purity. Specifically, when the Gini index is small, it indicates that a node primarily comprises observations from a single class <span id="id3">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p>An alternative approach to quantify node impurity is through the use of entropy, which is defined by the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0655b902-91d3-4a0a-836f-d7158d17e482">
<span class="eqno">(10.6)<a class="headerlink" href="#equation-0655b902-91d3-4a0a-836f-d7158d17e482" title="Permalink to this equation">#</a></span>\[\begin{equation}
D = − \sum_{k = 1}^{K} \hat{p}_{mk} \log\hat{p}_{mk},
\end{equation}\]</div>
<p>Here, the entropy measure captures the information content within a node by considering the proportions (<span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span>) of training observations belonging to the kth class in that specific node. Since the range of values for <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> lies between 0 and 1, the term <span class="math notranslate nohighlight">\(\hat{p}_{mk} \log\hat{p}_{mk}\)</span> is non-negative (<span class="math notranslate nohighlight">\(0 \leq \hat{p}_{mk} \log\hat{p}_{mk}\)</span>). The entropy value tends to approach zero when the <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> values are close to either zero or one. Hence, just like the Gini index, low entropy signifies that the node primarily comprises observations from a single class, implying node purity. In fact, it’s interesting to note that the Gini index and entropy are closely aligned numerically, sharing similarities in their behaviors and interpretations <span id="id4">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<div class="section" id="decisiontreeclassifier-algorithm-in-scikit-learn">
<h2><span class="section-number">10.3.1. </span>DecisionTreeClassifier algorithm in scikit-learn<a class="headerlink" href="#decisiontreeclassifier-algorithm-in-scikit-learn" title="Permalink to this headline">#</a></h2>
<p>The DecisionTreeClassifier in scikit-learn (sklearn) is a part of the machine learning library that implements decision tree-based classification algorithms. The classifier is based on the concept of recursive binary splitting of the feature space into regions that correspond to different class labels. The mathematical formulation of the DecisionTreeClassifier involves several components <span id="id5">[<a class="reference internal" href="../References.html#id54" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>, <a class="reference internal" href="../References.html#id53" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>, <a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<div class="admonition-decisiontreeclassifier-algorithm admonition">
<p class="admonition-title">DecisionTreeClassifier Algorithm</p>
<ol class="arabic">
<li><p><strong>Objective Function:</strong></p>
<ul class="simple">
<li><p>The primary goal of the DecisionTreeClassifier is to create a tree that effectively partitions the feature space, maximizing classification accuracy. It does this by recursively finding the optimal features and thresholds for splitting the data.</p></li>
</ul>
</li>
<li><p><strong>Splitting Criterion:</strong></p>
<ul>
<li><p>At each internal node of the tree, the algorithm chooses a feature (x_i) and a threshold (t) to split the data into two subsets: <span class="math notranslate nohighlight">\(D_{\text{left}}\)</span> and <span class="math notranslate nohighlight">\(D_{\text{right}}\)</span>. The split is determined by minimizing a chosen impurity measure, which can be one of the following:</p>
<ul>
<li><p><strong>Gini impurity</strong> (commonly used in scikit-learn):</p>
<div class="amsmath math notranslate nohighlight" id="equation-b64b3db9-20b6-48ed-ac2f-5afcbed03c62">
<span class="eqno">(10.7)<a class="headerlink" href="#equation-b64b3db9-20b6-48ed-ac2f-5afcbed03c62" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2 \end{equation}\]</div>
</li>
<li><p><strong>Entropy</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2127ec9e-cc06-4ee5-8f5f-3f860232f369">
<span class="eqno">(10.8)<a class="headerlink" href="#equation-2127ec9e-cc06-4ee5-8f5f-3f860232f369" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Entropy}(D) = -\sum_{k=1}^{K} p_k \log_2(p_k) \end{equation}\]</div>
</li>
<li><p><strong>Classification error</strong> (misclassification error):</p>
<div class="amsmath math notranslate nohighlight" id="equation-656d7b29-7a7e-46d4-92bb-160f393026f5">
<span class="eqno">(10.9)<a class="headerlink" href="#equation-656d7b29-7a7e-46d4-92bb-160f393026f5" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{Classification Error}(D) = 1 - \max_k(p_k) \end{equation}\]</div>
</li>
</ul>
</li>
</ul>
<p>where <span class="math notranslate nohighlight">\(p_k\)</span> is the proportion of samples of class <span class="math notranslate nohighlight">\(k\)</span> in the node.</p>
</li>
<li><p><strong>Recursive Splitting:</strong></p>
<ul class="simple">
<li><p>The data is recursively split into child nodes, applying the splitting criterion, until a stopping criterion is met. Common stopping criteria include reaching a maximum depth, having a minimum number of samples in a node, or having impurity below a certain threshold.</p></li>
</ul>
</li>
<li><p><strong>Leaf Nodes:</strong></p>
<ul class="simple">
<li><p>When a stopping criterion is reached, a leaf node is created. This leaf node represents a class label prediction. The predicted class is often determined by the majority class of samples in that node.</p></li>
</ul>
</li>
</ol>
<p class="rubric">Mathematical Representation:</p>
<ol class="arabic">
<li><p><strong>Splitting Decision:</strong></p>
<ul>
<li><p>For a binary split at node <span class="math notranslate nohighlight">\(m\)</span>, the decision rule is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-04293b35-e049-47cc-9d23-860c9c5d5641">
<span class="eqno">(10.10)<a class="headerlink" href="#equation-04293b35-e049-47cc-9d23-860c9c5d5641" title="Permalink to this equation">#</a></span>\[\begin{equation} x_i \leq t \end{equation}\]</div>
<p>or</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d99340e-375e-4028-8e19-e3061f660162">
<span class="eqno">(10.11)<a class="headerlink" href="#equation-8d99340e-375e-4028-8e19-e3061f660162" title="Permalink to this equation">#</a></span>\[\begin{equation} x_i &gt; t \end{equation}\]</div>
</li>
</ul>
</li>
<li><p><strong>Impurity Measure:</strong></p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(D_m\)</span> be the dataset at node <span class="math notranslate nohighlight">\(m\)</span>, and <span class="math notranslate nohighlight">\(D_{\text{left}}\)</span> and <span class="math notranslate nohighlight">\(D_{\text{right}}\)</span> be the subsets after the split. The impurity of the split is calculated using the chosen impurity measure:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8cbdae2a-c862-4c4f-a572-f1f71b280ea8">
<span class="eqno">(10.12)<a class="headerlink" href="#equation-8cbdae2a-c862-4c4f-a572-f1f71b280ea8" title="Permalink to this equation">#</a></span>\[\begin{align}
\text{Impurity}(D_m)&amp; = \frac{|D_{\text{left}}|}{|D_m|} \times \text{Impurity}(D_{\text{left}}) \notag \\
&amp; + \frac{|D_{\text{right}}|}{|D_m|} \times \text{Impurity}(D_{\text{right}})
\end{align}\]</div>
</li>
</ul>
</li>
<li><p><strong>Optimization:</strong></p>
<ul class="simple">
<li><p>The algorithm searches for the optimal <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(t\)</span> by minimizing the impurity measure at each internal node.</p></li>
</ul>
</li>
<li><p><strong>Leaf Node Prediction:</strong></p>
<ul class="simple">
<li><p>The predicted class at a leaf node <span class="math notranslate nohighlight">\(m\)</span> is the majority class in <span class="math notranslate nohighlight">\(D_m\)</span>.</p></li>
</ul>
</li>
</ol>
<p>This mathematical framework describes the decision-making process of the DecisionTreeClassifier in scikit-learn. The algorithm optimizes the choice of features and thresholds at each node to construct an effective decision tree for classification. It seeks to minimize impurity, promoting pure partitions where each leaf node predominantly contains samples of one class.</p>
</div>
</div>
<div class="section" id="example-synthetic-dataset">
<h2><span class="section-number">10.3.2. </span>Example: Synthetic Dataset<a class="headerlink" href="#example-synthetic-dataset" title="Permalink to this headline">#</a></h2>
<p><font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:</p>
<ul class="simple">
<li><p><strong>Number of Samples:</strong> 1000</p></li>
<li><p><strong>Number of Features:</strong> 2</p></li>
<li><p><strong>Number of Classes:</strong> 2</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as ‘Feature 1’ and ‘Feature 2’.</p></li>
</ul>
<p><strong>Outcome (Target Variable):</strong></p>
<ul class="simple">
<li><p>The dataset also includes a target variable called ‘Outcome.’ This variable assigns each data point to one of two distinct classes, identified as ‘Class 0’ and ‘Class 1’.</p></li>
</ul>
<p>The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Create a DataFrame</span>
<span class="n">Data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Feature </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">Data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#f5645a&quot;</span><span class="p">,</span> <span class="s2">&quot;#b781ea&quot;</span><span class="p">]</span>
<span class="n">edge_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#8A0002&#39;</span><span class="p">,</span> <span class="s1">&#39;#3C1F8B&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge_color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span><span class="p">):</span>
    <span class="n">Data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Data</span><span class="o">.</span><span class="n">Outcome</span> <span class="o">==</span> <span class="n">outcome</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Outcome = </span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Synthetic Dataset&#39;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/18e309ab69bdc7df92d673a0563451a808de155002450d1298a051e0028e30eb.png" src="../_images/18e309ab69bdc7df92d673a0563451a808de155002450d1298a051e0028e30eb.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_MyCountPlot</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span> <span class="o">=</span> <span class="n">edge_colors</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
    <span class="c1"># Create a figure and axes with a specified size</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    
    <span class="c1"># Count the occurrences of each category in the specified column</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    
    <span class="c1"># Create a bar plot with specified colors</span>
    <span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="n">edge_colors</span><span class="p">)</span>
    
    <span class="c1"># Set labels and title for the plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Count&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Countplot for </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1"> category&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add count values on top of each bar</span>
    <span class="k">for</span> <span class="n">bar</span> <span class="ow">in</span> <span class="n">bars</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">bar</span><span class="o">.</span><span class="n">get_width</span><span class="p">())</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">bar</span><span class="o">.</span><span class="n">get_width</span><span class="p">(),</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_y</span><span class="p">()</span> <span class="o">+</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_height</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">),</span>
                    <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

    <span class="c1"># Limit the x-axis range as specified by &#39;xlim&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># Call the function with specific parameters</span>
<span class="n">_MyCountPlot</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">],</span> <span class="n">data</span><span class="o">=</span><span class="n">Data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fe0eea741d43ce32f37e8e1e2fdadcbf20dadc7b20ea8b0ac0fc23f21dde6bf5.png" src="../_images/fe0eea741d43ce32f37e8e1e2fdadcbf20dadc7b20ea8b0ac0fc23f21dde6bf5.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#f4cccc&#39;</span><span class="p">,</span> <span class="s1">&#39;#e3d3f2&#39;</span><span class="p">])</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="s1">&#39;abcd&#39;</span><span class="p">):</span>
    <span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">splitter</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                       <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                       <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                       <span class="n">xlabel</span><span class="o">=</span> <span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                       <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge_color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="p">,</span> <span class="n">edge_colors</span><span class="p">):</span>
        <span class="n">Data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">Data</span><span class="o">.</span><span class="n">Outcome</span> <span class="o">==</span> <span class="n">outcome</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                               <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">edge_color</span><span class="p">,</span>
                                               <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Outcome = </span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) max_leaf_nodes = </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>  <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e7ee0477291be735d041bf47fbf747f109c1f177a7f418a62a9f782fb63bf0fd.png" src="../_images/e7ee0477291be735d041bf47fbf747f109c1f177a7f418a62a9f782fb63bf0fd.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">filled</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)],</span>
                   <span class="n">proportion</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d0f5cef86b7264bb3ac0fa8e9a8774db32843e29794d08252a4533d58aefed96.png" src="../_images/d0f5cef86b7264bb3ac0fa8e9a8774db32843e29794d08252a4533d58aefed96.png" />
</div>
</div>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<div class="figure align-center" id="id8">
<a class="reference internal image-reference" href="../_images/dts_fig01.jpg"><img alt="../_images/dts_fig01.jpg" src="../_images/dts_fig01.jpg" style="width: 550px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10.4 </span><span class="caption-text">Visual representation of the above Decision Tree Classifier.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</div>
<!-- 
import dtreeviz

viz_model = dtreeviz.model(dts, X, y, target_name= 'Outcome')
v = viz_model.view()
v.show()  
--></div>
<div class="section" id="id6">
<h2><span class="section-number">10.3.3. </span>Example: Synthetic Dataset<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h2>
<p><font color='Blue'><b>Example</b></font>:  In this code example, we employ a Decision Tree Classifier to demonstrate decision boundaries on synthetic data. The synthetic dataset is created using the <code class="docutils literal notranslate"><span class="pre">make_blobs</span></code> function from scikit-learn and possesses the following characteristics:</p>
<ul class="simple">
<li><p><strong>Sample Count:</strong> 2000</p></li>
<li><p><strong>Feature Count:</strong> 2</p></li>
<li><p><strong>Class Count:</strong> 3</p></li>
<li><p><strong>Random Seed (random_state):</strong> 0</p></li>
<li><p><strong>Cluster Standard Deviation (cluster_std):</strong> 1.0</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>This dataset comprises 2000 data points, each characterized by a pair of feature values known as ‘Feature 1’ and ‘Feature 2.’</p></li>
</ul>
<p><strong>Target Variable (Outcome):</strong></p>
<ul class="simple">
<li><p>The dataset incorporates a target variable labeled ‘Outcome,’ which assigns each data point to one of three distinct classes: ‘Class 0,’ ‘Class 1,’ and ‘Class 2.’</p></li>
</ul>
<p><strong>Colormap:</strong></p>
<ul class="simple">
<li><p>To facilitate class differentiation in the visualization, a vibrant colormap is employed for the scatter plot. It showcases three distinct colors: ‘Red,’ ‘Blue,’ and ‘Green.’</p></li>
</ul>
<p>This dataset is designed to emulate a scenario featuring three well-separated clusters, rendering it apt for endeavors related to multi-class classification. Each data point is assigned to one of the three classes, providing a foundation for experimentation and evaluation of machine learning algorithms tailored to multi-class classification challenges.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Create a bold colormap for the scatterplot</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;Red&quot;</span><span class="p">,</span> <span class="s2">&quot;Blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Green&quot;</span><span class="p">])</span>

<span class="c1"># Generate synthetic data using the make_blobs function</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Create a DataFrame to hold the generated data</span>
<span class="n">Data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Feature </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">Data</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

<span class="c1"># Display the first few rows of the data</span>
<span class="n">display</span><span class="p">(</span><span class="n">Data</span><span class="p">)</span>

<span class="c1"># Create a scatter plot using Seaborn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span>
                    <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>

<span class="c1"># Adjust layout for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature 1</th>
      <th>Feature 2</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.641391</td>
      <td>3.169513</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.293867</td>
      <td>4.093943</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.832691</td>
      <td>3.921762</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.607868</td>
      <td>4.681546</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.338723</td>
      <td>1.495610</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>0.279943</td>
      <td>4.013390</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>3.655535</td>
      <td>0.708829</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>1.512867</td>
      <td>4.708483</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>0.569967</td>
      <td>3.439742</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>2.006708</td>
      <td>2.256464</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div></div><img alt="../_images/015d2cff1c2e3a608eda2bc4352caf4d2440277d4d09d256b618ec817f64af73.png" src="../_images/015d2cff1c2e3a608eda2bc4352caf4d2440277d4d09d256b618ec817f64af73.png" />
</div>
</div>
<p>Let’s look at the dataset distribution</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_MyCountPlot</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">Data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a5d74dcea77d5eaef089d9093798b5c77b568a7e8eca3b132f4c7bb4ded81fba.png" src="../_images/a5d74dcea77d5eaef089d9093798b5c77b568a7e8eca3b132f4c7bb4ded81fba.png" />
</div>
</div>
<p>The classifier is trained with varying levels of maximum leaf nodes (10, 50, 200, and None) to demonstrate the impact of this parameter on the decision boundaries. The resulting plots are organized in a 2x2 grid to allow easy comparison. The decision boundaries are displayed using a light colormap, while the scatterplot of the synthetic data is overlaid for context. The x and y axes are labeled as ‘Feature 1’ and ‘Feature 2’, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;MistyRose&quot;</span><span class="p">,</span> <span class="s2">&quot;LightBlue&quot;</span><span class="p">,</span> <span class="s2">&quot;HoneyDew&quot;</span><span class="p">])</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="s1">&#39;abcd&#39;</span><span class="p">):</span>
    <span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">splitter</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                                       <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
                                       <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
                                       <span class="n">xlabel</span><span class="o">=</span> <span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span>
                                       <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">Data</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;Feature 1&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;Feature 2&#39;</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s1">&#39;Outcome&#39;</span><span class="p">,</span>
                        <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">alph</span><span class="si">}</span><span class="s1">) max_leaf_nodes = </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>  <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7255f6b5fff5d458d9a607b0bac2e2673ee9558730919a732836205307422761.png" src="../_images/7255f6b5fff5d458d9a607b0bac2e2673ee9558730919a732836205307422761.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">filled</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)],</span>
                   <span class="n">proportion</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/38f5333f9b950aca492c499e2ca96bafe582c6bbe32b0e15717e742d13212ebf.png" src="../_images/38f5333f9b950aca492c499e2ca96bafe582c6bbe32b0e15717e742d13212ebf.png" />
</div>
</div>
<p>In this code:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth=None</span></code>: The <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> parameter determines the maximum depth of the decision tree. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the tree will expand until all leaves are pure (contain only one class) or until they have fewer samples than the minimum samples required for a leaf node split. A deeper tree can capture more complex relationships in the data, but it might also lead to overfitting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=6</span></code>: This parameter restricts the maximum number of leaf nodes that the decision tree can have. Limiting the number of leaf nodes can help control the complexity of the tree and prevent overfitting. In this case, the tree is allowed to have up to 6 leaf nodes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_features=6</span></code>: This parameter defines the maximum number of features that are considered when looking for the best split at each node. It’s a way to control the randomness in the tree and potentially improve its generalization. Setting <code class="docutils literal notranslate"><span class="pre">max_features</span></code> to a specific number restricts the choice of features for each split. In this case, the model considers at most 6 features for each split.</p></li>
</ol>
<p>The following figure was produced using <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<div class="figure align-center" id="id9">
<a class="reference internal image-reference" href="../_images/dts_fig02.jpg"><img alt="../_images/dts_fig02.jpg" src="../_images/dts_fig02.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10.5 </span><span class="caption-text">Visual representation of the above Decision Tree Classifier.</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</div>
<!-- 
import dtreeviz

viz_model = dtreeviz.model(dts, X, y, target_name= 'Outcome')
v = viz_model.view()
v.show()  
--></div>
<div class="section" id="optimizing-decisiontreeclassifier-parameters">
<h2><span class="section-number">10.3.4. </span>Optimizing DecisionTreeClassifier Parameters<a class="headerlink" href="#optimizing-decisiontreeclassifier-parameters" title="Permalink to this headline">#</a></h2>
<p><font color='Blue'><b>Example:</b></font> In our initial demonstration, we will use the previously synthesized dataset with three classes, employing the default settings of the DecisionTreeClassifier (DTS).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Create a DecisionTreeClassifier instance</span>
<span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

<span class="c1"># Get the parameters of the DecisionTreeClassifier and print the parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dts</span><span class="o">.</span><span class="n">get_params</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;random_state&#39;: None, &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="c1"># Initialize KFold cross-validator</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Lists to store train and test scores for each fold</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Perform Cross-Validation</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># train</span>
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">dts</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
    <span class="c1"># test</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">dts</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>

<span class="c1">#  Print the Train and Test Scores for each fold</span>
<span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Train Score = </span><span class="si">{</span><span class="n">train_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Score = </span><span class="si">{</span><span class="n">test_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and print the average train and test scores</span>
<span class="n">average_train_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span>
<span class="n">average_test_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">30</span><span class="o">*</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Train Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Test Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">30</span><span class="o">*</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fold 1: Train Score = 1.0000, Test Score = 0.8825
Fold 2: Train Score = 1.0000, Test Score = 0.8725
Fold 3: Train Score = 1.0000, Test Score = 0.9025
Fold 4: Train Score = 1.0000, Test Score = 0.9125
Fold 5: Train Score = 1.0000, Test Score = 0.8825
______________________________
Mean Train Score: 1.0000 ± 0.0000
Mean Test Score: 0.8905 ± 0.0147
______________________________
</pre></div>
</div>
</div>
</div>
<p><strong>Fold-wise Train and Test Scores:</strong></p>
<p>For each fold (iteration) of the cross-validation process, both training and testing accuracy scores are provided.</p>
<ul class="simple">
<li><p><strong>Fold 1: Train Score = 1.0000, Test Score = 0.8825</strong>: In the first fold, the model achieved a perfect training score of 1.0, indicating it has memorized the training data. The test score is 0.8825, suggesting good generalization to new, unseen data.</p></li>
<li><p><strong>Fold 2: Train Score = 1.0000, Test Score = 0.8725</strong>: In the second fold, similar to the first, the model demonstrates a perfect training score but with a slightly lower test score of 0.8725, indicating slightly less generalization capability.</p></li>
<li><p><strong>Fold 3: Train Score = 1.0000, Test Score = 0.9025</strong>: The third fold also displays perfect training performance and a higher test score of 0.9025, showing good generalization.</p></li>
<li><p><strong>Fold 4: Train Score = 1.0000, Test Score = 0.9125</strong>: In the fourth fold, both training and test scores are perfect, suggesting excellent model performance in both aspects.</p></li>
<li><p><strong>Fold 5: Train Score = 1.0000, Test Score = 0.8825</strong>: Similar to previous folds, the model achieves a perfect training score, and the test score is 0.8825, indicating strong generalization.</p></li>
</ul>
<p><strong>Mean Train and Test Scores:</strong></p>
<p>After evaluating the model on all folds, the mean training and test scores, along with their variability, are calculated and displayed.</p>
<ul class="simple">
<li><p><strong>Mean Train Score: 1.0000 ± 0.0000</strong>: The mean training score remains at a perfect 1.0, with no variability (± 0.0000), suggesting consistent memorization of the training data across all folds.</p></li>
<li><p><strong>Mean Test Score: 0.8905 ± 0.0147</strong>: The mean test score is 0.8905, indicating the average performance of the model on the test data across all folds. The slight variability (± 0.0147) suggests a minor fluctuation in test scores across the folds.</p></li>
</ul>
<p><strong>Interpretation:</strong></p>
<p>The results indicate that the DecisionTreeClassifier consistently achieves a perfect training score across all folds, which may suggest overfitting to the training data. However, the test scores are relatively high, ranging from 0.8725 to 0.9125, which indicates good generalization to new, unseen data.</p>
<p>The small variability in test scores (± 0.0147) demonstrates the model’s stability in performance across different folds. The mean test score of 0.8905 reflects the model’s average performance on new, unseen data, which is quite robust, considering the variability.</p>
<p><font color='Blue'><b>Example:</b></font> We will work with the previously generated synthetic dataset. However, we will introduce some modifications to the default settings of the DecisionTreeClassifier (DTS).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Create a DecisionTreeClassifier instance</span>
<span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Get the parameters of the DecisionTreeClassifier and print the parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dts</span><span class="o">.</span><span class="n">get_params</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: 6, &#39;max_leaf_nodes&#39;: 6, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;random_state&#39;: None, &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize KFold cross-validator</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Lists to store train and test scores for each fold</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Perform Cross-Validation</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># slighlty change the parameters</span>
    <span class="n">dts</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">dts</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># train</span>
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">dts</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">train_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
    <span class="c1"># test</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">dts</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
    <span class="n">test_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span>

<span class="c1">#  Print the Train and Test Scores for each fold</span>
<span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fold </span><span class="si">{</span><span class="n">fold</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Train Score = </span><span class="si">{</span><span class="n">train_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Score = </span><span class="si">{</span><span class="n">test_scores</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Calculate and print the average train and test scores</span>
<span class="n">average_train_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span>
<span class="n">average_test_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">30</span><span class="o">*</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Train Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Test Score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">30</span><span class="o">*</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fold 1: Train Score = 0.9250, Test Score = 0.9175
Fold 2: Train Score = 0.9231, Test Score = 0.8875
Fold 3: Train Score = 0.9187, Test Score = 0.9225
Fold 4: Train Score = 0.9169, Test Score = 0.9300
Fold 5: Train Score = 0.9269, Test Score = 0.9125
______________________________
Mean Train Score: 0.9221 ± 0.0038
Mean Test Score: 0.9140 ± 0.0145
______________________________
</pre></div>
</div>
</div>
</div>
<p><strong>1. Fold-wise Train and Test Scores:</strong></p>
<ul class="simple">
<li><p>In each fold of the cross-validation process, both training and testing accuracy scores are provided.</p></li>
<li><p><strong>Fold 1: Train Score = 0.9250, Test Score = 0.9175</strong>: In the first fold, the model achieved a training accuracy score of 0.9250 and a test accuracy score of 0.9175. This indicates that the model has successfully captured patterns from the training data and demonstrates good generalization to new, unseen data within this fold.</p></li>
<li><p>The subsequent folds display varying training and test scores, offering insights into the model’s performance across different subsets of the data.</p></li>
</ul>
<p><strong>2. Mean Train and Test Scores:</strong></p>
<ul class="simple">
<li><p>After evaluating the model across all folds, the mean training and test scores, along with their variability, are computed and presented.</p></li>
<li><p><strong>Mean Train Score: 0.9221 ± 0.0038</strong>: The mean training score is 0.9221, signifying that the model’s average performance on the training data is approximately 92.21%. The “± 0.0038” indicates a slight degree of variability in the training scores across the folds.</p></li>
<li><p><strong>Mean Test Score: 0.9140 ± 0.0145</strong>: The mean test score is 0.9140, highlighting the model’s average performance on the test data across all folds. The “± 0.0145” signifies a minor level of variability in the test scores across the folds.</p></li>
</ul>
<p><strong>3. Interpretation:</strong></p>
<ul class="simple">
<li><p>The results demonstrate a relatively consistent performance of the model across the different folds in the cross-validation process.</p></li>
<li><p>Training scores are slightly higher than test scores, which is expected, as the model is trained on the training data. However, the differences are not substantial, indicating that the model is not significantly overfitting.</p></li>
<li><p>The mean test score of 0.9140 indicates that, on average, the model performs well on new, unseen data. The small variability (± 0.0145) in the test scores across folds suggests that the model’s generalization performance remains stable.</p></li>
</ul>
<ol class="arabic">
<li><p><strong>Fold-wise Training and Testing Accuracy:</strong>
In this section, the performance of the model is assessed fold by fold using cross-validation. Each fold’s training and testing accuracy scores are showcased individually to provide insights into the model’s behavior on different subsets of data.</p>
<p>For instance, the line “Fold 1: Train Score = 0.7521, Test Score = 0.8525” reveals that in the first fold, the model attained a training accuracy score of 0.7521 and a test accuracy score of 0.8525. These values indicate that the model effectively grasped patterns from the training data and was able to generalize this knowledge to new, unseen data within this fold.</p>
<p>Similar evaluations are presented for Folds 2 through 5, offering a comprehensive perspective on the model’s performance variability across diverse data segments.</p>
</li>
<li><p><strong>Mean Training and Testing Accuracy:</strong>
After evaluating the model across all folds, the average training and testing accuracy scores are computed and displayed in this section.</p>
<p>The “Mean Train Score: 0.7822 ± 0.0258” signifies that the model achieved an average training score of 0.7822, implying a consistent performance level of around 78.22% across the different folds. The “± 0.0258” conveys a slight level of variability in the training scores, suggesting that the model’s performance is relatively stable.</p>
<p>Similarly, the “Mean Test Score: 0.7785 ± 0.0854” denotes the model’s average performance on the test data across all folds, yielding an average accuracy of approximately 77.85%. The “± 0.0854” represents a moderate degree of variability in the test scores, indicating that the model’s ability to generalize to new data varies moderately across folds.</p>
</li>
<li><p><strong>Interpretation of Results:</strong>
An analysis of the presented results yields several insights:</p>
<ul class="simple">
<li><p>The model exhibits a consistent performance pattern across different folds, showcasing its general stability.</p></li>
<li><p>Although the training scores are slightly higher than the test scores, the difference is not substantial, indicating that the model’s training is not leading to significant overfitting.</p></li>
<li><p>The mean test score of 0.7785 suggests that the model demonstrates moderate proficiency when presented with new, unseen data. The moderate variability (± 0.0854) in the test scores highlights the model’s adaptability to diverse scenarios, albeit with some variation in its generalization ability.</p></li>
</ul>
</li>
</ol>
<p>To improve the accuracy results of your Decision Tree Classifier, you can try the following strategies:</p>
<ol class="arabic simple">
<li><p><strong>Hyperparameter Tuning:</strong> Experiment with different hyperparameters of the DecisionTreeClassifier to find the combination that gives the best results. You’ve already set <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>, and <code class="docutils literal notranslate"><span class="pre">max_features</span></code>, but you can also consider other parameters like <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>. Using a technique like GridSearchCV or RandomizedSearchCV can help you systematically search through a range of hyperparameter values.</p></li>
<li><p><strong>Feature Engineering:</strong> Carefully select and engineer your features. Sometimes, adding relevant features or transforming existing ones can improve the model’s performance.</p></li>
<li><p><strong>Ensemble Methods:</strong> Consider using ensemble methods like Random Forests or Gradient Boosting. These methods combine multiple decision trees to improve overall performance and reduce overfitting.</p></li>
<li><p><strong>Data Preprocessing:</strong> Ensure your data is properly preprocessed. This includes handling missing values, scaling or normalizing numerical features, and encoding categorical variables correctly.</p></li>
<li><p><strong>Handling Imbalanced Data:</strong> If your classes are imbalanced, techniques like oversampling, undersampling, or using specialized algorithms like SMOTE (Synthetic Minority Over-sampling Technique) can help balance the class distribution.</p></li>
<li><p><strong>Cross-Validation Strategies:</strong> Try different cross-validation strategies such as stratified k-fold or leave-one-out cross-validation. These strategies can help ensure that each fold represents the data distribution more accurately.</p></li>
<li><p><strong>Different Algorithms:</strong> While Decision Trees can be powerful, other algorithms like Random Forests, Support Vector Machines, and Neural Networks might perform better for certain datasets.</p></li>
</ol>
<p>Remember that improving accuracy might require experimentation and domain knowledge to find the best strategies for your specific dataset and problem.</p>
<p>Here’s an example of how you might apply hyperparameter tuning using GridSearchCV:</p>
<div class="section" id="gridsearchcv">
<h3><span class="section-number">10.3.4.1. </span>GridSearchCV<a class="headerlink" href="#gridsearchcv" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> is a method provided by the scikit-learn library in Python that performs hyperparameter tuning for machine learning models. Hyperparameters are parameters that are set before the training process begins and cannot be learned from the data. They significantly impact the performance and behavior of a machine learning model. <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> is a technique used to systematically search through a predefined set of hyperparameter values to find the combination that results in the best model performance <span id="id7">[<a class="reference internal" href="../References.html#id4" title="F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p>Here’s how <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> works:</p>
<ol class="arabic simple">
<li><p><strong>Select Model and Hyperparameters:</strong> You first choose the machine learning algorithm you want to use (e.g., Decision Tree, Random Forest, Support Vector Machine, etc.) and determine the hyperparameters associated with that algorithm that you want to tune.</p></li>
<li><p><strong>Define Parameter Grid:</strong> You create a dictionary where the keys are the names of the hyperparameters you want to tune, and the values are lists of possible values for those hyperparameters. This forms a grid of all possible combinations.</p></li>
<li><p><strong>Cross-Validation:</strong> <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> uses cross-validation to evaluate the performance of each combination of hyperparameters. Cross-validation involves splitting your dataset into multiple subsets (folds), training the model on a subset of the data, and validating it on the remaining subset. This process is repeated for each fold, and the results are averaged to provide a more robust evaluation.</p></li>
<li><p><strong>Evaluation:</strong> For each combination of hyperparameters, <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> trains the model using cross-validation and calculates a performance metric (such as accuracy, F1-score, etc.) for each fold. The average performance across all folds is then used as the evaluation metric for that combination.</p></li>
<li><p><strong>Best Hyperparameters:</strong> After evaluating all combinations, <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> identifies the combination of hyperparameters that resulted in the best performance metric.</p></li>
<li><p><strong>Retrain and Test:</strong> Once the best hyperparameters are found, you can retrain your model using the entire training dataset and these optimized hyperparameters. Then, you can test the final model on an independent test dataset to assess its generalization performance.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Define the hyperparameters to tune</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;max_leaf_nodes&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Create a custom scoring function for F1 score in a multiclass setting</span>
<span class="n">scoring</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>

<span class="c1"># Initialize GridSearchCV</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
                           <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                           <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span><span class="p">,</span>
                           <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Perform GridSearchCV</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print best hyperparameters and corresponding score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best Hyperparameters:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best Score:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best Hyperparameters: {&#39;max_depth&#39;: 5, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: 15}
Best Score: 0.921662911966974
</pre></div>
</div>
</div>
</div>
<p>Let’s break down the code step by step:</p>
<ol class="arabic simple">
<li><p><strong>Import Necessary Libraries:</strong></p>
<ul class="simple">
<li><p>The code imports <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code> and <code class="docutils literal notranslate"><span class="pre">metrics</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. These libraries are used for performing hyperparameter tuning and evaluating the model’s performance.</p></li>
</ul>
</li>
<li><p><strong>Define the Hyperparameters to Tune:</strong></p>
<ul class="simple">
<li><p>A dictionary called <code class="docutils literal notranslate"><span class="pre">param_grid</span></code> is defined, which lists various hyperparameters for the DecisionTreeClassifier that you want to tune. These hyperparameters include:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth</span></code>: The maximum depth of the decision tree.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>: The maximum number of leaf nodes in the tree.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_features</span></code>: The maximum number of features to consider when making splits.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Create a Custom Scoring Function for F1 Score:</strong></p>
<ul class="simple">
<li><p>The code uses <code class="docutils literal notranslate"><span class="pre">metrics.make_scorer</span></code> to create a custom scoring function for the F1 score in a multiclass setting. The <code class="docutils literal notranslate"><span class="pre">average='weighted'</span></code> parameter indicates that the weighted F1 score should be used as the evaluation metric. This is suitable for multiclass classification problems.</p></li>
</ul>
</li>
<li><p><strong>Initialize GridSearchCV:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> is initialized with the following parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">estimator=DecisionTreeClassifier()</span></code>: This specifies the model you want to tune, which is a DecisionTreeClassifier in this case.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">param_grid=param_grid</span></code>: It provides the hyperparameter grid to explore.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scoring=scoring</span></code>: The custom scoring function for evaluation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cv=5</span></code>: The number of cross-validation folds (in this case, 5-fold cross-validation).</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Perform GridSearchCV:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">grid_search.fit(X,</span> <span class="pre">y)</span></code> is used to perform the hyperparameter tuning. <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> should be your feature matrix and target vector.</p></li>
</ul>
</li>
<li><p><strong>Print Best Hyperparameters and Score:</strong></p>
<ul class="simple">
<li><p>After the grid search is completed, the code prints the best hyperparameters found by GridSearchCV using <code class="docutils literal notranslate"><span class="pre">grid_search.best_params_</span></code>. It also prints the corresponding score using <code class="docutils literal notranslate"><span class="pre">grid_search.best_score_</span></code>, which is the best weighted F1 score achieved during the grid search.</p></li>
</ul>
</li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C10S2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10.2. </span>Regression Trees</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C10S4.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10.4. </span>Regression Trees and Linear Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decisiontreeclassifier-algorithm-in-scikit-learn">10.3.1. DecisionTreeClassifier algorithm in scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-synthetic-dataset">10.3.2. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">10.3.3. Example: Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-decisiontreeclassifier-parameters">10.3.4. Optimizing DecisionTreeClassifier Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gridsearchcv">10.3.4.1. GridSearchCV</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>