

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.1. Fundamental Structure of Decision Trees &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG_680_C10S1';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.2. Regression Trees" href="ENGG_680_C10S2.html" />
    <link rel="prev" title="10. Tree-Based Methods" href="ReadMe.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression  (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">10. Tree-Based Methods</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ReadMe.html">12. Introduction to Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S2.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S3.html">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fundamental Structure of Decision Trees</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-nodes-internal-nodes-and-leaf-nodes">10.1.1. Root Nodes, Internal Nodes, and Leaf Nodes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-tree-based-methods">10.1.2. Popular tree-based methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-of-building-decision-trees">10.1.3. Process of Building Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages-of-decision-trees">10.1.4. Advantages and Disadvantages of Decision Trees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-decision-trees">10.1.4.1. Advantages of Decision Trees:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages-of-decision-trees">10.1.4.2. Disadvantages of Decision Trees:</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="fundamental-structure-of-decision-trees">
<h1><span class="section-number">10.1. </span>Fundamental Structure of Decision Trees<a class="headerlink" href="#fundamental-structure-of-decision-trees" title="Permalink to this headline">#</a></h1>
<p>Decision trees are structured models that represent a sequence of decisions, leading to predictions or classifications. Picture an upside-down tree where each node represents a decision or a prediction. The tree starts at the top with the root node and branches down, ending with leaf nodes at the bottom. This structure reflects a step-by-step process of answering questions or evaluating conditions to arrive at an outcome.</p>
<div class="section" id="root-nodes-internal-nodes-and-leaf-nodes">
<h2><span class="section-number">10.1.1. </span>Root Nodes, Internal Nodes, and Leaf Nodes<a class="headerlink" href="#root-nodes-internal-nodes-and-leaf-nodes" title="Permalink to this headline">#</a></h2>
<p><strong>1. Root Node:</strong></p>
<p>The root node serves as the starting point for the decision tree, symbolizing the initial query that sets the tone for subsequent decisions. It is the first question posed to the dataset, splitting the data into subsets based on a chosen feature’s condition. This division sets the groundwork for the tree’s subsequent branching and decision-making process. In a classification scenario, the root node may pose a question such as “Is the income greater than $50,000?” which divides the data into two subsets: those with income above $50,000 and those with income below or equal to $50,000 <span id="id1">[<a class="reference internal" href="../References.html#id49" title="E. Alpaydin. Introduction to Machine Learning, fourth edition. Adaptive Computation and Machine Learning series. MIT Press, 2020. ISBN 9780262043793. URL: https://books.google.ca/books?id=tZnSDwAAQBAJ.">Alpaydin, 2020</a>, <a class="reference internal" href="../References.html#id126" title="Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Volume 4. Springer, 2006.">Bishop and Nasrabadi, 2006</a>, <a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>.</p>
<p><font color='Blue'><b>Example:</b></font>
In a medical diagnosis context, the root node could ask, “Is the patient’s temperature above 38°C?” Based on the answer, the data splits into two subsets: one with high temperature and another with normal temperature. This division lays the foundation for the subsequent tree structure, where further questions are posed to narrow down the diagnosis.</p>
<p><strong>2. Internal Nodes:</strong>
Internal nodes act as decision points in the tree, presenting questions about specific features to further divide the data. These nodes guide the data down different branches based on the answers to the questions they pose. Each internal node represents a feature and its associated condition, dictating the path data points will follow as they traverse the tree. Internal nodes play a pivotal role in navigating through the decision-making process.</p>
<p><font color='Blue'><b>Example:</b></font>
Continuing with the medical diagnosis example, an internal node might ask, “Does the patient have a cough?” If the answer is “Yes,” the data proceeds down one branch, and if the answer is “No,” it goes down another. This branching allows the model to consider various symptoms and their interplay in making an accurate diagnosis.</p>
<div class="figure align-center" id="id15">
<a class="reference internal image-reference" href="../_images/Tree_Based_Example.png"><img alt="../_images/Tree_Based_Example.png" src="../_images/Tree_Based_Example.png" style="width: 480px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10.1 </span><span class="caption-text">An Example of Root Node and Internal Nodes</span><a class="headerlink" href="#id15" title="Permalink to this image">#</a></p>
</div>
<p><strong>3. Leaf Nodes:</strong>
Leaf nodes mark the conclusion of a decision path within the tree. They represent the ultimate predictions or classifications that the tree arrives at. Each leaf node corresponds to a specific outcome, category, or numerical value. Once a data point reaches a leaf node, the prediction or classification associated with that leaf node becomes the final decision for that data point.</p>
<p><font color='Blue'><b>Example:</b></font>
As we progress in the medical diagnosis scenario, we arrive at the leaf nodes, which represent the ultimate conclusions in the decision tree. For instance, a leaf node could correspond to the diagnosis ‘Common Cold’ if a patient’s symptoms align with that specific outcome. Alternatively, another leaf node might represent the diagnosis ‘Influenza’ for a different set of symptoms. Each leaf node signifies a definitive decision or classification, and once a patient’s data reaches a leaf node, the corresponding diagnosis becomes the final determination for that particular case.</p>
</div>
<div class="section" id="popular-tree-based-methods">
<h2><span class="section-number">10.1.2. </span>Popular tree-based methods<a class="headerlink" href="#popular-tree-based-methods" title="Permalink to this headline">#</a></h2>
<p>Some popular tree-based methods for classification and regression are <span id="id2">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<ul class="simple">
<li><p><strong>Decision trees</strong>: They split the predictor space into simple regions based on rules, and assign the target variable class or value to each region.</p></li>
<li><p><strong>Random forests</strong>: They combine many decision trees, each trained on a random subset of the data and features, and average their predictions.</p></li>
<li><p><strong>Boosting</strong>: They iteratively fit decision trees to the residuals of the previous trees, and combine them with weights to improve accuracy.</p></li>
</ul>
<p>Scikit-Learn (sklearn) offers an array of robust methods for building classification and regression trees, each tailored to specific tasks and data complexities <span id="id3">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<p><strong>Classification Tree Methods:</strong></p>
<ol class="arabic simple">
<li><p><strong>DecisionTreeClassifier:</strong> Leveraging the CART algorithm, this class constructs classification trees. It employs Gini impurity or entropy as criteria for selecting optimal splits. By adjusting parameters like <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>, you can fine-tune the depth and node splitting strategy <span id="id4">[<a class="reference internal" href="../References.html#id143" title="K. Gallatin and C. Albon. Machine Learning with Python Cookbook. O'Reilly Media, 2023. ISBN 9781098135690. URL: https://books.google.ca/books?id=Wq3NEAAAQBAJ.">Gallatin and Albon, 2023</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>RandomForestClassifier:</strong> Embracing ensemble methods, RandomForestClassifier amalgamates multiple decision trees, amplifying classification accuracy. It mitigates overfitting concerns by aggregating predictions across trees and randomly selecting features, ensuring robust generalization <span id="id5">[<a class="reference internal" href="../References.html#id53" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>GradientBoostingClassifier:</strong> Enabling the power of gradient boosting for classification, this method orchestrates a sequence of weak learners, often decision trees. It iteratively adjusts their weights to enhance predictive performance, making it adept at capturing complex relationships <span id="id6">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>AdaBoostClassifier:</strong> By harnessing adaptive boosting, AdaBoostClassifier consolidates several weak classifiers—frequently decision trees—into a formidable classification tool. It accentuates misclassified instances, refining accuracy through focused learning <span id="id7">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
</ol>
<p><strong>Regression Tree Methods:</strong></p>
<ol class="arabic simple">
<li><p><strong>DecisionTreeRegressor:</strong> Mirroring its classification counterpart, DecisionTreeRegressor assembles regression trees via the CART algorithm. It prioritizes minimizing mean squared error, pivotal in selecting optimal feature splits for enhanced prediction precision <span id="id8">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>RandomForestRegressor:</strong> In the realm of regression, RandomForestRegressor shines as a counterpart to the classification variant. It fuses numerous regression trees, collectively predicting continuous target variables with improved robustness <span id="id9">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>GradientBoostingRegressor:</strong> Mirroring the prowess of GradientBoostingClassifier, this method crafts a sequence of weak regression models via gradient boosting. It optimizes model weights iteratively, enhancing predictive accuracy for regression tasks <span id="id10">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
<li><p><strong>AdaBoostRegressor:</strong> Applying adaptive boosting, AdaBoostRegressor synergizes weak regression models into a potent predictor. It focuses on data points with elevated prediction errors, refining the model’s performance with targeted learning <span id="id11">[<a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p></li>
</ol>
<p>Scikit-Learn’s comprehensive suite of classification and regression tree methods empowers data scientists and machine learning practitioners to tackle diverse challenges, crafting accurate models tailored to specific objectives and data characteristics <span id="id12">[<a class="reference internal" href="../References.html#id4" title="F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
</div>
<div class="section" id="process-of-building-decision-trees">
<h2><span class="section-number">10.1.3. </span>Process of Building Decision Trees<a class="headerlink" href="#process-of-building-decision-trees" title="Permalink to this headline">#</a></h2>
<p>The process of building decision trees for regression and classification tasks involves similar steps with some variations in criteria and considerations. Here’s an overview of the process for both scenarios <span id="id13">[<a class="reference internal" href="../References.html#id49" title="E. Alpaydin. Introduction to Machine Learning, fourth edition. Adaptive Computation and Machine Learning series. MIT Press, 2020. ISBN 9780262043793. URL: https://books.google.ca/books?id=tZnSDwAAQBAJ.">Alpaydin, 2020</a>, <a class="reference internal" href="../References.html#id126" title="Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Volume 4. Springer, 2006.">Bishop and Nasrabadi, 2006</a>, <a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>]</span>:</p>
<p><strong>Building Decision Trees for Classification:</strong></p>
<ol class="arabic simple">
<li><p><strong>Data Preparation:</strong> Begin with a labeled dataset containing input features and corresponding class labels.</p></li>
<li><p><strong>Choosing a Criterion:</strong> Select a criterion to measure the quality of splits. Common criteria include Gini impurity and entropy. These quantify the impurity or randomness in class distribution within subsets.</p></li>
<li><p><strong>Root Node Selection:</strong> Choose the best feature and split point for the root node based on the selected criterion. This minimizes impurity or maximizes information gain.</p></li>
<li><p><strong>Recursive Splitting:</strong> For each internal node, recursively select the best feature and split point based on the chosen criterion. Continue until a stopping criterion is met, such as reaching a maximum depth or a minimum number of samples in a leaf node.</p></li>
<li><p><strong>Leaf Node Creation:</strong> When a stopping criterion is met or the data is sufficiently pure, create a leaf node. Assign the majority class in the subset to the leaf node.</p></li>
<li><p><strong>Prediction:</strong> To classify a new data point, traverse the tree from the root to a leaf node based on feature conditions. The leaf node’s majority class becomes the predicted class.</p></li>
</ol>
<p><strong>Building Decision Trees for Regression:</strong></p>
<ol class="arabic simple">
<li><p><strong>Data Preparation:</strong> Begin with a dataset containing input features and corresponding numeric target values.</p></li>
<li><p><strong>Choosing a Criterion:</strong> In regression, the criterion typically used is mean squared error (MSE). It measures the average squared difference between predicted and actual target values.</p></li>
<li><p><strong>Root Node Selection:</strong> Choose the best feature and split point for the root node to minimize the MSE.</p></li>
<li><p><strong>Recursive Splitting:</strong> Similar to classification, recursively select the best feature and split point based on the MSE. Stop when a stopping criterion is met.</p></li>
<li><p><strong>Leaf Node Creation:</strong> When the stopping criterion is met, create a leaf node. Assign the mean of target values in the subset to the leaf node.</p></li>
<li><p><strong>Prediction:</strong> To predict a new target value, traverse the tree from the root to a leaf node based on feature conditions. The leaf node’s mean target value becomes the predicted value.</p></li>
</ol>
<p>Both regression and classification decision trees aim to create a tree structure that captures patterns and relationships in the data. The main difference lies in the choice of criterion and the type of output being predicted. Keep in mind that while decision trees can be highly interpretable, they are susceptible to overfitting. Techniques like setting maximum depth, pruning, and using ensemble methods can help address this challenge and improve the model’s generalization capabilities.</p>
</div>
<div class="section" id="advantages-and-disadvantages-of-decision-trees">
<h2><span class="section-number">10.1.4. </span>Advantages and Disadvantages of Decision Trees<a class="headerlink" href="#advantages-and-disadvantages-of-decision-trees" title="Permalink to this headline">#</a></h2>
<p><strong>Decision Trees (DTs)</strong> are a versatile machine learning technique used for both classification and regression tasks. They operate by constructing a tree-like model where each internal node represents a decision based on the values of input features, and each leaf node corresponds to a predicted value or class label. Decision trees aim to create simple decision rules from provided data, enhancing interpretability and visualization. This tree structure enables piecewise constant approximations of complex relationships within the data <span id="id14">[<a class="reference internal" href="../References.html#id3" title="Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics. Springer Cham, 2023. ISBN 9783031391897. URL: https://link.springer.com/book/10.1007/978-3-031-38747-0.">James <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id57" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="section" id="advantages-of-decision-trees">
<h3><span class="section-number">10.1.4.1. </span>Advantages of Decision Trees:<a class="headerlink" href="#advantages-of-decision-trees" title="Permalink to this headline">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Interpretability</strong>: Decision trees are intuitively comprehensible and their structure can be easily visualized, aiding in explaining prediction rationale.</p></li>
<li><p><strong>Minimal Data Preparation</strong>: Compared to other algorithms, decision trees require less data preprocessing, alleviating the need for normalization, dummy variables, or missing value handling.</p></li>
<li><p><strong>Efficiency</strong>: Prediction cost for decision trees grows logarithmically with the training data size, rendering them computationally efficient.</p></li>
<li><p><strong>Handling of Mixed Data</strong>: Decision trees adeptly accommodate both numerical and categorical data, though certain implementations, such as scikit-learn’s, may have categorical variable limitations.</p></li>
<li><p><strong>Multi-output Support</strong>: Decision trees can address problems with multiple target variables, facilitating solutions for multi-output problems.</p></li>
<li><p><strong>White Box Model</strong>: Decision trees remain transparent, providing easily interpretable explanations for predictions, in contrast to “black box” models like neural networks.</p></li>
<li><p><strong>Validation and Reliability</strong>: The model can undergo validation through statistical tests, aiding in evaluating its dependability.</p></li>
<li><p><strong>Robustness</strong>: Decision trees can yield reasonable performance even in scenarios where model assumptions slightly deviate from actual data-generating processes.</p></li>
</ol>
</div>
<div class="section" id="disadvantages-of-decision-trees">
<h3><span class="section-number">10.1.4.2. </span>Disadvantages of Decision Trees:<a class="headerlink" href="#disadvantages-of-decision-trees" title="Permalink to this headline">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Overfitting</strong>: Decision trees can overly tailor themselves to training data, causing poor generalization to new data. Pruning and controlling tree depth are techniques to counteract this issue.</p></li>
<li><p><strong>Instability</strong>: Minor data changes may yield vastly different trees, resulting in model instability. Ensemble methods like Random Forests mitigate this problem.</p></li>
<li><p><strong>Lack of Smoothness</strong>: Decision tree predictions manifest as piecewise constant approximations, rendering them unsuitable for smooth extrapolation.</p></li>
<li><p><strong>Computational Complexity</strong>: Optimal decision tree learning poses computational challenges; practical algorithms utilize heuristics to attain locally optimal solutions. Ensemble methods mitigate this limitation.</p></li>
<li><p><strong>Difficulty with Complex Concepts</strong>: Complex logical concepts like XOR, parity, or multiplexer problems may challenge decision tree learning.</p></li>
<li><p><strong>Bias in Imbalanced Data</strong>: Decision trees may yield biased models when certain classes dominate the dataset. Balancing the dataset before training can alleviate this concern.</p></li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ReadMe.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Tree-Based Methods</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C10S2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10.2. </span>Regression Trees</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-nodes-internal-nodes-and-leaf-nodes">10.1.1. Root Nodes, Internal Nodes, and Leaf Nodes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-tree-based-methods">10.1.2. Popular tree-based methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-of-building-decision-trees">10.1.3. Process of Building Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages-of-decision-trees">10.1.4. Advantages and Disadvantages of Decision Trees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-decision-trees">10.1.4.1. Advantages of Decision Trees:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages-of-decision-trees">10.1.4.2. Disadvantages of Decision Trees:</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>