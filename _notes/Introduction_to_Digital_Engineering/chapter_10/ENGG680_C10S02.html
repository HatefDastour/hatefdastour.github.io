
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>10.2. Regression Trees &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_10/ENGG680_C10S02';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.3. Classification Trees" href="ENGG680_C10S03.html" />
    <link rel="prev" title="10.1. Fundamental Structure of Decision Trees" href="ENGG680_C10S01.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ENGG680_C09.html">9. An Introduction to Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S02.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S04.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C10.html">10. Tree-Based Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S03.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_12/ENGG680_C12.html">12. Introduction to Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_12/ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression Trees</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-auto-mpg-dataset">10.2.1. Example: Auto MPG dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-ln-mgp-as-a-function-of-horsepower">10.2.1.1. Modeling ln(MGP) as a function of Horsepower</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-ln-mgp-as-a-function-of-horsepower-and-cylinders">10.2.1.2. Modeling ln(MGP) as a function of Horsepower and Cylinders</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decisiontreeregressor-algorithm-in-scikit-learn">10.2.2. DecisionTreeRegressor algorithm in scikit-learn</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression-trees">
<h1><span class="section-number">10.2. </span>Regression Trees<a class="headerlink" href="#regression-trees" title="Link to this heading">#</a></h1>
<p>Regression trees are a type of decision tree that is specifically designed for solving <strong>regression problems</strong>. Unlike classification trees, which predict discrete class labels, regression trees predict <strong>continuous numerical values</strong>. Regression trees are a powerful tool for modeling and predicting numerical outcomes, making them a valuable asset in various fields such as finance, economics, engineering, and any domain where the target variable is continuous.</p>
<p>Here’s how regression trees work <span id="id1">[<a class="reference internal" href="../References.html#id22" title="L. Breiman. Classification and Regression Trees. CRC Press, 2017. ISBN 9781351460491. URL: https://books.google.ca/books?id=MGlQDwAAQBAJ.">Breiman, 2017</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Structure:</strong> A regression tree has a tree-like structure, with a <strong>root node</strong>, <strong>internal nodes</strong>, <strong>branches</strong>, and <strong>leaf nodes</strong>. Each internal node represents a splitting decision based on a feature, and each leaf node contains a predicted numerical value.</p></li>
<li><p><strong>Splitting Criteria:</strong> The main goal of a regression tree is to minimize the <strong>variance</strong> of the target variable within each leaf node. The algorithm chooses the feature and the split point that result in the most significant reduction in variance after the split. This reduction in variance is often measured using <strong>mean squared error (MSE)</strong>, which calculates the average squared difference between the predicted values and the actual values within each leaf node.</p></li>
<li><p><strong>Recursive Splitting:</strong> The construction of a regression tree involves a recursive process. The algorithm starts at the root node, selects the best feature and split point to minimize variance, and divides the data into subsets based on the chosen split. The process continues for each subset, creating child nodes and further splitting the data until a <strong>stopping criterion</strong> is met (e.g., a maximum tree depth or a minimum number of samples in a leaf node).</p></li>
<li><p><strong>Leaf Nodes and Predictions:</strong> Once the tree construction process is complete, the leaf nodes contain the predicted numerical values. When you have a new data point, you traverse the tree from the root node to a specific leaf node based on the values of the input features. The predicted value in that leaf node becomes the model’s prediction for the new data point.</p></li>
<li><p><strong>Pruning:</strong> Like other decision trees, regression trees can be prone to <strong>overfitting</strong>, where the model captures noise in the training data rather than the underlying patterns. Pruning is a technique used to prevent overfitting by removing or merging unnecessary branches from the tree. This leads to a more generalized and less complex model that performs better on new, unseen data.</p></li>
</ol>
<p>Regression trees are a valuable tool when you need to predict a continuous outcome based on a set of input features. However, it’s essential to keep in mind that decision trees, including regression trees, can be sensitive to the specific training data and may not always generalize well to new data. Techniques like pruning and using <strong>ensemble methods</strong> (e.g., Random Forests) can help improve the predictive performance of regression trees.</p>
<div class="alert alert-block alert-warning">
<p><strong>Regression Tree Algorithm</strong></p>
<ol class="arabic simple">
<li><p><strong>Start</strong></p></li>
<li><p>Define the problem as a <strong>regression task</strong>, where the goal is to predict a continuous numerical value based on a set of input features.</p></li>
<li><p>Collect and preprocess the dataset, such as handling missing values, outliers, or scaling the features.</p></li>
<li><p><strong>Construct a Regression Tree:</strong></p>
<ul class="simple">
<li><p>Define the <strong>root node</strong>, which contains the entire dataset.</p></li>
<li><p>Choose a feature to split the data based on <strong>variance reduction</strong>, which is the difference between the variance of the parent node and the weighted average variance of the child nodes.</p></li>
<li><p>Calculate <strong>mean squared error (MSE)</strong> for the split, which is the average squared difference between the predicted values and the actual values in each node.</p></li>
<li><p>Check if the <strong>stopping criterion</strong> is met (e.g., max depth or min samples). This criterion helps to prevent overfitting by limiting the growth of the tree.</p>
<ul>
<li><p>If yes, create a <strong>leaf node</strong> with the predicted value, which is the mean of the actual values in that node.</p></li>
<li><p>If no, create <strong>child nodes</strong> and repeat the process for each subset.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Pruning (Optional):</strong></p>
<ul class="simple">
<li><p>Evaluate the tree for overfitting, which means that the tree captures noise in the training data rather than the underlying patterns.</p></li>
<li><p>Identify unnecessary branches for removal, which are the branches that do not improve the performance of the tree on a validation set.</p></li>
<li><p>Remove or merge branches to create a <strong>simpler model</strong>, which is more generalizable and less complex.</p></li>
</ul>
</li>
<li><p><strong>End</strong></p></li>
</ol>
</div><section id="example-auto-mpg-dataset">
<h2><span class="section-number">10.2.1. </span>Example: Auto MPG dataset<a class="headerlink" href="#example-auto-mpg-dataset" title="Link to this heading">#</a></h2>
<p><font color='Blue'><b>Example</b></font>: Recall Auto MPG dataset from from the <a class="reference external" href="http://archive.ics.uci.edu/dataset/9/auto+mpg">UCI Machine Learning Repository</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip</span>

<span class="c1"># Define column names based on the dataset&#39;s description</span>
<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MPG&#39;</span><span class="p">,</span> <span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span> <span class="s1">&#39;Displacement&#39;</span><span class="p">,</span> <span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Acceleration&#39;</span><span class="p">,</span> <span class="s1">&#39;Model_Year&#39;</span><span class="p">,</span> <span class="s1">&#39;Origin&#39;</span><span class="p">,</span> <span class="s1">&#39;Car_Name&#39;</span><span class="p">]</span>

<span class="c1"># Read the dataset with column names, treating &#39;?&#39; as missing values, and remove rows with missing values</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;auto-mpg.data&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
                          <span class="n">na_values</span><span class="o">=</span><span class="s1">&#39;?&#39;</span><span class="p">,</span>
                          <span class="n">delim_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Remove the &#39;Car_Name&#39; column from the DataFrame</span>
<span class="n">auto_mpg_df</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Car_Name&#39;</span><span class="p">])</span>

<span class="c1"># Display the resulting DataFrame</span>
<span class="n">display</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="p">[[</span><span class="s1">&#39;MPG&#39;</span><span class="p">,</span> <span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;Cylinders&#39;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MPG</th>
      <th>Horsepower</th>
      <th>Cylinders</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>130.0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>165.0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>150.0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>150.0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>140.0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>387</th>
      <td>27.0</td>
      <td>86.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>388</th>
      <td>44.0</td>
      <td>52.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>389</th>
      <td>32.0</td>
      <td>84.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>390</th>
      <td>28.0</td>
      <td>79.0</td>
      <td>4</td>
    </tr>
    <tr>
      <th>391</th>
      <td>31.0</td>
      <td>82.0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>392 rows × 3 columns</p>
</div></div></div>
</div>
<section id="modeling-ln-mgp-as-a-function-of-horsepower">
<h3><span class="section-number">10.2.1.1. </span>Modeling ln(MGP) as a function of Horsepower<a class="headerlink" href="#modeling-ln-mgp-as-a-function-of-horsepower" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Prepare the input features and target variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="p">[[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">MPG</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>  <span class="c1"># Take the natural logarithm of MPG</span>

<span class="c1"># Create a Decision Tree Regressor with specific settings</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span>
                                 <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit the Decision Tree Regressor to the data</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a figure and axis for plotting the decision tree</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">2.75</span><span class="p">))</span>

<span class="c1"># Names of the features</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Visualize the decision tree</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                   <span class="n">impurity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Show impurity in nodes</span>
                   <span class="n">node_ids</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Show node IDs</span>
                   <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>           <span class="c1"># Fill nodes with colors</span>
                   <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>  <span class="c1"># Names of the features</span>
                   <span class="n">proportion</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>       <span class="c1"># Display class proportion</span>

<span class="c1"># Adjust the layout for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9afa226eae48948f77f1ac831b09f46c454a1bed24562a7c12ee060128e4ea84.png" src="../_images/9afa226eae48948f77f1ac831b09f46c454a1bed24562a7c12ee060128e4ea84.png" />
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><strong>square_error</strong>: Square error is a criterion used to measure the quality of a split in a decision tree regressor. It is equivalent to the mean squared error (MSE), which is the average squared difference between the predicted and actual target values in each node. It also represents the variance reduction achieved by the split, which is the goal of feature selection. Square error minimizes the L2 loss by using the mean target value as the prediction in each terminal node.</p></li>
<li><p><strong>samples</strong>: In a decision tree, “samples” refers to the number of training data points in a node. It indicates how many data points are involved in a split or a prediction at that node.</p></li>
<li><p><strong>value</strong>: The “value” is the predicted output for a new data point that falls into a node. It depends on the regression criterion used to build the tree. For example, if the criterion is square error, the “value” is the mean target value of the samples in that node. If the criterion is absolute error, the “value” is the median target value of the samples in that node.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- Horsepower &lt;= 97.50
|   |--- value: [3.32]
|--- Horsepower &gt;  97.50
|   |--- value: [2.81]
</pre></div>
</div>
</div>
</div>
<p>The split of the regression tree can be interpreted as follows:</p>
<ol class="arabic simple" start="0">
<li><p>The tree is divided based on the feature “Horsepower.”</p></li>
<li><p>If the “Horsepower” is less than or equal to 97.50, the predicted value is 3.32.</p></li>
<li><p>If the “Horsepower” is greater than 97.50, the predicted value is 2.81.</p></li>
</ol>
<p>This tree represents a binary decision structure where the regression model predicts different values depending on whether the “Horsepower” is above or below the threshold of 97.50.</p>
<p>To get <code class="docutils literal notranslate"><span class="pre">squared_error</span></code> and <code class="docutils literal notranslate"><span class="pre">value</span></code> for <code class="docutils literal notranslate"><span class="pre">node#1</span></code>, we have,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the thresholds</span>
<span class="n">_horsepower</span> <span class="o">=</span> <span class="mf">97.5</span>

<span class="c1"># Node #01: Predict and Evaluate Subset</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Node #1: Predict and Evaluate Subset&#39;</span><span class="p">)</span>
<span class="c1"># Import metrics from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Predict the target variable for data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">_horsepower</span><span class="p">])</span>

<span class="c1"># Get the indices of the data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">_horsepower</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Calculate the mean squared error (MSE) for the predicted values</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>

<span class="c1"># Print the squared error (MSE) with three decimal places</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;squared_error = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Calculate the percentage of samples in this node compared to the entire dataset</span>
<span class="n">sample_percentage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Print the percentage of samples in this node</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;samples = </span><span class="si">{</span><span class="n">sample_percentage</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="c1"># Calculate and print the mean value of the predicted target variable in this node</span>
<span class="n">mean_value_node1</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value = </span><span class="si">{</span><span class="n">mean_value_node1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Node #1: Predict and Evaluate Subset
squared_error = 0.051
samples = 56.9%
value = 3.317
</pre></div>
</div>
</div>
</div>
<p>To get <code class="docutils literal notranslate"><span class="pre">squared_error</span></code> and <code class="docutils literal notranslate"><span class="pre">value</span></code> for <code class="docutils literal notranslate"><span class="pre">node#2</span></code>, we have,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node #02: Predict and Evaluate Subset</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Node #2: Predict and Evaluate Subset&#39;</span><span class="p">)</span>
<span class="c1"># Import metrics from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Predict the target variable for data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">_horsepower</span><span class="p">])</span>

<span class="c1"># Get the indices of the data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">_horsepower</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Calculate the mean squared error (MSE) for the predicted values</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>

<span class="c1"># Print the squared error (MSE) with three decimal places</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;squared_error = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Calculate the percentage of samples in this node compared to the entire dataset</span>
<span class="n">sample_percentage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Print the percentage of samples in this node</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;samples = </span><span class="si">{</span><span class="n">sample_percentage</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="c1"># Calculate and print the mean value of the predicted target variable in this node</span>
<span class="n">mean_value_node2</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value = </span><span class="si">{</span><span class="n">mean_value_node2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Node #2: Predict and Evaluate Subset
squared_error = 0.054
samples = 43.1%
value = 2.810
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a figure and axis for the plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Define tick positions and limits for the plot</span>
<span class="n">xticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">_horsepower</span><span class="p">,</span> <span class="mi">230</span><span class="p">]</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">245</span><span class="p">]</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">]</span>

<span class="c1"># Create a scatter plot of data points</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
               <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;DodgerBlue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;Navy&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Set labels, ticks, and limits for the axes</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;ln(MPG)&#39;</span><span class="p">,</span>
           <span class="n">xticks</span><span class="o">=</span><span class="n">xticks</span><span class="p">,</span> <span class="n">yticks</span><span class="o">=</span><span class="n">yticks</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">)</span>

<span class="c1"># Add a vertical dashed line at x = 4.5</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
              <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;Black&#39;</span><span class="p">)</span>

<span class="c1"># Add horizontal dashed lines for mean values</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">mean_value_node1</span><span class="p">,</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
              <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;Red&#39;</span><span class="p">,</span>
              <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;ln(MPG) = </span><span class="si">{</span><span class="n">mean_value_node1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">mean_value_node2</span><span class="p">,</span> <span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;Green&#39;</span><span class="p">,</span>
              <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;ln(MPG) = </span><span class="si">{</span><span class="n">mean_value_node2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Annotations for the regions</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{R_1}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{R_2}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">160</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Fill regions with different colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;LimeGreen&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Add a grid to the plot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="c1"># Ensure a tight layout for the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/4dd69a6fc537357a0f4d1308e3be61c37c8ec85b811d36171d568bbfa08b4f88.png" src="../_images/4dd69a6fc537357a0f4d1308e3be61c37c8ec85b811d36171d568bbfa08b4f88.png" />
</div>
</div>
<p>The decision tree effectively divides the data into two distinct regions within the predictor space:</p>
<ol class="arabic simple">
<li><p><strong>Region 1 - <span class="math notranslate nohighlight">\(Hoursepower \leq 97.5\)</span> (<span class="math notranslate nohighlight">\(R_1\)</span>):</strong></p>
<ul class="simple">
<li><p>Mathematically expressed as: <span class="math notranslate nohighlight">\(R_1 = \{X~|~\text{Hoursepower} \leq 97.5\}\)</span></p></li>
</ul>
</li>
<li><p><strong>Region 2 - <span class="math notranslate nohighlight">\(Hoursepower &gt; 97.5\)</span> (<span class="math notranslate nohighlight">\(R_2\)</span>):</strong></p>
<ul class="simple">
<li><p>Mathematically expressed as: <span class="math notranslate nohighlight">\(R_2 = \{X~|~\text{Hoursepower} &gt; 97.5\}\)</span></p></li>
</ul>
</li>
</ol>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="../_images/dtr_fig01.png"><img alt="../_images/dtr_fig01.png" src="../_images/dtr_fig01.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.2 </span><span class="caption-text">A visual representation of the above Decision Tree Regressor.</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- 
import dtreeviz

viz_model = dtreeviz.model(reg, X, y, feature_names = feature_names, target_name= 'ln(MPG)')

v = viz_model.view()     # render as SVG into internal object 
v.show()                 # pop up window
--></section>
<section id="modeling-ln-mgp-as-a-function-of-horsepower-and-cylinders">
<h3><span class="section-number">10.2.1.2. </span>Modeling ln(MGP) as a function of Horsepower and Cylinders<a class="headerlink" href="#modeling-ln-mgp-as-a-function-of-horsepower-and-cylinders" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="c1"># Prepare the input features and target variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">auto_mpg_df</span><span class="p">[[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;Cylinders&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">MPG</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>  <span class="c1"># Take the natural logarithm of MPG</span>

<span class="c1"># Create a Decision Tree Regressor with specific settings</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Fit the Decision Tree Regressor to the data</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a figure and axis for plotting the decision tree</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="c1"># Names of the features</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Visualize the decision tree</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                   <span class="n">impurity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Show impurity in nodes</span>
                   <span class="n">node_ids</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Show node IDs</span>
                   <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>           <span class="c1"># Fill nodes with colors</span>
                   <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>  <span class="c1"># Names of the features</span>
                   <span class="n">proportion</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>       <span class="c1"># Display class proportion</span>

<span class="c1"># Adjust the layout for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3aebe4536e785df4835f5189460fe1a7677cd6c117c9953d16cfb13f979e8bb8.png" src="../_images/3aebe4536e785df4835f5189460fe1a7677cd6c117c9953d16cfb13f979e8bb8.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- Cylinders &lt;= 5.50
|   |--- value: [3.35]
|--- Cylinders &gt;  5.50
|   |--- Horsepower &lt;= 139.50
|   |   |--- value: [2.96]
|   |--- Horsepower &gt;  139.50
|   |   |--- value: [2.65]
</pre></div>
</div>
</div>
</div>
<p>The tree is organized as follows:</p>
<ol class="arabic simple" start="0">
<li><p>The top-level split is based on the feature “Cylinders” with a threshold of 5.50. This means that the dataset is initially divided into two groups: one where the number of cylinders is less than or equal to 5.50 and the other where it is greater than 5.50.</p></li>
<li><p>For the first group (Cylinders &lt;= 5.50), the model predicts a value of 3.35. This value represents the predicted outcome for instances in this subset.</p></li>
<li><p>For the second group (Cylinders &gt; 5.50), further splitting is performed based on the feature “Horsepower” with a threshold of 139.50. This results in two additional subsets.</p></li>
<li><p>In the subset where Horsepower is less than or equal to 139.50, the model predicts a value of 2.96.</p></li>
<li><p>In the subset where Horsepower is greater than 139.50, the model predicts a value of 2.65.</p></li>
</ol>
<p>This decision tree regressor is a structured representation of a predictive model used for regression tasks. It divides the data into different subsets based on specific feature values to make predictions about a target variable, typically a numeric value. In this case, the target variable is not explicitly mentioned, but it represents a numeric value that we are trying to predict.</p>
<p>For instance to get <code class="docutils literal notranslate"><span class="pre">squared_error</span></code> and <code class="docutils literal notranslate"><span class="pre">value</span></code> for <code class="docutils literal notranslate"><span class="pre">node#1</span></code>, we have,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define Thresholds</span>
<span class="n">_cylinder</span> <span class="o">=</span> <span class="mf">5.5</span>
<span class="n">_horsepower</span> <span class="o">=</span> <span class="mf">139.5</span>

<span class="c1"># Node #01: Predict and Evaluate Subset</span>

<span class="c1"># Import metrics from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="c1"># Predict the target variable for data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">_cylinder</span><span class="p">])</span>

<span class="c1"># Get the indices of the data points where &#39;Cylinders&#39; is less than 5.5</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">_cylinder</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Calculate the mean squared error (MSE) for the predicted values</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>

<span class="c1"># Print the squared error (MSE) with three decimal places</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;squared_error = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Calculate the percentage of samples in this node compared to the entire dataset</span>
<span class="n">sample_percentage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Print the percentage of samples in this node</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;samples = </span><span class="si">{</span><span class="n">sample_percentage</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

<span class="c1"># Calculate and print the mean value of the predicted target variable in this node</span>
<span class="n">mean_value</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value = </span><span class="si">{</span><span class="n">mean_value</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>squared_error = 0.039
samples = 52.6%
value = 3.351
</pre></div>
</div>
</div>
</div>
<p>Observe that,</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Create a figure with three subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Top subplot: Histogram of the natural logarithm of MPG</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">MPG</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ln(MPG)&#39;</span><span class="p">,</span>
           <span class="n">color</span> <span class="o">=</span><span class="s1">&#39;Violet&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\ln$(MPG)&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">])</span>

<span class="c1"># Middle subplot: Cylinders Histogram</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">Cylinders</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span>
           <span class="n">color</span><span class="o">=</span><span class="s1">&#39;RoyalBlue&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">250</span><span class="p">])</span>

<span class="c1"># Add a vertical line at x = 5.5 in the middle subplot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">5.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Cylinders = </span><span class="si">{</span><span class="n">_cylinder</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Bottom subplot: Horsepower Histogram</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="o">.</span><span class="n">Horsepower</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span>
           <span class="n">color</span> <span class="o">=</span><span class="s1">&#39;lightYellow&#39;</span><span class="p">,</span> <span class="n">ec</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>

<span class="c1"># Add a vertical line at x = 139.5 in the bottom subplot</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">139.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Horsepower = </span><span class="si">{</span><span class="n">_horsepower</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Add legends to the middle and bottom subplots</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">a</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/1f10dc32c42c6f7116475d9162d4aabea89388d15b8c8eec1886c94dfdd74919.png" src="../_images/1f10dc32c42c6f7116475d9162d4aabea89388d15b8c8eec1886c94dfdd74919.png" />
</div>
</div>
<p>The decision tree effectively divides the data into three distinct regions within the predictor space:</p>
<ol class="arabic simple">
<li><p><strong>Region 1 - Cylinders with 5 or Fewer Cylinders (<span class="math notranslate nohighlight">\(R_1\)</span>):</strong></p>
<ul class="simple">
<li><p>This region, denoted as <span class="math notranslate nohighlight">\(R_1\)</span>, comprises vehicles with four or fewer cylinders.</p></li>
<li><p>Mathematically expressed as: <span class="math notranslate nohighlight">\(R_1 = \{X~|~\text{Cylinders} \leq 5.5\}\)</span></p></li>
</ul>
</li>
<li><p><strong>Region 2 - Horsepower with Fewer Than 139.5 Horsepower (<span class="math notranslate nohighlight">\(R_2\)</span>):</strong></p>
<ul class="simple">
<li><p>The second region, labeled as <span class="math notranslate nohighlight">\(R_2\)</span>, includes vehicles with 6 or more cylinders and less than 139.5 horsepower.</p></li>
<li><p>Mathematically expressed as: <span class="math notranslate nohighlight">\(R_2 = \{X~|~\text{Cylinders} &gt; 5.5,~\text{Horsepower} \leq 139.5\}\)</span></p></li>
</ul>
</li>
<li><p><strong>Region 3 - Horsepower with At Least 139.5 Horsepower (<span class="math notranslate nohighlight">\(R_3\)</span>):</strong></p>
<ul class="simple">
<li><p>The third region, referred to as <span class="math notranslate nohighlight">\(R_3\)</span>, encompasses vehicles with five or more cylinders and at least 139.5 horsepower.</p></li>
<li><p>Mathematically expressed as: <span class="math notranslate nohighlight">\(R_3 = \{X~|~\text{Cylinders} &gt; 5.5,~\text{Horsepower} &gt; 139.5\}\)</span></p></li>
</ul>
</li>
</ol>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Define tick positions and limits for the plot</span>
<span class="n">xticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">_cylinder</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">_horsepower</span><span class="p">,</span> <span class="mi">230</span><span class="p">]</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">]</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">245</span><span class="p">]</span>

<span class="c1"># Scatter plot of data points</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">auto_mpg_df</span><span class="p">[</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">],</span> <span class="n">auto_mpg_df</span><span class="p">[</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">],</span>
               <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;Red&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;DarkRed&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Set labels, ticks, and limits for the axes</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Cylinders&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Horsepower&#39;</span><span class="p">,</span>
           <span class="n">xticks</span><span class="o">=</span><span class="n">xticks</span><span class="p">,</span> <span class="n">yticks</span><span class="o">=</span><span class="n">yticks</span><span class="p">,</span>
           <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">)</span>

<span class="c1"># Vertical dashed line at x = 4.5</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;Black&#39;</span><span class="p">)</span>

<span class="c1"># Horizontal dashed line at y = 139.5</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">yticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;Black&#39;</span><span class="p">)</span>

<span class="c1"># Annotations for the regions</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{R_1}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">117.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{R_2}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{R_3}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">180</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Fill regions with different colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;LimeGreen&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">yticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;aqua&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="n">xticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">yticks</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Add a grid to the plot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Ensure a tight layout for the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/b5e9de2723eccebefed93be312eb15b256093f0e05d18d013471cdff53b54c93.png" src="../_images/b5e9de2723eccebefed93be312eb15b256093f0e05d18d013471cdff53b54c93.png" />
</div>
</div>
<p>So, to recap, in the context of decision trees:</p>
<ul class="simple">
<li><p>The terminal nodes or leaves (<span class="math notranslate nohighlight">\(R_1\)</span>, <span class="math notranslate nohighlight">\(R_2\)</span>, <span class="math notranslate nohighlight">\(R_3\)</span>) correspond to the distinct regions where the observations ultimately fall.</p></li>
<li><p>Decision trees are typically drawn in a manner where these leaves appear at the bottom of the tree.</p></li>
<li><p>The points along the tree where the predictor space splits into segments are referred to as internal nodes.</p></li>
<li><p>The segments connecting these nodes are termed branches, akin to the branches of an actual tree.</p></li>
</ul>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="../_images/dtr_fig02.png"><img alt="../_images/dtr_fig02.png" src="../_images/dtr_fig02.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.3 </span><span class="caption-text">Visual representation of the above Decision Tree Regressor.</span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!--
import dtreeviz

viz_model = dtreeviz.model(reg, X, y, feature_names = feature_names, target_name= 'ln(MPG)')

v = viz_model.view()     # render as SVG into internal object 
v.show()                 # pop up window
--></section>
</section>
<section id="decisiontreeregressor-algorithm-in-scikit-learn">
<h2><span class="section-number">10.2.2. </span>DecisionTreeRegressor algorithm in scikit-learn<a class="headerlink" href="#decisiontreeregressor-algorithm-in-scikit-learn" title="Link to this heading">#</a></h2>
<p>The DecisionTreeRegressor algorithm in scikit-learn is a machine learning technique used for <strong>regression tasks</strong>, where the goal is to predict a continuous target variable based on input features. The algorithm constructs a <strong>decision tree</strong> to partition the feature space into regions and make predictions within each region. The partitioning is done by recursively splitting the data into subsets that optimize a certain <strong>criterion</strong>, which measures the quality of the splits. Three common criteria used for regression in DecisionTreeRegressor are <strong>Mean Squared Error (MSE)</strong>, <strong>Half Poisson Deviance</strong>, and <strong>Mean Absolute Error (MAE)</strong>.</p>
<p>Let’s go through the mathematical explanations of these criteria <span id="id2">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>:</p>
<ol class="arabic">
<li><p><strong>Mean Squared Error (MSE)</strong>:
The Mean Squared Error is a commonly used criterion for regression tasks. It aims to minimize the average squared difference between the predicted values (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) and the actual target values (<span class="math notranslate nohighlight">\(y\)</span>) within each node of the decision tree <span id="id3">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{y}_m\)</span>: The mean target value of the samples in node <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_m\)</span>: The number of samples in node <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Q_m\)</span>: The set of samples in node <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \bar{y}_m &amp;= \frac{1}{n_m} \sum_{y \in Q_m} y &amp;&amp;\text{(Mean target value)} \\
    H(Q_m) &amp;= \frac{1}{n_m} \sum_{y \in Q_m} (\hat{y} - y)^2 &amp;&amp; \text{(MSE)}
    \end{align*}\]</div>
</li>
<li><p><strong>Half Poisson Deviance</strong>:
The Half Poisson Deviance criterion is suitable for situations where the target variable represents counts or frequencies. It aims to minimize a measure of the difference between the observed (<span class="math notranslate nohighlight">\(y\)</span>) and predicted (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) values, scaled by a logarithmic term <span id="id4">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    H(Q_m) &amp;= \frac{1}{n_m} \sum_{y \in Q_m} \left( y \log\frac{y}{\hat{y}_m} - y + \hat{y}_m \right) \quad \text{(Half Poisson Deviance)}
    \end{align*}\]</div>
</li>
<li><p><strong>Mean Absolute Error (MAE)</strong>:
The Mean Absolute Error criterion aims to minimize the average absolute difference between the predicted values (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) and the actual target values (<span class="math notranslate nohighlight">\(y\)</span>) within each node <span id="id5">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(median(y)_m\)</span>: The median target value of the samples in node <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    median(y)_m &amp;= \underset{y \in Q_m}{\mathrm{median}}(y) &amp;&amp; \text{(Median target value)} \\
    H(Q_m) &amp;= \frac{1}{n_m} \sum_{y \in Q_m} |\hat{y} - y| &amp;&amp; \text{(MAE)}
    \end{align*}\]</div>
</li>
</ol>
<p>The DecisionTreeRegressor algorithm selects the split that minimizes the chosen criterion at each node during the tree-building process. This recursive partitioning continues until a <strong>stopping criterion</strong> is met, such as reaching a predefined maximum depth or the number of samples in a node falling below a certain threshold.</p>
<p>It’s worth noting that while these criteria provide guidelines for constructing the decision tree, the actual implementation may involve additional optimizations and techniques to handle overfitting and improve predictive performance. For more details, you can refer to the <span id="id6">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="admonition-decisiontreeregressor-algorithm admonition">
<p class="admonition-title">DecisionTreeRegressor Algorithm</p>
<ol class="arabic">
<li><p><strong>Initialization</strong>: Start with the entire dataset, <span class="math notranslate nohighlight">\(D\)</span>, which consists of input feature vectors <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^n\)</span> and corresponding target values <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, l\)</span>, where <span class="math notranslate nohighlight">\(l\)</span> is the number of samples.</p></li>
<li><p><strong>Node Splitting (Recursive Process)</strong>:</p>
<p>a. Check if a <strong>stopping criterion</strong> is met (e.g., maximum depth, minimum samples per leaf). This criterion helps to prevent overfitting by limiting the growth of the tree.</p>
<ul class="simple">
<li><p>If yes, create a <strong>terminal node</strong> with a prediction value. For example, if using MSE, set the prediction value as the mean of the target values in the node.</p></li>
<li><p>If no, proceed to the next step.</p></li>
</ul>
<p>b. For each feature <span class="math notranslate nohighlight">\(j\)</span> and each possible split value <span class="math notranslate nohighlight">\(s\)</span>:</p>
<ul>
<li><p>i. Split the dataset into two subsets, <span class="math notranslate nohighlight">\(D_{\text{left}}\)</span> and <span class="math notranslate nohighlight">\(D_{\text{right}}\)</span>, based on the condition <span class="math notranslate nohighlight">\(x_{ij} \leq s\)</span>, where <span class="math notranslate nohighlight">\(x_{ij}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th feature of the <span class="math notranslate nohighlight">\(i\)</span>-th sample.</p></li>
<li><p>ii. Calculate the <strong>quality of the split</strong> using one of the criteria:</p>
<ul>
<li><p><strong>Mean Squared Error (MSE)</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bb3d4f25-488c-4f74-9447-cf77606ec7e6">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-bb3d4f25-488c-4f74-9447-cf77606ec7e6" title="Permalink to this equation">#</a></span>\[\begin{equation}H(D) = \frac{1}{l} \sum_{i=1}^{l} (\hat{y}_i - y_i)^2\end{equation}\]</div>
</li>
<li><p><strong>Half Poisson Deviance</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5a2904c7-d637-47c9-85b7-391fca0783d5">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-5a2904c7-d637-47c9-85b7-391fca0783d5" title="Permalink to this equation">#</a></span>\[\begin{equation}H(D) = \frac{1}{l} \sum_{i=1}^{l} \left( y_i \log\frac{y_i}{\hat{y}_i} - y_i + \hat{y}_i \right)\end{equation}\]</div>
</li>
<li><p><strong>Mean Absolute Error (MAE)</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-79fa4ecd-8752-48cb-b713-0e27db7bb8cc">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-79fa4ecd-8752-48cb-b713-0e27db7bb8cc" title="Permalink to this equation">#</a></span>\[\begin{equation}H(D) = \frac{1}{l} \sum_{i=1}^{l} |\hat{y}_i - y_i|\end{equation}\]</div>
</li>
</ul>
</li>
<li><p>iii. Choose the split that <strong>minimizes <span class="math notranslate nohighlight">\(H(D)\)</span></strong>, i.e., the split that results in the lowest criterion value.</p></li>
</ul>
<p>c. Create a <strong>decision node</strong> with the chosen split feature <span class="math notranslate nohighlight">\(j\)</span> and split value <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>d. Recursively apply the algorithm to the subsets <span class="math notranslate nohighlight">\(D_{\text{left}}\)</span> and <span class="math notranslate nohighlight">\(D_{\text{right}}\)</span> to grow the tree.</p>
</li>
<li><p><strong>Prediction</strong>: To make a prediction for a new input vector <span class="math notranslate nohighlight">\(x\)</span>, traverse the decision tree from the root node to a leaf node based on the feature values of <span class="math notranslate nohighlight">\(x\)</span>. The prediction value for <span class="math notranslate nohighlight">\(x\)</span> is the value associated with the leaf node.</p></li>
</ol>
</div>
<p><font color='Blue'><b>Example:</b></font>
In this example, a decision tree is employed to model and fit a cosine curve amidst noisy observations. The decision tree captures local linear relationships, effectively approximating the underlying cosine curve while considering the noisy nature of the data. This scenario showcases how decision trees can adapt to data patterns and perform regression tasks. The presented example is an adapted version of an example available in scikit-learn’s documentation
<span id="id7">[<a class="reference internal" href="../References.html#id53" title="scikit-learn Developers. Scikit-learn user guide. https://scikit-learn.org/stable/user_guide.html, 2023. [Online; accessed 01-August-2023].">scikit-learn Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is an modified version of an example by Sklearn:</span>
<span class="c1"># https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html</span>

<span class="c1"># Import the necessary modules and libraries</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="c1"># Create a random dataset</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y</span><span class="p">[::</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>

<span class="c1"># Predict</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="c1"># Create decision tree regressors with different max depths</span>
<span class="n">max_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;DodgerBlue&quot;</span><span class="p">,</span> <span class="s2">&quot;Violet&quot;</span><span class="p">,</span> <span class="s2">&quot;OrangeRed&quot;</span><span class="p">,</span> <span class="s2">&quot;Green&quot;</span><span class="p">]</span>

<span class="c1"># Plot the results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">max_depths</span><span class="p">,</span> <span class="n">colors</span><span class="p">)):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
    
    <span class="n">regr</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;max_depth = </span><span class="si">{</span><span class="n">depth</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/77a2f9109ddfd9dabf18c9e6f7e0a0e5c3ff274fa703dc38b6968d1caeb37e99.png" src="../_images/77a2f9109ddfd9dabf18c9e6f7e0a0e5c3ff274fa703dc38b6968d1caeb37e99.png" />
</div>
</div>
<p>In this context, the <code class="docutils literal notranslate"><span class="pre">max_depths</span></code> list specifies different values for the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> parameter of the DecisionTreeRegressor. Each value determines the maximum depth to which the decision tree can grow during its construction.</p>
<ol class="arabic simple">
<li><p><strong>max_depth = 1</strong>:
With a maximum depth of 1, the decision tree will only have a single split. This means that the tree will create one node that best splits the data into two regions based on some feature threshold. The decision boundary will be a horizontal or vertical line, which will not capture the complexity of the underlying cosine curve. This model will likely underfit the data.</p></li>
<li><p><strong>max_depth = 2</strong>:
Increasing the maximum depth to 2 allows the decision tree to make one additional split. The tree will now have a root node and two child nodes. This might allow the decision tree to capture some basic curvature of the cosine curve, but it’s still limited in terms of complexity.</p></li>
<li><p><strong>max_depth = 3</strong>:
With a maximum depth of 3, the decision tree can create three levels of nodes. This extra depth allows the tree to capture more of the curvature in the cosine curve, but it might still miss some finer details. The model might start to capture the initial rise and the subsequent fall of the cosine curve.</p></li>
<li><p><strong>max_depth = 4</strong>:
Increasing the maximum depth to 4 adds another level of complexity to the decision tree. The tree can now create four levels of nodes. This could allow the tree to better approximate the shape of the cosine curve, capturing more of its oscillations and details.</p></li>
</ol>
<!-- gridspec_kw={'height_ratios': [1, 2, 3]} --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="k">def</span> <span class="nf">_DTR_TreePlot</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a decision tree plot based on the specified parameters.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - max_depth (int): Maximum depth of the decision tree.</span>
<span class="sd">    - figsize (tuple): Figure size (width, height).</span>

<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">regr</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">regr</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                       <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">impurity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">node_ids</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>    
                       <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span>
                       <span class="n">proportion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decision Tree with max_depth = </span><span class="si">{</span><span class="n">max_depth</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">export_text</span><span class="p">(</span><span class="n">regr</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>max_depth = 1</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_DTR_TreePlot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- x &lt;= 1.36
|   |--- value: [0.74]
|--- x &gt;  1.36
|   |--- value: [-0.57]
</pre></div>
</div>
<img alt="../_images/0113d19b73f5fc286f5f72af71999a1d1fe8d4e65b16ba8b51dc7e114d5bcb33.png" src="../_images/0113d19b73f5fc286f5f72af71999a1d1fe8d4e65b16ba8b51dc7e114d5bcb33.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dtreeviz</span>

<span class="n">regr</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">viz_model</span> <span class="o">=</span> <span class="n">dtreeviz</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">regr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">target_name</span><span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">viz_model</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>     <span class="c1"># render as SVG into internal object </span>
<span class="n">v</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="../_images/dtr_fig03.png"><img alt="../_images/dtr_fig03.png" src="../_images/dtr_fig03.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10.4 </span><span class="caption-text">A visual representation of the above Decision Tree Regressor.</span><a class="headerlink" href="#id10" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!--
import dtreeviz

regr = DecisionTreeRegressor(max_depth=1).fit(X, y)

viz_model = dtreeviz.model(regr, X, y, feature_names = 'x', target_name= 'y')

v = viz_model.view()     # render as SVG into internal object 
v.show()
--><p>Here is how to read the tree:</p>
<ul class="simple">
<li><p><strong>node #0</strong> shows the root node, which is the starting point of the tree. It splits the data based on the value of x. If x is less than or equal to 1.36, then the data goes to the left branch. If x is greater than 1.36, then the data goes to the right branch.</p></li>
<li><p><strong>node #1</strong> shows the left terminal node, which is the end point of the tree. It has a predicted value of 0.74, which means that the average value of the target variable for the data that reaches this node is 0.74.</p></li>
<li><p><strong>node #2</strong> shows the right terminal node, which has a predicted value of -0.57, which means that the average value of the target variable for the data that reaches this node is -0.57.</p></li>
</ul>
<p><strong>max_depth = 2</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_DTR_TreePlot</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|--- x &lt;= 1.36
|   |--- x &lt;= 0.05
|   |   |--- value: [-0.15]
|   |--- x &gt;  0.05
|   |   |--- value: [0.78]
|--- x &gt;  1.36
|   |--- x &lt;= 4.28
|   |   |--- value: [-0.71]
|   |--- x &gt;  4.28
|   |   |--- value: [-0.01]
</pre></div>
</div>
<img alt="../_images/2d26b91f1997d22ee8970bc12e9180d85c591b86219b50d6980c1d59a9bd98f9.png" src="../_images/2d26b91f1997d22ee8970bc12e9180d85c591b86219b50d6980c1d59a9bd98f9.png" />
</div>
</div>
<p>The following figure was generated utilizing  <a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>.</p>
<figure class="align-center" id="id11">
<img alt="../_images/dtr_fig04.png" src="../_images/dtr_fig04.png" />
<figcaption>
<p><span class="caption-number">Fig. 10.5 </span><span class="caption-text">A visual representation of the above Decision Tree Regressor.</span><a class="headerlink" href="#id11" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!--
import dtreeviz

regr = DecisionTreeRegressor(max_depth=2).fit(X, y)

viz_model = dtreeviz.model(regr, X, y, feature_names = 'x', target_name= 'y')

v = viz_model.view()     # render as SVG into internal object 
v.show()
--><p>Here is how to read the tree:</p>
<ul class="simple">
<li><p><strong>node #0</strong> shows the root node, which is the starting point of the tree. It splits the data based on the value of x. If x is less than or equal to 1.36, then the data goes to the left branch. If x is greater than 1.36, then the data goes to the right branch.</p></li>
<li><p><strong>node #1</strong> shows the left internal node, which is a node that splits the data further based on the value of x. If x is less than or equal to 0.05, then the data goes to the left branch. If x is greater than 0.05, then the data goes to the right branch.</p></li>
<li><p><strong>node #2</strong> shows the left terminal node, which has a predicted value of -0.15, which means that the average value of the target variable for the data that reaches this node is -0.15.</p></li>
<li><p><strong>node #3</strong> shows the right terminal node, which has a predicted value of 0.78, which means that the average value of the target variable for the data that reaches this node is 0.78.</p></li>
<li><p><strong>node #4</strong> shows the right internal node, which is a node that splits the data further based on the value of x. If x is less than or equal to 4.28, then the data goes to the left branch. If x is greater than 4.28, then the data goes to the right branch.</p></li>
<li><p><strong>node #5</strong> shows the left terminal node, which has a predicted value of -0.71, which means that the average value of the target variable for the data that reaches this node is -0.71.</p></li>
<li><p><strong>node #6</strong> shows the right terminal node, which has a predicted value of -0.01, which means that the average value of the target variable for the data that reaches this node is -0.01.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C10S01.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10.1. </span>Fundamental Structure of Decision Trees</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C10S03.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10.3. </span>Classification Trees</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-auto-mpg-dataset">10.2.1. Example: Auto MPG dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-ln-mgp-as-a-function-of-horsepower">10.2.1.1. Modeling ln(MGP) as a function of Horsepower</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modeling-ln-mgp-as-a-function-of-horsepower-and-cylinders">10.2.1.2. Modeling ln(MGP) as a function of Horsepower and Cylinders</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decisiontreeregressor-algorithm-in-scikit-learn">10.2.2. DecisionTreeRegressor algorithm in scikit-learn</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>