

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12.2. Fundamentals of Neural Networks &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_12/ENGG_680_C12S2';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.3. Tensors and Variables" href="ENGG_680_C12S3.html" />
    <link rel="prev" title="12.1. Understanding Deep Learning" href="ENGG_680_C12S1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">12. Introduction to Deep Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S3.html">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fundamentals of Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-their-role-in-computational-modeling">12.2.1. Neurons and Their Role in Computational Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-elements-of-an-artificial-neuron">12.2.1.1. Key Elements of an Artificial Neuron:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-and-learning-in-neural-networks">12.2.1.2. Computation and Learning in Neural Networks:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">12.2.2. Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-activation-functions">12.2.2.1. The Role of Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-step-function">12.2.2.2. Binary Step function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">12.2.2.3. ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">12.2.2.4. Leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">12.2.2.5. Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent">12.2.2.6. Tanh (Hyperbolic Tangent)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">12.2.2.7. Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-linear-unit-elu">12.2.2.8. Exponential Linear Unit (ELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward-and-backpropagation-concepts">12.2.3. Feedforward and Backpropagation Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">12.2.3.1. Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">12.2.3.2. Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions-in-deep-learning">12.2.4. Loss Functions in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter">12.2.4.1. Why Loss Functions Matter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-functions-help-the-model">12.2.4.2. How Loss Functions Help the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-loss-functions">12.2.4.3. Popular Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-optimization-in-deep-learning">12.2.5. Training and Optimization in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-process-unraveling-the-steps">12.2.5.1. The Training Process: Unraveling the Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrating-the-process-a-flowchart-perspective">12.2.5.2. Illustrating the Process: A Flowchart Perspective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.2.6. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="fundamentals-of-neural-networks">
<h1><span class="section-number">12.2. </span>Fundamentals of Neural Networks<a class="headerlink" href="#fundamentals-of-neural-networks" title="Permalink to this headline">#</a></h1>
<div class="section" id="neurons-and-their-role-in-computational-modeling">
<h2><span class="section-number">12.2.1. </span>Neurons and Their Role in Computational Modeling<a class="headerlink" href="#neurons-and-their-role-in-computational-modeling" title="Permalink to this headline">#</a></h2>
<p>Neurons are like the building blocks of artificial neural networks (ANNs), and they’re inspired by how our brains work. Just like how our own brain cells process and share information, artificial neurons do a similar job. When it comes to ANNs, neurons act as the workers that take in data and turn it into something useful <span id="id1">[<a class="reference internal" href="../References.html#id113" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id115" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id114" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>. Think of these neurons as tiny processors. They take the information given to them, adjust its importance, do some calculations, and then give out a result. This process happens layer by layer, and it’s really good at finding patterns in data. This is why neural networks are great at things like recognizing pictures and understanding language. They’re like the brains behind some really smart computer programs.</p>
<div class="section" id="key-elements-of-an-artificial-neuron">
<h3><span class="section-number">12.2.1.1. </span>Key Elements of an Artificial Neuron:<a class="headerlink" href="#key-elements-of-an-artificial-neuron" title="Permalink to this headline">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Inputs:</strong> Neurons receive data from the preceding layer or directly from the input dataset. Each piece of input data represents various attributes, features, or facets of the information under scrutiny.</p></li>
<li><p><strong>Weights:</strong> Think of weights as significance labels for the input data. These weights determine the importance of each piece of information in shaping the neuron’s output. During the learning process, these weights are adjusted to optimize the neuron’s predictive abilities.</p></li>
<li><p><strong>Bias:</strong> A bias serves as an additional factor that enhances the neuron’s adaptability and flexibility. Visualize it as an adjustable knob fine-tuning the neuron’s behavior, enabling it to better comprehend intricate relationships within the data.</p></li>
<li><p><strong>Summation:</strong> All the weighted inputs and the bias are aggregated, much like assembling puzzle pieces to reveal a larger picture. This summation, executed in a specific manner, illustrates how all the individual input components interrelate.</p></li>
<li><p><strong>Activation Function:</strong> The aggregated result from the previous step undergoes an activation function. This function determines the neuron’s response pattern, akin to its “disposition.” The disposition can range from positive to negative or fall somewhere in between, playing a crucial role in processing diverse data patterns.</p></li>
<li><p><strong>Output:</strong> The activation function yields the neuron’s “decision” or interpretation of the data. It articulates, “I believe this is the meaning of the data.” This decision can then propagate to other neurons, progressively constructing a deeper comprehension of the information. Alternatively, if it belongs to the final layer, it can constitute the network’s ultimate conclusion or prediction.</p></li>
</ol>
<p>In essence, an artificial neuron operates as a miniature decision-maker, ingesting information, evaluating its importance, and delivering its perspective on the matter. When numerous neurons collaborate, they exhibit remarkable capabilities, such as pattern recognition and deciphering intricate data structures <span id="id2">[<a class="reference internal" href="../References.html#id113" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id115" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id114" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>.</p>
<!-- ### Components of an Artificial Neuron:

1. **Inputs:** Neurons take in information from the layer before them or directly from the data being worked with. These bits of information can stand for different characteristics, qualities, or parts of the data under consideration.

2. **Weights:** Think of weights as labels that say how important each piece of information is. These labels matter because they decide how much each input contributes to what the neuron ultimately says. When the network learns, it adjusts these labels to make the best predictions.

3. **Bias:** A bias is like a little extra something added to the mix. It helps the neuron adapt and be more flexible. Imagine it as an adjustment knob that fine-tunes the neuron's behavior, making it better at understanding complicated connections.

4. **Summation:** Now, all the weighted inputs and the bias get added up together. It's kind of like combining puzzle pieces to get a bigger picture. This addition, done in a certain way, shows how all the input parts fit together.

5. **Activation Function:** The summed-up result from before gets sent through an activation function. This function gives the neuron a certain way of reacting. It's like the neuron's "mood." This mood can be positive, negative, or somewhere in between, and it's important for dealing with all sorts of data patterns.

6. **Output:** The activation function gives us the neuron's "decision." It says, "Okay, I think this is what the data means." This decision can then pass on to other neurons, step by step, building up a bigger understanding of the data. Or, if it's the final layer, it can be the network's ultimate answer or prediction.

In a nutshell, an artificial neuron is like a little decision-maker that takes in information, weighs it up, and gives its own take on what's going on. When lots of these neurons work together, they can do some pretty clever stuff, like recognizing patterns and making sense out of complex data {cite:p}`mehlig2021machine,ye2022geometry,aggarwal2023neural`. -->
<div class="figure align-center" id="id18">
<a class="reference internal image-reference" href="../_images/Elements_ANN.jpg"><img alt="../_images/Elements_ANN.jpg" src="../_images/Elements_ANN.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.1 </span><span class="caption-text">The main usage of the Activation Function is to convert the aggregated and weighted input from a node into an output value that can be propagated to the next hidden layer or used as an output.</span><a class="headerlink" href="#id18" title="Permalink to this image">#</a></p>
</div>
<p><font color='Blue'><b>Example:</b></font> A linear transformation stands as a pivotal building block, forming the basis upon which more complex architectures are constructed. At its core, a linear transformation takes input data and performs a weighted combination of its features, ultimately yielding an output. This process is encapsulated by the mathematical equation <span class="math notranslate nohighlight">\(\hat{y} = W.X + B\)</span>, where <span class="math notranslate nohighlight">\(\hat{y}\)</span> signifies the resulting output <span id="id3">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<p><strong>Components of a Linear Transformation:</strong></p>
<ol class="arabic simple">
<li><p><strong>Output (<span class="math notranslate nohighlight">\(z\)</span>):</strong>
The output <span class="math notranslate nohighlight">\(z\)</span> represents the result of the linear transformation. It encapsulates the synthesized information that arises from the combination of input features according to learned weights and biases. While this transformation might seem simplistic, its elegance lies in its foundation—a direct linear relationship between inputs and outputs <span id="id4">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Weight Matrix (<span class="math notranslate nohighlight">\(W\)</span>)</strong>
The weight matrix <span class="math notranslate nohighlight">\(W\)</span> is the crux of the linear transformation. This matrix encompasses the learned weights assigned to each feature in the input data. In essence, it determines the influence that each feature wields in the process of generating the output. The arrangement of these weights in the matrix permits a holistic view of the interplay between input features and their corresponding importance <span id="id5">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Input Matrix (<span class="math notranslate nohighlight">\(X\)</span>):</strong>
The input matrix <span class="math notranslate nohighlight">\(X\)</span> forms the bedrock of the transformation. Comprising rows of input samples and columns of distinct features, <span class="math notranslate nohighlight">\(X\)</span> encapsulates the raw information that the model processes. Each column of the matrix corresponds to a specific feature, and each row corresponds to an individual sample. This representation allows the linear transformation to operate simultaneously on multiple samples, making it a vectorized and efficient computation <span id="id6">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Bias Matrix (<span class="math notranslate nohighlight">\(B\)</span>):</strong>
While the weight matrix captures the interactions between input features, the bias matrix <span class="math notranslate nohighlight">\(B\)</span> introduces an additional degree of freedom. It is a vector that is added element-wise to the weighted sum of features before producing the final output. This term permits the model to account for inherent offsets or baseline values that are not directly captured by the weighted features alone <span id="id7">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
</ol>
</div>
<div class="section" id="computation-and-learning-in-neural-networks">
<h3><span class="section-number">12.2.1.2. </span>Computation and Learning in Neural Networks:<a class="headerlink" href="#computation-and-learning-in-neural-networks" title="Permalink to this headline">#</a></h3>
<p>The inner workings of an artificial neuron closely emulate the information processing mechanisms of biological neurons. Yet, the true potential of artificial neurons is unlocked when they are orchestrated into layered configurations, interconnected to establish a neural network. Within this network, neurons cooperate in a concerted effort to acquire the ability to convert raw input data into coherent and meaningful representations. This collective learning process equips the network to make predictions and execute various tasks founded upon the acquired knowledge of patterns <span id="id8">[<a class="reference internal" href="../References.html#id113" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id115" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id114" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>.</p>
</div>
</div>
<div class="section" id="activation-functions">
<h2><span class="section-number">12.2.2. </span>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<p>As we journey from simple linear models to the complex landscapes of deep learning, activation functions step forward as vital agents of transformation. Their essence lies in imparting neural networks with the power to capture non-linear phenomena, opening pathways for representing intricate data relationships. By transcending the boundaries of linear operations, activation functions unlock the network’s potential to tackle tasks that involve subtle patterns and complex interactions.</p>
<!-- Activation functions introduce non-linearity to the neural network, enabling it to capture complex patterns in data. Some common activation functions include: -->
<div class="section" id="the-role-of-activation-functions">
<h3><span class="section-number">12.2.2.1. </span>The Role of Activation Functions<a class="headerlink" href="#the-role-of-activation-functions" title="Permalink to this headline">#</a></h3>
<p>Deep learning activation functions are mathematical functions that are applied to the output of a layer of a neural network. They determine how the output of the layer is transformed into the input for the next layer, or the final prediction of the model. Activation functions are essential for deep learning because they introduce non-linearity into the network, which allows it to learn complex patterns from data <span id="id9">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<div class="figure align-center" id="id19">
<a class="reference internal image-reference" href="../_images/Elements_ANN.jpg"><img alt="../_images/Elements_ANN.jpg" src="../_images/Elements_ANN.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.2 </span><span class="caption-text">The main usage of the Activation Function is to convert the aggregated and weighted input from a node into an output value that can be propagated to the next hidden layer or used as an output.</span><a class="headerlink" href="#id19" title="Permalink to this image">#</a></p>
</div>
<p>There are many types of activation functions that have different properties and effects on the network performance. Some of the most common activation functions are:</p>
</div>
<div class="section" id="binary-step-function">
<h3><span class="section-number">12.2.2.2. </span>Binary Step function<a class="headerlink" href="#binary-step-function" title="Permalink to this headline">#</a></h3>
<p>The Binary Step function is a simple activation function used in binary classification tasks, where the output of a neuron or a model needs to be binary, often representing classes like 0 and 1. Mathematically, the Binary Step function is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-78ec668d-9347-4301-9861-e0394e15e0ae">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-78ec668d-9347-4301-9861-e0394e15e0ae" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
0, &amp; \text{if } x &lt; 0 \\
1, &amp; \text{if } x \geq 0
\end{cases}
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x\)</span> is the input to the function. The Binary Step function returns 0 if the input is less than 0, and it returns 1 if the input is greater than or equal to 0. It essentially provides a threshold-based output where any positive input is mapped to the value 1 and any negative input is mapped to the value 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Define the Binary Step function</span>
<span class="k">def</span> <span class="nf">binary_step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Binary Step function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">binary_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Binary Step&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Binary Step Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7b22f260cb70af58a4db229918306fea1771c5bc131e84a5fd85a9e2db6e2366.png" src="../_images/7b22f260cb70af58a4db229918306fea1771c5bc131e84a5fd85a9e2db6e2366.png" />
</div>
</div>
</div>
<div class="section" id="relu-rectified-linear-unit">
<h3><span class="section-number">12.2.2.3. </span>ReLU (Rectified Linear Unit)<a class="headerlink" href="#relu-rectified-linear-unit" title="Permalink to this headline">#</a></h3>
<p>The Rectified Linear Unit, commonly known as ReLU, is a piecewise linear function that outputs zero for negative input values and directly outputs the input if it’s positive. This activation function is straightforward, computationally efficient, and widely favored for many hidden layers within deep learning architectures. ReLU plays a crucial role in alleviating the vanishing gradient problem, which occurs when gradients become too small to effectively update weights. However, it’s worth noting the potential challenge of the dying ReLU problem, where certain neurons can become inactive due to zero gradients. Mathematically, the ReLU function is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-166d6bdc-3afa-4414-8d00-a0a2a8965d8c">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-166d6bdc-3afa-4414-8d00-a0a2a8965d8c" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \max(0, x)\end{equation}\]</div>
<p>ReLU’s appeal lies in its simplicity, efficiency, and its capability to address gradient-related issues, making it a cornerstone of modern neural networks <span id="id10">[<a class="reference internal" href="../References.html#id108" title="Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: a comprehensive survey and benchmark. Neurocomputing, 2022.">Dubey <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id106" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 1026–1034. 2015.">He <em>et al.</em>, 2015</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the ReLU function</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the ReLU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;ReLU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a263b9ce637971b936dc3ca9153668a0a84d1b2c0f0a4132f8971683966f8cb3.png" src="../_images/a263b9ce637971b936dc3ca9153668a0a84d1b2c0f0a4132f8971683966f8cb3.png" />
</div>
</div>
</div>
<div class="section" id="leaky-relu">
<h3><span class="section-number">12.2.2.4. </span>Leaky ReLU<a class="headerlink" href="#leaky-relu" title="Permalink to this headline">#</a></h3>
<p>The Leaky ReLU (Rectified Linear Unit) is an activation function commonly used in neural networks that addresses some of the limitations of the standard ReLU function. Mathematically, the Leaky ReLU function is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3cab519b-2428-4966-a5ce-1086ed65ce17">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-3cab519b-2428-4966-a5ce-1086ed65ce17" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
x, &amp; \text{if } x &gt; 0 \\
\alpha x, &amp; \text{if } x \leq 0
\end{cases}
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a small positive constant (usually a very small value like 0.01). The purpose of the Leaky ReLU is to allow a small gradient to flow through when the input is less than or equal to zero, addressing the “dying ReLU” problem where some neurons might become inactive during training due to always outputting zero for negative inputs.</p>
<p>In mathematical terms, the Leaky ReLU function behaves as follows:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(x &gt; 0\)</span>, the function behaves like the identity function, meaning it returns the input value itself.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(x \leq 0\)</span>, the function scales the input by the small positive constant <span class="math notranslate nohighlight">\(\alpha\)</span>, resulting in a linear function with a slope of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Leaky ReLU function</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Leaky ReLU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Leaky ReLU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Leaky ReLU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fec202d00a816288507ce71b86a379a2daa6196a696b1b8e5dd403d316eaf45e.png" src="../_images/fec202d00a816288507ce71b86a379a2daa6196a696b1b8e5dd403d316eaf45e.png" />
</div>
</div>
</div>
<div class="section" id="sigmoid-function">
<h3><span class="section-number">12.2.2.5. </span>Sigmoid Function<a class="headerlink" href="#sigmoid-function" title="Permalink to this headline">#</a></h3>
<p>The Sigmoid function offers a smooth curve that maps inputs onto a range between 0 and 1. This characteristic makes it suitable for tasks that require output probabilities. The sigmoid function is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5a5741e-8039-4a7c-9fa3-6aea5b09f56a">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-f5a5741e-8039-4a7c-9fa3-6aea5b09f56a" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \frac{1}{1 + e^{-x}}\end{equation}\]</div>
<p>Though widely used in earlier neural networks, the sigmoid function has somewhat fallen out of favor due to its susceptibility to vanishing gradients during training <span id="id11">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Sigmoid function</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Sigmoid function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sigmoid&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Sigmoid Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c2b1a733b30c19e903bd784ad028a3e3a97c3e3c2e8a6a3c2ac3c4d0c244f291.png" src="../_images/c2b1a733b30c19e903bd784ad028a3e3a97c3e3c2e8a6a3c2ac3c4d0c244f291.png" />
</div>
</div>
</div>
<div class="section" id="tanh-hyperbolic-tangent">
<h3><span class="section-number">12.2.2.6. </span>Tanh (Hyperbolic Tangent)<a class="headerlink" href="#tanh-hyperbolic-tangent" title="Permalink to this headline">#</a></h3>
<p>The Hyperbolic Tangent, often denoted as Tanh, is an activation function that ranges between -1 and 1. Similar to the sigmoid function, Tanh offers a smooth curve, and its centered nature around zero aids in mitigating the vanishing gradient problem. The Tanh function is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e0b1b36-9d5f-4a54-a034-252cb77c4f7e">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-4e0b1b36-9d5f-4a54-a034-252cb77c4f7e" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\end{equation}\]</div>
<p>Tanh strikes a balance between the extremities of the sigmoid function and is thus considered an improvement in many scenarios
<span id="id12">[<a class="reference internal" href="../References.html#id100" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id104" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Hyperbolic Tangent (Tanh) function</span>
<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Tanh function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Tanh&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Tanh Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/597911de4ea7f681f6efda3a4cf7708bff56893ee137de5e47d77ba5ac87e98b.png" src="../_images/597911de4ea7f681f6efda3a4cf7708bff56893ee137de5e47d77ba5ac87e98b.png" />
</div>
</div>
</div>
<div class="section" id="softmax">
<h3><span class="section-number">12.2.2.7. </span>Softmax<a class="headerlink" href="#softmax" title="Permalink to this headline">#</a></h3>
<p>The Softmax function computes the probability <span class="math notranslate nohighlight">\( p_i \)</span> for each class <span class="math notranslate nohighlight">\( i \)</span> as follows <span id="id13">[<a class="reference internal" href="../References.html#id92" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc9e636b-6cbd-4247-940a-f7430ea2feb0">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-bc9e636b-6cbd-4247-940a-f7430ea2feb0" title="Permalink to this equation">#</a></span>\[\begin{equation}
p_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\end{equation}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( e \)</span> represents the base of the natural logarithm (Euler’s number).</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> is the raw score or logit associated with the <span class="math notranslate nohighlight">\( i \)</span>-th class.</p></li>
<li><p>The denominator <span class="math notranslate nohighlight">\( \sum_{j=1}^{n} e^{x_j} \)</span> ensures that the probabilities sum up to 1, creating a valid probability distribution.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Softmax function</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Subtracting the maximum value for numerical stability</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Softmax function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Softmax&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Softmax Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ef3e67398359edbc6a17aad56f8094c6267235b75b4504fc06b1aa1051dde7e2.png" src="../_images/ef3e67398359edbc6a17aad56f8094c6267235b75b4504fc06b1aa1051dde7e2.png" />
</div>
</div>
</div>
<div class="section" id="exponential-linear-unit-elu">
<h3><span class="section-number">12.2.2.8. </span>Exponential Linear Unit (ELU)<a class="headerlink" href="#exponential-linear-unit-elu" title="Permalink to this headline">#</a></h3>
<p>The Exponential Linear Unit (ELU) is an activation function used in neural networks that overcomes some of the limitations of the ReLU function, such as the “dying ReLU” problem. Mathematically, the ELU function is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-532336ea-df1e-49ae-89f3-daccf814257d">
<span class="eqno">(12.7)<a class="headerlink" href="#equation-532336ea-df1e-49ae-89f3-daccf814257d" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
x, &amp; \text{if } x &gt; 0 \\
\alpha (e^x - 1), &amp; \text{if } x \leq 0
\end{cases}
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a positive constant that controls the function’s behavior for negative inputs. The ELU function offers a smooth transition between the positive and negative domains, and the introduction of the exponential term for negative inputs prevents neurons from becoming inactive during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the ELU function</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the ELU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ELU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;ELU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/56c6822c541fe26ac84dec8192bdda01b07cb6f20bc46f824bf97fb36e097d22.png" src="../_images/56c6822c541fe26ac84dec8192bdda01b07cb6f20bc46f824bf97fb36e097d22.png" />
</div>
</div>
</div>
</div>
<div class="section" id="feedforward-and-backpropagation-concepts">
<h2><span class="section-number">12.2.3. </span>Feedforward and Backpropagation Concepts<a class="headerlink" href="#feedforward-and-backpropagation-concepts" title="Permalink to this headline">#</a></h2>
<p>Neural networks, like the intricate workings of our own brains, have a central role in modern machine learning and artificial intelligence. Their ability to process information and learn hinges on the coordinated interplay of two key steps: feedforward and backpropagation.</p>
<div class="section" id="feedforward">
<h3><span class="section-number">12.2.3.1. </span>Feedforward<a class="headerlink" href="#feedforward" title="Permalink to this headline">#</a></h3>
<p><strong>Feedforward</strong>, much like how our senses process information, takes the first step in a neural network’s journey. This process involves these important stages <span id="id14">[<a class="reference internal" href="../References.html#id116" title="Okan Erkaymaz. Resilient back-propagation approach in small-world feed-forward neural network topology based on newman–watts algorithm. Neural Computing and Applications, 32(20):16279–16289, 2020.">Erkaymaz, 2020</a>, <a class="reference internal" href="../References.html#id118" title="M. Maynard. Neural Networks: Introduction to Artificial Neurons, Backpropagation and Multilayer Feedforward Neural Networks with Real-World Applications. Advanced Data Analytics. Independently Published, 2020. ISBN 9798642783528. URL: https://books.google.ca/books?id=82F5zQEACAAJ.">Maynard, 2020</a>, <a class="reference internal" href="../References.html#id117" title="Xinghuo Yu, M Onder Efe, and Okyay Kaynak. A general backpropagation algorithm for feedforward neural networks learning. IEEE transactions on neural networks, 13(1):251–254, 2002.">Yu <em>et al.</em>, 2002</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Input Layer:</strong> At the beginning, the input data resides in the input layer. Each neuron here represents a specific feature or characteristic of the data.</p></li>
<li><p><strong>Hidden Layers:</strong> The real magic unfolds as the data travels through hidden layers. Neurons in these layers interpret inputs from the prior layer, assigning them weights and passing them through an activation function. This transformation introduces non-linear changes, allowing the network to decipher intricate data relationships.</p></li>
<li><p><strong>Output Layer:</strong> The journey concludes in the output layer, where transformed data takes its final shape—prediction. In tasks like classification, each neuron embodies a class, with the one activated most indicating the network’s decision.</p></li>
</ol>
<p>This dance between feedforward and backpropagation embodies the essence of neural network learning, propelling machines to understand, learn, and predict much like our own minds do.</p>
<div class="figure align-center" id="id20">
<a class="reference internal image-reference" href="../_images/DeepNeuralNetwork.png"><img alt="../_images/DeepNeuralNetwork.png" src="../_images/DeepNeuralNetwork.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.3 </span><span class="caption-text">Input, hidden, and output layers form the bedrock of neural networks. Image courtesy of IBM <span id="id15">[<a class="reference internal" href="../References.html#id119" title="IBM. What is a neural network? https://www.ibm.com/topics/neural-networks, 2023. [Online; accessed 01-August-2023].">IBM, 2023</a>]</span>.</span><a class="headerlink" href="#id20" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="backpropagation">
<h3><span class="section-number">12.2.3.2. </span>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">#</a></h3>
<p><strong>Backpropagation</strong>, akin to a mentor guiding its protege, seamlessly takes over from the feedforward process. It masterfully orchestrates a nuanced sequence of weight and bias adjustments, finely tuning them to minimize prediction errors as the network evolves over countless iterations. The intricacies of backpropagation follow this choreography <span id="id16">[<a class="reference internal" href="../References.html#id116" title="Okan Erkaymaz. Resilient back-propagation approach in small-world feed-forward neural network topology based on newman–watts algorithm. Neural Computing and Applications, 32(20):16279–16289, 2020.">Erkaymaz, 2020</a>, <a class="reference internal" href="../References.html#id118" title="M. Maynard. Neural Networks: Introduction to Artificial Neurons, Backpropagation and Multilayer Feedforward Neural Networks with Real-World Applications. Advanced Data Analytics. Independently Published, 2020. ISBN 9798642783528. URL: https://books.google.ca/books?id=82F5zQEACAAJ.">Maynard, 2020</a>, <a class="reference internal" href="../References.html#id117" title="Xinghuo Yu, M Onder Efe, and Okyay Kaynak. A general backpropagation algorithm for feedforward neural networks learning. IEEE transactions on neural networks, 13(1):251–254, 2002.">Yu <em>et al.</em>, 2002</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Loss Calculation:</strong> The stage is set with the calculation of the prediction error, measured by a chosen loss function—whether it’s the Mean Squared Error for regression or the Cross-Entropy for classification. This error acts as a mirror, reflecting the divergence between the network’s predictions and the truth nestled within the target values.</p></li>
<li><p><strong>Gradient Descent:</strong> Taking inspiration from the gradient of a hill, backpropagation embarks on a journey of gradient descent to diminish this error. The gradient—the compass guiding change—is calculated, pointing out the direction in which the network’s parameters (weights and biases) need to shift to reduce the error.</p></li>
<li><p><strong>Weight Updates:</strong> Guided by the gradient’s wisdom, the network undertakes the delicate task of adjusting its parameters. With a touch as gentle as a sculptor refining a masterpiece, weights and biases shift in harmony with the gradient’s tune, each update nudging the network closer to precision.</p></li>
<li><p><strong>Iterative Process:</strong> The ballet of feedforward and backpropagation unfolds iteratively, much like the graceful cycle of a phoenix’s rebirth. With each epoch, the network’s predictive prowess soars to new heights. The loss gracefully dwindles, a testament to the network’s unwavering dedication to unraveling the intricate truths woven within the data.</p></li>
</ol>
<p>Through this symbiotic synergy of feedforward and backpropagation, the neural network metamorphoses. It learns to decipher the intricate tapestry of patterns and relationships woven into the data. This journey of discovery endows the network with the remarkable capacity to navigate uncharted territories, performing tasks spanning from classification and regression to the complexities of image recognition and natural language processing. As it evolves, the network transcends its initial state, becoming a vessel of artificial intelligence, tirelessly deciphering the symphony of data that unfolds before it.</p>
</div>
</div>
<div class="section" id="loss-functions-in-deep-learning">
<h2><span class="section-number">12.2.4. </span>Loss Functions in Deep Learning<a class="headerlink" href="#loss-functions-in-deep-learning" title="Permalink to this headline">#</a></h2>
<p>In the world of deep learning, where models learn from data to make predictions, loss functions are like helpful guides. These tools are basically math tools that help us figure out how well a neural network is doing. They do this by comparing what the network predicted with what’s actually true. This comparison gives the network important feedback that it uses to get better at its job.</p>
<div class="section" id="why-loss-functions-matter">
<h3><span class="section-number">12.2.4.1. </span>Why Loss Functions Matter<a class="headerlink" href="#why-loss-functions-matter" title="Permalink to this headline">#</a></h3>
<p>Deep learning is all about making models that can work well on new, unseen data. A loss function is a way to measure how much the model’s guesses differ from the real answers. By measuring this difference, the loss function gives us a number that tells us how wrong the model is. The model can then adjust itself based on this number to make its mistakes smaller. So, think of a loss function as a sort of guide that helps the model move in the right direction.</p>
</div>
<div class="section" id="how-loss-functions-help-the-model">
<h3><span class="section-number">12.2.4.2. </span>How Loss Functions Help the Model<a class="headerlink" href="#how-loss-functions-help-the-model" title="Permalink to this headline">#</a></h3>
<p>Loss functions help models improve over time. During training, the model keeps changing its inner settings (like weights and biases) based on the feedback from the loss function. It’s like the model is trying to minimize the loss. This process helps the model get better at its job by making its guesses more accurate. As the model trains and tweaks itself, it gets closer and closer to making predictions that match reality.</p>
<p>As we move forward, we’ll look at how activation functions, loss functions, and ways to optimize work together. These different parts all team up to help the model learn and grow, from its simple beginnings to its complex abilities in deep learning.</p>
</div>
<div class="section" id="popular-loss-functions">
<h3><span class="section-number">12.2.4.3. </span>Popular Loss Functions<a class="headerlink" href="#popular-loss-functions" title="Permalink to this headline">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Mean Squared Error (MSE)</strong>: This loss function is applied to regression tasks, where the aim is to predict continuous numerical values. It calculates the average of the squared differences between the predicted and actual values. Mathematically, it’s expressed as <span id="id17">[<a class="reference internal" href="../References.html#id112" title="M.P. Deisenroth, A.A. Faisal, and C.S. Ong. Mathematics for Machine Learning. Cambridge University Press, 2020. ISBN 9781108470049. URL: https://books.google.ca/books?id=pFjPDwAAQBAJ.">Deisenroth <em>et al.</em>, 2020</a>, <a class="reference internal" href="../References.html#id53" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>, <a class="reference internal" href="../References.html#id110" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-172a8d14-43c2-4fa7-b759-9491fedec329">
<span class="eqno">(12.8)<a class="headerlink" href="#equation-172a8d14-43c2-4fa7-b759-9491fedec329" title="Permalink to this equation">#</a></span>\[\begin{equation}MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> represents the actual value for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Binary Cross-Entropy Loss</strong>: Used for binary classification tasks. It quantifies the difference between predicted probabilities and the actual binary labels. Mathematically, it’s given by:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-2b27292b-f7e4-46a6-99a3-242ff37ad35a">
<span class="eqno">(12.9)<a class="headerlink" href="#equation-2b27292b-f7e4-46a6-99a3-242ff37ad35a" title="Permalink to this equation">#</a></span>\[\begin{equation}BCE = -\frac{1}{n} \sum_{i=1}^{n} \left(y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i)\right),\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the actual binary label for the <span class="math notranslate nohighlight">\(i\)</span>th instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> is the predicted probability of the positive class for the <span class="math notranslate nohighlight">\(i\)</span>th instance.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Categorical Cross-Entropy Loss</strong>: Suited for multi-class classification problems. It measures the difference between predicted class probabilities and the true class labels. The formula is:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-fddadab9-5812-457e-a8d8-f2639e8ee940">
<span class="eqno">(12.10)<a class="headerlink" href="#equation-fddadab9-5812-457e-a8d8-f2639e8ee940" title="Permalink to this equation">#</a></span>\[\begin{equation}CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \cdot \log(p_{ij}),\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{ij}\)</span> is an indicator (0 or 1) if instance <span class="math notranslate nohighlight">\(i\)</span> belongs to class <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{ij}\)</span> is the predicted probability of instance <span class="math notranslate nohighlight">\(i\)</span> belonging to class <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<ol class="arabic" start="4">
<li><p><strong>Sparse Categorical Cross-Entropy Loss</strong>: This variation of categorical cross-entropy is useful when true labels are provided as integers, instead of one-hot encoded vectors. It still measures the difference between predicted and actual class probabilities. Mathematically, it’s defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-fe364a11-276e-4521-905e-f046ab9da8d3">
<span class="eqno">(12.11)<a class="headerlink" href="#equation-fe364a11-276e-4521-905e-f046ab9da8d3" title="Permalink to this equation">#</a></span>\[\begin{equation} SCCE = -\frac{1}{n} \sum_{i=1}^{n} \log(p_{i, y_i}) \end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\( p_{i, y_i} \)</span> is the predicted probability of instance <span class="math notranslate nohighlight">\( i \)</span> belonging to its true class <span class="math notranslate nohighlight">\( y_i \)</span>.</p></li>
</ul>
</li>
<li><p><strong>Hinge Loss (SVM Loss)</strong>: Primarily used for support vector machines (SVMs) and classification tasks in neural networks. It encourages the correct class scores to be higher than the incorrect class scores by a predefined margin. The hinge loss can be defined as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-ee9d6273-c853-42f9-b556-47d314e1605c">
<span class="eqno">(12.12)<a class="headerlink" href="#equation-ee9d6273-c853-42f9-b556-47d314e1605c" title="Permalink to this equation">#</a></span>\[\begin{equation}HingeLoss = \max(0, 1 - y_i \cdot \hat{y}_i),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual label (either -1 or 1), and <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted score for class <span class="math notranslate nohighlight">\(i\)</span>.</p>
<ol class="arabic simple" start="6">
<li><p><strong>Huber Loss</strong>: A robust loss function applied in regression tasks. It offers a balance between mean squared error and mean absolute error, which makes it less sensitive to outliers. Mathematically, it’s defined as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-c5a5e8fe-da66-44f2-b019-dd85cbf6af17">
<span class="eqno">(12.13)<a class="headerlink" href="#equation-c5a5e8fe-da66-44f2-b019-dd85cbf6af17" title="Permalink to this equation">#</a></span>\[\begin{equation}HuberLoss = \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2 &amp; \text{if } |y_i - \hat{y}_i| \leq \delta \\ \delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2 &amp; \text{otherwise} \end{cases}\end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\delta\)</span> is a hyperparameter that determines the point where the loss transitions from quadratic to linear behavior.</p>
<ol class="arabic simple" start="7">
<li><p><strong>Kullback-Leibler Divergence (KL Divergence)</strong>: Commonly used in generative models like variational autoencoders. It measures the difference between two probability distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-bae04821-b16b-4544-ba6e-19336dd9f052">
<span class="eqno">(12.14)<a class="headerlink" href="#equation-bae04821-b16b-4544-ba6e-19336dd9f052" title="Permalink to this equation">#</a></span>\[\begin{equation}KLD(P \parallel Q) = \sum_{i} P(i) \cdot \log\left(\frac{P(i)}{Q(i)}\right)\end{equation}\]</div>
<ol class="arabic simple" start="8">
<li><p><strong>Triplet Loss</strong>: This loss is used in tasks like face recognition and similarity learning. It ensures that embeddings of similar examples are closer in the embedding space than embeddings of dissimilar examples. Mathematically, for a triplet of examples <span class="math notranslate nohighlight">\((a, p, n)\)</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-801415dd-0412-4fa9-a408-a792bc106e41">
<span class="eqno">(12.15)<a class="headerlink" href="#equation-801415dd-0412-4fa9-a408-a792bc106e41" title="Permalink to this equation">#</a></span>\[\begin{equation}TripletLoss = \max(d(a, p) - d(a, n) + \alpha, 0),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(d(x, y)\)</span> measures the distance between embeddings <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a margin parameter.</p>
<ol class="arabic simple" start="9">
<li><p><strong>Focal Loss</strong>: Particularly useful for imbalanced classification problems. It down-weights the contribution of well-classified examples and emphasizes hard, misclassified examples. It’s formulated as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-0576c185-db2a-4a94-ba8e-1a5007c9860d">
<span class="eqno">(12.16)<a class="headerlink" href="#equation-0576c185-db2a-4a94-ba8e-1a5007c9860d" title="Permalink to this equation">#</a></span>\[\begin{equation}FocalLoss = -\frac{1}{n} \sum_{i=1}^{n} \left((1 - p_i)^{\gamma} \cdot y_i \cdot \log(p_i)\right),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i\)</span> is the predicted probability for class <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(y_i\)</span> is the actual label, and <span class="math notranslate nohighlight">\(\gamma\)</span> is a focusing parameter.</p>
<ol class="arabic simple" start="10">
<li><p><strong>Dice Loss</strong>: Commonly used in medical image segmentation tasks. It’s based on the Sørensen-Dice coefficient and calculates the overlap between predicted and true segmentation masks.</p></li>
<li><p><strong>Wasserstein Loss (Wasserstein GAN Loss)</strong>: Utilized in Wasserstein GANs (Generative Adversarial Networks) to provide a more stable training signal for the generator. It’s associated with the Wasserstein distance, which measures the difference between two probability distributions.</p></li>
</ol>
</div>
</div>
<div class="section" id="training-and-optimization-in-deep-learning">
<h2><span class="section-number">12.2.5. </span>Training and Optimization in Deep Learning<a class="headerlink" href="#training-and-optimization-in-deep-learning" title="Permalink to this headline">#</a></h2>
<p>In the world of deep learning, the process of training a neural network involves iteratively adjusting its parameters to learn from data and improve its performance on a specific task. This intricate process is at the heart of building effective neural networks that can make accurate predictions, classifications, and generate meaningful outputs. In this section, we will explore the key steps involved in training and optimizing neural networks.</p>
<div class="section" id="the-training-process-unraveling-the-steps">
<h3><span class="section-number">12.2.5.1. </span>The Training Process: Unraveling the Steps<a class="headerlink" href="#the-training-process-unraveling-the-steps" title="Permalink to this headline">#</a></h3>
<p>The journey of training a neural network encompasses several fundamental stages, each contributing to the network’s ability to comprehend complex patterns within the data it encounters. These stages can be outlined as follows:</p>
<ol class="arabic simple">
<li><p><strong>Data Collection &amp; Preprocessing:</strong> The process commences with the collection of relevant and appropriately labeled data. High-quality training data is essential for ensuring the network generalizes well to unseen examples. Data preprocessing, which involves tasks like normalization, scaling, and handling missing values, prepares the data for effective learning.</p></li>
<li><p><strong>Model Architecture Design:</strong> Crafting an appropriate model architecture is crucial. This step involves defining the structure of the neural network, including the number and arrangement of layers, the type of activation functions used, and how information flows through the network.</p></li>
<li><p><strong>Initialize Model Parameters:</strong> Before training begins, the model’s parameters (weights and biases) are initialized. Proper initialization can set the stage for faster and more stable convergence during optimization.</p></li>
<li><p><strong>Forward Pass:</strong> During the forward pass, input data is fed into the network, and the model generates predictions. This pass involves applying the defined transformations and activation functions layer by layer.</p></li>
<li><p><strong>Calculate Loss:</strong> The generated predictions are compared with the ground truth labels using a predefined loss function. This function quantifies the discrepancy between predictions and actual values.</p></li>
<li><p><strong>Backpropagation:</strong> Backpropagation involves calculating the gradients of the loss with respect to the model’s parameters. This step enables the network to understand how changes in each parameter affect the overall loss.</p></li>
<li><p><strong>Update Model Parameters:</strong> Using the calculated gradients, optimization algorithms like gradient descent are employed to update the model’s parameters. These updates aim to minimize the loss and steer the network towards better performance.</p></li>
<li><p><strong>Check Convergence:</strong> The training process is performed iteratively. After each iteration (epoch), the convergence of the model is evaluated by monitoring the changes in the loss and other performance metrics. If the model shows signs of convergence or meets predefined criteria, training can stop.</p></li>
</ol>
</div>
<div class="section" id="illustrating-the-process-a-flowchart-perspective">
<h3><span class="section-number">12.2.5.2. </span>Illustrating the Process: A Flowchart Perspective<a class="headerlink" href="#illustrating-the-process-a-flowchart-perspective" title="Permalink to this headline">#</a></h3>
<p>To better visualize the intricacies of training and optimization, consider the following flowchart that outlines the sequential steps involved:</p>
<div class="figure align-center" id="id21">
<a class="reference internal image-reference" href="../_images/DL_Flowchart.jpg"><img alt="../_images/DL_Flowchart.jpg" src="../_images/DL_Flowchart.jpg" style="width: 550px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.4 </span><span class="caption-text">The Structure of Training and Optimization in Deep Learning.</span><a class="headerlink" href="#id21" title="Permalink to this image">#</a></p>
</div>
<p>In this flowchart:</p>
<ol class="arabic simple">
<li><p><strong>Data Collection &amp; Preprocessing:</strong> Gathering and preparing the training data.</p></li>
<li><p><strong>Model Architecture Design:</strong> Defining the structure of the neural network.</p></li>
<li><p><strong>Initialize Model Parameters:</strong> Setting initial weights and biases.</p></li>
<li><p><strong>Forward Pass:</strong> Propagating input data through the network to generate predictions.</p></li>
<li><p><strong>Calculate Loss:</strong> Comparing predictions to actual values and computing a loss/error metric.</p></li>
<li><p><strong>Backpropagation:</strong> Calculating gradients of the loss with respect to model parameters.</p></li>
<li><p><strong>Update Model Parameters:</strong> Adjusting weights and biases using optimization algorithms (e.g., gradient descent).</p></li>
<li><p><strong>Check Convergence:</strong> Evaluating whether the training process has converged or met a stopping criterion.</p></li>
<li><p><strong>Repeat Backpropagation &amp; Update:</strong> If convergence is not reached, repeat the backpropagation and parameter update steps.</p></li>
<li><p><strong>Converged:</strong> Once convergence is achieved, the model is considered trained.</p></li>
<li><p><strong>Trained Model:</strong> The neural network with optimized parameters.</p></li>
<li><p><strong>End:</strong> End of the training process.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Please note that deep learning training involves many details and variations, so this flowchart is a simplified representation. Depending on the context and level of detail you want to provide, you can expand each step and include more specific information.</p>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2><span class="section-number">12.2.6. </span>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>In wrapping up our exploration of linear models within the realm of deep learning, we find ourselves at the gateway of a larger universe. Linear models offer us a starting point, shedding light on how neural networks make predictions based on data. Yet, as we peer further, it’s clear that life’s complexities demand more than just linear thinking.</p>
<p>Stepping into the domain of deep learning, we encounter three crucial components: activation functions, loss functions, and optimization techniques. Activation functions add twists and turns to our neural pathways, enabling networks to decipher the intricate patterns hidden in data. Loss functions act as guiding lights, revealing how much our predictions deviate from the truth. Optimization techniques act like sculptors, refining the network’s inner workings and molding it into a more proficient predictor.</p>
<p>However, our journey is not confined to linear routes. Deep learning ushers us into the realm of more intricate architectures. These architectures, filled with layers of computations, transform and amplify the information they receive. This expansion leads us to exciting domains – from understanding visual data with convolutional networks to conversing with computers through recurrent networks. Moreover, we delve into creative territories, crafting new content with generative models and mastering the art of making informed decisions with reinforcement learning.</p>
<p>As we conclude this phase of our exploration, it’s important to recognize that our current understanding is but a stepping stone. The landscape of deep learning continues to expand, offering us a plethora of challenges and opportunities. By grasping the essentials, we equip ourselves to navigate this ever-evolving field with confidence and curiosity.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C12S1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.1. </span>Understanding Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C12S3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12.3. </span>Tensors and Variables</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-their-role-in-computational-modeling">12.2.1. Neurons and Their Role in Computational Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-elements-of-an-artificial-neuron">12.2.1.1. Key Elements of an Artificial Neuron:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-and-learning-in-neural-networks">12.2.1.2. Computation and Learning in Neural Networks:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">12.2.2. Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-activation-functions">12.2.2.1. The Role of Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-step-function">12.2.2.2. Binary Step function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">12.2.2.3. ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">12.2.2.4. Leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">12.2.2.5. Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent">12.2.2.6. Tanh (Hyperbolic Tangent)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">12.2.2.7. Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-linear-unit-elu">12.2.2.8. Exponential Linear Unit (ELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward-and-backpropagation-concepts">12.2.3. Feedforward and Backpropagation Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">12.2.3.1. Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">12.2.3.2. Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions-in-deep-learning">12.2.4. Loss Functions in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter">12.2.4.1. Why Loss Functions Matter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-functions-help-the-model">12.2.4.2. How Loss Functions Help the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-loss-functions">12.2.4.3. Popular Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-optimization-in-deep-learning">12.2.5. Training and Optimization in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-process-unraveling-the-steps">12.2.5.1. The Training Process: Unraveling the Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrating-the-process-a-flowchart-perspective">12.2.5.2. Illustrating the Process: A Flowchart Perspective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.2.6. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>