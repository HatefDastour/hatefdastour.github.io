

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>12.9. Deep Learning Architectures &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_12/ENGG_680_C12S09';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.10. Image classification with TensorFlow" href="ENGG_680_C12S10.html" />
    <link rel="prev" title="12.8. Multilayer Perceptron (MLP)" href="ENGG_680_C12S08.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">12. Introduction to Deep Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_2.html">13.2. Sample Questions 2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-between-tensorflow-and-pytorch">12.9.1. Choice between TensorFlow and PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-cnn">12.9.2. Convolutional Neural Network (CNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rescaling-layer">12.9.2.1. Rescaling Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers">12.9.2.2. Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">12.9.2.3. Pooling Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-layers">12.9.2.4. Dropout Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten-layers">12.9.2.5. Flatten Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-layers-fully-connected-layers">12.9.2.6. Dense Layers (Fully Connected Layers)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-architectures">
<h1><span class="section-number">12.9. </span>Deep Learning Architectures<a class="headerlink" href="#deep-learning-architectures" title="Permalink to this heading">#</a></h1>
<p>Deep learning architectures are the models or frameworks of artificial neural networks that consist of multiple layers of interconnected units or neurons that can learn from data and perform various tasks, such as image processing, speech recognition, natural language processing, and more. Deep learning architectures can be classified into different categories based on their structure, function, and learning method. Some of the most common and popular categories of deep learning architectures are:</p>
<ul class="simple">
<li><p><strong>Autoencoder (AE)</strong>: These are neural networks that use unsupervised learning to learn a compressed representation of the input data, such as images or text. AE networks consist of an encoder and a decoder, where the encoder reduces the dimensionality of the input data, and the decoder reconstructs the original data from the compressed representation. AE networks can reconstruct the original data from the compressed representation, and are widely used for data compression, denoising, and feature extraction tasks <span id="id1">[<a class="reference internal" href="../References.html#id122" title="Q. Liang, W. Wang, X. Liu, Z. Na, X. Li, and B. Zhang. Communications, Signal Processing, and Systems: Proceedings of the 9th International Conference on Communications, Signal Processing, and Systems. Lecture Notes in Electrical Engineering. Springer Nature Singapore, 2021. ISBN 9789811584114. URL: https://books.google.ca/books?id=RDYyEAAAQBAJ.">Liang <em>et al.</em>, 2021</a>, <a class="reference internal" href="../References.html#id165" title="K.B. Prakash, R. Kannan, S.A. Alexander, and G.R. Kanagachidambaresan. Advanced Deep Learning for Engineers and Scientists: A Practical Approach. EAI/Springer Innovations in Communication and Computing. Springer International Publishing, 2021. ISBN 9783030665180. URL: https://books.google.ca/books?id=MDsNzgEACAAJ.">Prakash <em>et al.</em>, 2021</a>]</span>.</p></li>
<li><p><strong>Convolutional neural networks (CNNs)</strong>: These are neural networks that use convolutional layers to extract features from the input data, such as images or text. Convolutional layers apply filters or kernels to the input data, and produce feature maps that capture the local patterns or structures of the data. CNNs are especially good at recognizing patterns and objects in images, and are widely used for computer vision tasks <span id="id2">[<a class="reference internal" href="../References.html#id107" title="S. Khan, H. Rahmani, S.A.A. Shah, and M. Bennamoun. A Guide to Convolutional Neural Networks for Computer Vision. Synthesis Lectures on Computer Vision. Springer International Publishing, 2022. ISBN 9783031018213. URL: https://books.google.ca/books?id=54ZyEAAAQBAJ.">Khan <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id148" title="L. Mou and Z. Jin. Tree-Based Convolutional Neural Networks: Principles and Applications. SpringerBriefs in Computer Science. Springer Nature Singapore, 2018. ISBN 9789811318702. URL: https://books.google.ca/books?id=eQlxDwAAQBAJ.">Mou and Jin, 2018</a>]</span>.</p></li>
<li><p><strong>Deep Belief Network (DBN)</strong>: These are neural networks that use unsupervised learning to learn a hierarchical representation of the input data, such as images or text. DBN networks consist of multiple layers of RBMs, where the output of one RBM is the input of the next RBM. DBN networks can learn complex and abstract features of the data, and are widely used for data generation, recognition, and classification tasks <span id="id3">[<a class="reference internal" href="../References.html#id91" title="Yuming Hua, Junhai Guo, and Hua Zhao. Deep belief networks and deep learning. In Proceedings of 2015 International Conference on Intelligent Computing and Internet of Things, 1–4. IEEE, 2015.">Hua <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id128" title="N. Lopes and B. Ribeiro. Machine Learning for Adaptive Many-Core Machines - A Practical Approach. Studies in Big Data. Springer International Publishing, 2014. ISBN 9783319069388. URL: https://books.google.ca/books?id=oTkqBAAAQBAJ.">Lopes and Ribeiro, 2014</a>]</span>.</p></li>
<li><p><strong>Deep Stacking Network (DSN)</strong>: These are neural networks that use supervised learning to learn a hierarchical representation of the input data, such as images or text. DSN networks consist of multiple layers of FNNs, where the output of one FNN is the input of the next FNN. DSN networks can learn complex and nonlinear functions of the data, and are widely used for regression and classification tasks <span id="id4">[<a class="reference internal" href="../References.html#id43" title="Li Deng, Xiaodong He, and Jianfeng Gao. Deep stacking networks for information retrieval. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, volume, 3153-3157. 2013. doi:10.1109/ICASSP.2013.6638239.">Deng <em>et al.</em>, 2013</a>, <a class="reference internal" href="../References.html#id72" title="A. Fred, M. De Marsico, and M. Figueiredo. Pattern Recognition: Applications and Methods: 4th International Conference, ICPRAM 2015, Lisbon, Portugal, January 10-12, 2015, Revised Selected Papers. Lecture Notes in Computer Science. Springer International Publishing, 2016. ISBN 9783319276779. URL: https://books.google.ca/books?id=Bm9aCwAAQBAJ.">Fred <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id128" title="N. Lopes and B. Ribeiro. Machine Learning for Adaptive Many-Core Machines - A Practical Approach. Studies in Big Data. Springer International Publishing, 2014. ISBN 9783319069388. URL: https://books.google.ca/books?id=oTkqBAAAQBAJ.">Lopes and Ribeiro, 2014</a>]</span>.</p></li>
<li><p><strong>Feedforward neural networks (FNNs)</strong>: These are the simplest and most basic type of neural networks that have a linear or sequential structure. They consist of an input layer, an output layer, and one or more hidden layers in between. The information flows from the input layer to the output layer in a forward direction, without any feedback loops or cycles. FNNs can learn to approximate any function, and are widely used for regression and classification tasks <span id="id5">[<a class="reference internal" href="../References.html#id14" title="C.M. Bishop. Neural Networks for Pattern Recognition. Advanced Texts in Econometrics. Clarendon Press, 1995. ISBN 9780198538646. URL: https://books.google.ca/books?id=T0S0BgAAQBAJ.">Bishop, 1995</a>, <a class="reference internal" href="../References.html#id70" title="T.L. Fine. Feedforward Neural Network Methodology. Information Science and Statistics. Springer New York, 2006. ISBN 9780387226491. URL: https://books.google.ca/books?id=s-PlBwAAQBAJ.">Fine, 2006</a>, <a class="reference internal" href="../References.html#id106" title="N. Ketkar and J. Moolayil. Deep Learning with Python: Learn Best Practices of Deep Learning Models with PyTorch. Apress, 2021. ISBN 9781484253632. URL: https://books.google.ca/books?id=FHJ0yAEACAAJ.">Ketkar and Moolayil, 2021</a>]</span>.</p></li>
<li><p><strong>Generative adversarial network (GAN)</strong>: These are neural networks that use unsupervised learning to generate realistic and diverse data samples, such as images or text. GAN networks consist of two competing networks: a generator and a discriminator, where the generator tries to fool the discriminator by producing fake data, and the discriminator tries to distinguish between real and fake data. GAN networks can create novel and high-quality data samples, and are widely used for image synthesis, style transfer, and text generation tasks <span id="id6">[<a class="reference internal" href="../References.html#id101" title="S. Kaddoura. A Primer on Generative Adversarial Networks. Springer International Publishing, 2023. ISBN 9783031326622. URL: https://books.google.ca/books?id=dJUr0AEACAAJ.">Kaddoura, 2023</a>, <a class="reference internal" href="../References.html#id213" title="Hemant Yadav, Jalpesh Vasa, and Rudra Patel. Gan (generative adversarial network)-based image super-resolution: a technical perspective. In International Conference on Information and Communication Technology for Intelligent Systems, 283–293. Springer, 2023.">Yadav <em>et al.</em>, 2023</a>]</span>.</p></li>
<li><p><strong>Gated Recurrent Unit (GRU)</strong>: These are another special type of RNNs that have reset and update gates that can control the flow of information within the hidden layers. GRU networks are simpler and faster than LSTM networks, and achieve similar or better performance on some tasks. GRU networks are widely used for natural language processing and speech recognition tasks
<span id="id7">[<a class="reference internal" href="../References.html#id55" title="Rahul Dey and Fathi M. Salem. Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS), volume, 1597-1600. 2017. doi:10.1109/MWSCAS.2017.8053243.">Dey and Salem, 2017</a>, <a class="reference internal" href="../References.html#id180" title="F.M. Salem. Recurrent Neural Networks: From Simple to Gated Architectures. Springer International Publishing, 2022. ISBN 9783030899295. URL: https://books.google.ca/books?id=bJpXEAAAQBAJ.">Salem, 2022</a>]</span>.</p></li>
<li><p><strong>Graph neural network (GNN)</strong>: These are neural networks that use graph structures to model the input data, such as social networks or molecules. GNN networks consist of multiple layers of graph convolution or graph attention, where each layer updates the node features based on the neighboring nodes and edges. GNN networks can learn the complex and non-Euclidean relationships of the data, and are widely used for graph analysis, node classification, and link prediction tasks  <span id="id8">[<a class="reference internal" href="../References.html#id85" title="W.L. Hamilton. Graph Representation Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers, 2020. ISBN 9781681739649. URL: https://books.google.ca/books?id=Csj-DwAAQBAJ.">Hamilton, 2020</a>, <a class="reference internal" href="../References.html#id208" title="L. Wu, P. Cui, J. Pei, and L. Zhao. Graph Neural Networks: Foundations, Frontiers, and Applications. Springer Nature Singapore, 2022. ISBN 9789811660542. URL: https://books.google.ca/books?id=XplXEAAAQBAJ.">Wu <em>et al.</em>, 2022</a>]</span>.</p></li>
<li><p><strong>Long short-term memory (LSTM)</strong>: These are a special type of RNNs that have memory cells or gates that can store or forget information over long periods of time. LSTM networks can handle long and complex sequences of data, and overcome the problems of vanishing or exploding gradients that affect RNNs. LSTM networks are widely used for machine translation, text generation, and sentiment analysis tasks <span id="id9">[<a class="reference internal" href="../References.html#id82" title="A. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computational Intelligence. Springer Berlin Heidelberg, 2012. ISBN 9783642247972. URL: https://books.google.ca/books?id=wpb-CAAAQBAJ.">Graves, 2012</a>]</span>.</p></li>
<li><p><strong>Neural Turing machine (NTM)</strong>: These are neural networks that use external memory to store and manipulate information, such as symbols or sequences. NTM networks consist of a controller and a memory, where the controller reads and writes to the memory using attention mechanisms. NTM networks can learn to perform algorithmic tasks and generalization, and are widely used for program synthesis, reasoning, and meta-learning tasks<span id="id10">[<a class="reference internal" href="../References.html#id68" title="Soroor Malekmohamadi Faradonbe and Faramarz Safi-Esfahani. A classifier task based on neural turing machine and particle swarm algorithm. Neurocomputing, 396:133-152, 2020. doi:10.1016/j.neucom.2018.07.097.">Faradonbe and Safi-Esfahani, 2020</a>, <a class="reference internal" href="../References.html#id130" title="Soroor Malekmohamadi Faradonbe, Faramarz Safi-Esfahani, and Morteza Karimian-Kelishadrokhi. A review on neural turing machine (ntm). SN Computer Science, 1(6):333, 2020.">Malekmohamadi Faradonbe <em>et al.</em>, 2020</a>]</span>.</p></li>
<li><p><strong>Recurrent neural networks (RNNs)</strong>: These are neural networks that have feedback loops or connections between the hidden layers, which allow them to process sequential data, such as speech or text. RNNs can learn from the previous inputs and outputs, and capture the temporal or sequential dependencies of the data. RNNs are widely used for natural language processing and speech recognition tasks <span id="id11">[<a class="reference internal" href="../References.html#id4" title="Muhammad Shamsul Alam, Farhan Bin Mohamed, Ali Selamat, and Akm Bellal Hossain. A review of recurrent neural network based camera localization for indoor environments. IEEE Access, 11():43985-44009, 2023. doi:10.1109/ACCESS.2023.3272479.">Alam <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id180" title="F.M. Salem. Recurrent Neural Networks: From Simple to Gated Architectures. Springer International Publishing, 2022. ISBN 9783030899295. URL: https://books.google.ca/books?id=bJpXEAAAQBAJ.">Salem, 2022</a>]</span>.</p></li>
<li><p><strong>Restricted Boltzmann Machine (RBM)</strong>: These are neural networks that use unsupervised learning to learn a probabilistic distribution of the input data, such as images or text. RBM networks consist of a visible layer and a hidden layer, where the visible layer represents the input data, and the hidden layer represents the latent variables or features. RBM networks can generate new data samples from the learned distribution, and are widely used for data generation, recommendation, and collaborative filtering tasks <span id="id12">[<a class="reference internal" href="../References.html#id179" title="F. Sabry. Restricted Boltzmann Machine: Fundamentals and Applications for Unlocking the Hidden Layers of Artificial Intelligence. Artificial Intelligence. One Billion Knowledgeable, 2023. URL: https://books.google.ca/books?id=uQzHEAAAQBAJ.">Sabry, 2023</a>, <a class="reference internal" href="../References.html#id214" title="W.Q. Yan. Computational Methods for Deep Learning: Theoretic, Practice and Applications. Texts in Computer Science. Springer International Publishing, 2020. ISBN 9783030610814. URL: https://books.google.ca/books?id=AP4MEAAAQBAJ.">Yan, 2020</a>]</span>.</p></li>
<li><p><strong>Self-Organizing Map (SOM)</strong>: These are neural networks that use unsupervised learning to create a low-dimensional representation of the input data, such as images or text. SOM networks can cluster or group similar data points together, and preserve the topological or spatial relationships of the data. SOM networks are widely used for data visualization, dimensionality reduction, and anomaly detection tasks <span id="id13">[<a class="reference internal" href="../References.html#id110" title="T. Kohonen. Self-Organizing Maps. Springer Series in Information Sciences. Springer Berlin Heidelberg, 2012. ISBN 9783642569272. URL: https://books.google.ca/books?id=M-zxCAAAQBAJ.">Kohonen, 2012</a>, <a class="reference internal" href="../References.html#id202" title="A. Vellido, K. Gibert, C. Angulo, and J.D.M. Guerrero. Advances in Self-Organizing Maps, Learning Vector Quantization, Clustering and Data Visualization: Proceedings of the 13th International Workshop, WSOM+ 2019, Barcelona, Spain, June 26-28, 2019. Advances in Intelligent Systems and Computing. Springer International Publishing, 2019. ISBN 9783030196424. URL: https://books.google.ca/books?id=moOVDwAAQBAJ.">Vellido <em>et al.</em>, 2019</a>]</span>.</p></li>
<li><p><strong>Transformer</strong>: These are neural networks that use attention mechanisms to process sequential data, such as speech or text. Attention mechanisms allow the network to focus on the relevant parts of the input and output sequences, and encode the dependencies and relationships of the data. Transformer networks can handle long and complex sequences of data, and are widely used for natural language processing and speech recognition tasks <span id="id14">[<a class="reference internal" href="../References.html#id13" title="Moinak Bhattacharya, Shubham Jain, and Prateek Prasanna. Radiotransformer: a cascaded global-focal transformer for visual attention–guided disease classification. In European Conference on Computer Vision, 679–698. Springer, 2022. doi:10.1186/s40537-021-00444-8.">Bhattacharya <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id210" title="Yong Xia, Yueqi Xiong, and Kuanquan Wang. A transformer model blended with cnn and denoising autoencoder for inter-patient ecg arrhythmia classification. Biomedical Signal Processing and Control, 86:105271, 2023.">Xia <em>et al.</em>, 2023</a>, <a class="reference internal" href="../References.html#id211" title="Yan Xiong, Guo Xinya, and Junjie Xu. Cnn-transformer: a deep learning method for automatically identifying learning engagement. Education and Information Technologies, pages 1–20, 2023.">Xiong <em>et al.</em>, 2023</a>]</span>.</p></li>
</ul>
<p>These are some of the categories of deep learning architectures that are used for various applications.</p>
<section id="choice-between-tensorflow-and-pytorch">
<h2><span class="section-number">12.9.1. </span>Choice between TensorFlow and PyTorch<a class="headerlink" href="#choice-between-tensorflow-and-pytorch" title="Permalink to this heading">#</a></h2>
<p>All the mentioned architectures, namely Autoencoder (AE), Convolutional Neural Networks (CNNs), Deep Belief Network (DBN), Deep Stacking Network (DSN), Feedforward Neural Networks (FNNs), Generative Adversarial Network (GAN), Gated Recurrent Unit (GRU), Graph Neural Network (GNN), Long Short-Term Memory (LSTM), Neural Turing Machine (NTM), Recurrent Neural Networks (RNNs), Restricted Boltzmann Machine (RBM), Self-Organizing Map (SOM), and Transformer, are available in both TensorFlow and PyTorch frameworks. These frameworks provide comprehensive support for implementing and training various deep learning architectures, allowing researchers and practitioners to choose the framework that aligns with their preferences and requirements.</p>
<ul class="simple">
<li><p>TensorFlow and PyTorch are two of the most popular and widely used frameworks for deep learning. They both offer high-level APIs and low-level interfaces for building, training, and deploying various deep learning models. They also support GPU and distributed computing, as well as various tools and libraries for data processing, visualization, debugging, and testing.</p></li>
<li><p>The choice of framework depends on several factors, such as the type of problem, the level of customization, the ease of use, the performance, the documentation, and the community support. There is no definitive answer to which framework is better, as they both have their own strengths and weaknesses. Some of the main differences and similarities between TensorFlow and PyTorch are summarized in the table below  <span id="id15">[<a class="reference internal" href="../References.html#id38" title="Hulin Dai, Xuan Peng, Xuanhua Shi, Ligang He, Qian Xiong, and Hai Jin. Reveal training performance mystery between tensorflow and pytorch in the single gpu environment. Science China Information Sciences, 65(1):112103, 2021. doi:10.1007/s11432-020-3182-1.">Dai <em>et al.</em>, 2021</a>, <a class="reference internal" href="../References.html#id156" title="Ovidiu-Constantin Novac, Mihai Cristian Chirodea, Cornelia Mihaela Novac, Nicu Bizon, Mihai Oproescu, Ovidiu Petru Stan, and Cornelia Emilia Gordan. Analysis of the application efficiency of tensorflow and pytorch in convolutional neural network. Sensors, 22(22):8872, 2022.">Novac <em>et al.</em>, 2022</a>]</span>:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>TensorFlow</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Programming style</p></td>
<td><p>Static graph</p></td>
<td><p>Dynamic graph</p></td>
</tr>
<tr class="row-odd"><td><p>Debugging</p></td>
<td><p>Difficult</p></td>
<td><p>Easy</p></td>
</tr>
<tr class="row-even"><td><p>Deployment</p></td>
<td><p>Easy</p></td>
<td><p>Difficult</p></td>
</tr>
<tr class="row-odd"><td><p>Performance</p></td>
<td><p>Fast</p></td>
<td><p>Slow</p></td>
</tr>
<tr class="row-even"><td><p>Documentation</p></td>
<td><p>Comprehensive</p></td>
<td><p>Sparse</p></td>
</tr>
<tr class="row-odd"><td><p>Community</p></td>
<td><p>Large</p></td>
<td><p>Small</p></td>
</tr>
<tr class="row-even"><td><p>Ecosystem</p></td>
<td><p>Rich</p></td>
<td><p>Poor</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>As for the deep learning architectures, they are designed for different purposes and applications, such as image recognition, natural language processing, generative modeling, reinforcement learning, etc. Each architecture has its own advantages and disadvantages, and the choice of architecture depends on the task, the data, and the desired outcome. Some of the main characteristics and applications of the architectures you mentioned are summarized in the table below <span id="id16">[<a class="reference internal" href="../References.html#id6" title="Laith Alzubaidi, Jinglan Zhang, Amjad J Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, José Santamaría, Mohammed A Fadhel, Muthana Al-Amidie, and Laith Farhan. Review of deep learning: concepts, cnn architectures, challenges, applications, future directions. Journal of big Data, 8(1):53, 2021. doi:10.1186/s40537-021-00444-8.">Alzubaidi <em>et al.</em>, 2021</a>, <a class="reference internal" href="../References.html#id116" title="Yunseok Kwak, Won Joon Yun, Jae Pyoung Kim, Hyunhee Cho, Jihong Park, Minseok Choi, Soyi Jung, and Joongheon Kim. Quantum distributed deep learning architectures: models, discussions, and applications. ICT Express, 9(3):486–491, 2023.">Kwak <em>et al.</em>, 2023</a>]</span>:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Characteristics</p></th>
<th class="head"><p>Applications</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Autoencoder (AE)</p></td>
<td><p>A type of neural network that learns to reconstruct its input by encoding it into a latent representation and decoding it back.</p></td>
<td><p>Data compression, denoising, anomaly detection, dimensionality reduction, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Convolutional Neural Network (CNN)</p></td>
<td><p>A type of neural network that uses convolutional layers to extract features from images or other structured data.</p></td>
<td><p>Image recognition, object detection, face recognition, semantic segmentation, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Deep Belief Network (DBN)</p></td>
<td><p>A type of neural network that consists of multiple layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other.</p></td>
<td><p>Feature extraction, unsupervised learning, generative modeling, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Deep Stacking Network (DSN)</p></td>
<td><p>A type of neural network that consists of multiple layers of simple classifiers stacked on top of each other.</p></td>
<td><p>Supervised learning, classification, regression, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Feedforward Neural Network (FNN)</p></td>
<td><p>A type of neural network that consists of multiple layers of neurons connected in a forward direction.</p></td>
<td><p>Supervised learning, classification, regression, function approximation, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Generative Adversarial Network (GAN)</p></td>
<td><p>A type of neural network that consists of two competing networks: a generator that tries to produce realistic samples from a latent distribution, and a discriminator that tries to distinguish between real and fake samples.</p></td>
<td><p>Image synthesis, style transfer, image inpainting, super-resolution, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Gated Recurrent Unit (GRU)</p></td>
<td><p>A type of recurrent neural network that uses gated units to control the flow of information through time.</p></td>
<td><p>Sequence modeling, natural language processing, speech recognition, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Graph Neural Network (GNN)</p></td>
<td><p>A type of neural network that operates on graph-structured data, such as social networks, molecular graphs, knowledge graphs, etc.</p></td>
<td><p>Graph analysis, node classification, link prediction, graph generation, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Long Short-Term Memory (LSTM)</p></td>
<td><p>A type of recurrent neural network that uses memory cells to store and access information over long time intervals.</p></td>
<td><p>Sequence modeling, natural language processing, speech recognition, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Neural Turing Machine (NTM)</p></td>
<td><p>A type of neural network that augments a recurrent neural network with an external memory that can be read and written by the network.</p></td>
<td><p>Memory-augmented learning, algorithm learning, program synthesis, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Recurrent Neural Network (RNN)</p></td>
<td><p>A type of neural network that has recurrent connections that allow it to process sequential data.</p></td>
<td><p>Sequence modeling, natural language processing, speech recognition, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Restricted Boltzmann Machine (RBM)</p></td>
<td><p>A type of neural network that consists of two layers of stochastic units: a visible layer that represents the input, and a hidden layer that represents the latent features.</p></td>
<td><p>Unsupervised learning, feature extraction, generative modeling, etc.</p></td>
</tr>
<tr class="row-even"><td><p>Self-Organizing Map (SOM)</p></td>
<td><p>A type of neural network that consists of a grid of neurons that learn to map high-dimensional input data to low-dimensional output space.</p></td>
<td><p>Data visualization, clustering, dimensionality reduction, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer</p></td>
<td><p>A type of neural network that uses attention mechanisms to encode and decode sequential data.</p></td>
<td><p>Natural language processing, machine translation, text generation, etc.</p></td>
</tr>
</tbody>
</table>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>MLP can play a versatile role in various deep learning architectures, contingent upon the specific task and dataset. For instance:</p>
<ul class="simple">
<li><p>MLP serves as a fundamental deep learning architecture for supervised learning tasks, encompassing classification and regression, particularly well-suited for structured or tabular data. It excels at approximating intricate functions by adjusting its weights based on input-output pairs. However, MLP may exhibit limitations when confronted with sequential or spatial data, as it lacks the inherent capacity to capture temporal or spatial dependencies in the data.</p></li>
<li><p>MLP functions as a pivotal component within broader deep learning architectures like CNNs, RNNs, or Transformers, where it assumes specialized roles in executing specific functions or subtasks. For instance, it can act as a fully connected layer at the conclusion of a CNN, facilitating classification on extracted features. Alternatively, within a Transformer architecture, MLP can operate as a feed-forward network to process attention outputs. Moreover, MLP finds application as a classifier or regressor atop other architectures such as RNNs or GNNs, contributing to the generation of final outputs.</p></li>
</ul>
</div>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Within this course, a comprehensive exploration of the aforementioned methods is beyond our scope. Instead, we will offer illustrative examples of common deep learning applications within the realm of engineering.</p>
</div>
</section>
<section id="convolutional-neural-network-cnn">
<h2><span class="section-number">12.9.2. </span>Convolutional Neural Network (CNN)<a class="headerlink" href="#convolutional-neural-network-cnn" title="Permalink to this heading">#</a></h2>
<p>Convolutional Neural Networks (CNNs) constitute a specialized category of neural networks meticulously designed for the efficient processing of grid-like data, notably applied to domains such as image analysis. This architectural paradigm draws inspiration from the intricacies of visual processing observed in the human brain. Central to the functionality of CNNs are the convolutional layers, acting as specialized filters adept at detecting intricate patterns and features within the input data.</p>
<p>I assume you want to refine your text by adding some information about the benefits and drawbacks of rescaling, as well as some examples of other preprocessing layers. Here is a possible way to do that:</p>
<section id="rescaling-layer">
<h3><span class="section-number">12.9.2.1. </span>Rescaling Layer<a class="headerlink" href="#rescaling-layer" title="Permalink to this heading">#</a></h3>
<p>The “Rescaling Layer” is a crucial preprocessing step in neural networks, especially when dealing with image-based tasks. Its primary purpose is to rescale input images to specific dimensions, thereby ensuring consistency in pixel values and creating a standardized input format for subsequent layers in the neural network.</p>
<p>In the context of image processing, rescaling involves adjusting the pixel values of input images to a predefined range, commonly between 0 and 1. This normalization serves several important functions. Firstly, it helps prevent numerical instabilities that may arise if pixel values span a broad range, which could impact the stability and convergence of the learning process. Secondly, normalization mitigates the impact of varying illumination conditions or color scales in images, promoting a consistent representation across different inputs.</p>
<p>Consider an example where an image has pixel values ranging from 0 to 255 (common in grayscale images). The Rescaling Layer would transform these values to a standardized range, such as between 0 and 1, by dividing each pixel value by 255. This normalization ensures that the neural network processes images in a consistent manner, regardless of the original intensity scale.</p>
<p>By rescaling pixel values to a standardized range, the neural network becomes less sensitive to variations in absolute intensity across different images. Instead, it focuses on learning the relative relationships between pixel values, enabling the network to extract meaningful patterns and features from the images. This is vital for effective learning, as it ensures that the network’s parameters are adjusted based on the intrinsic structure of the data rather than variations in intensity.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Rescaling</span></code> layer in TensorFlow is a convenient tool for rescaling the pixel values of input data. This layer is particularly useful in preprocessing images before feeding them into a neural network. The primary purpose of <code class="docutils literal notranslate"><span class="pre">Rescaling</span></code> is to normalize the pixel values, ensuring that they fall within a specified range.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling">Rescaling</a> layer in TensorFlow is a preprocessing layer that rescales input data to a specified range. It is often employed for normalizing pixel values in images, promoting stability and uniformity in the input data. The layer takes two parameters: <code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">offset</span></code>. The rescaling formula is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cb9f6bcc-598c-463f-9767-c5796b587034">
<span class="eqno">(12.41)<a class="headerlink" href="#equation-cb9f6bcc-598c-463f-9767-c5796b587034" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{output} = \text{input} \times \text{scale} + \text{offset} \end{equation}\]</div>
<ul class="simple">
<li><p><strong>Scale:</strong> A multiplier to rescale the input data.</p></li>
<li><p><strong>Offset:</strong> An additive term to adjust the rescaled values.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Assume input_data is an image tensor with pixel values in the range [0, 255]</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">100.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">200.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">50.0</span><span class="p">,</span> <span class="mf">75.0</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">]])</span>

<span class="c1"># Define the Rescaling layer with a scale of 1/255 to normalize pixel values to [0, 1]</span>
<span class="n">rescaling_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Rescaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>

<span class="c1"># Apply Rescaling to the input_data</span>
<span class="n">normalized_data</span> <span class="o">=</span> <span class="n">rescaling_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Display the original and normalized data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Normalized Data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">normalized_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Data:
tf.Tensor(
[[100. 150. 200.]
 [ 50.  75.  25.]], shape=(2, 3), dtype=float32)

Normalized Data:
tf.Tensor(
[[0.3921569  0.5882353  0.7843138 ]
 [0.19607845 0.29411766 0.09803922]], shape=(2, 3), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">Rescaling</span></code> layer is employed with a scale of <span class="math notranslate nohighlight">\( \frac{1}{255} \)</span>. The layer effectively scales the pixel values in the input_data tensor, transforming them to the normalized range [0, 1]. This normalization is particularly useful in image processing tasks, contributing to the stability and convergence of neural networks during training. The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Rescaling</span></code> layer serves as a straightforward and efficient way to perform rescaling on input data.</p>
<p>However, rescaling is not the only preprocessing step that can be applied to input data. Depending on the task and the data, other preprocessing layers may be more suitable or necessary. Some of the other preprocessing layers that can be used in TensorFlow are:</p>
<ul class="simple">
<li><p><strong>CenterCrop:</strong> A layer that crops the central portion of the input data, reducing the spatial dimensions and removing the irrelevant or noisy parts of the data. This layer can be useful for focusing on the most important features of the data, such as the face in a portrait image.</p></li>
<li><p><strong>RandomCrop:</strong> A layer that randomly crops a portion of the input data, introducing some variation and diversity in the data. This layer can be useful for augmenting the data and preventing overfitting, as it exposes the network to different perspectives and scenarios of the data.</p></li>
<li><p><strong>RandomFlip:</strong> A layer that randomly flips the input data horizontally or vertically, creating a mirror image of the data. This layer can also be useful for augmenting the data and preventing overfitting, as it simulates different orientations and viewpoints of the data.</p></li>
<li><p><strong>RandomRotation:</strong> A layer that randomly rotates the input data by a certain angle, creating a rotated image of the data. This layer can also be useful for augmenting the data and preventing overfitting, as it mimics different rotations and angles of the data.</p></li>
<li><p><strong>Normalization:</strong> A layer that normalizes the input data by subtracting the mean and dividing by the standard deviation of the data. This layer can be useful for standardizing the data and improving the performance of the network, as it makes the data more Gaussian-like and reduces the variance of the data.</p></li>
</ul>
<p>These are some of the preprocessing layers that can be used in TensorFlow to transform and enhance the input data. Depending on the task and the data, different combinations and sequences of preprocessing layers can be applied to achieve the best results. Preprocessing is an essential step in neural networks, as it can affect the quality and efficiency of the learning process.</p>
</section>
<section id="convolutional-layers">
<h3><span class="section-number">12.9.2.2. </span>Convolutional Layers<a class="headerlink" href="#convolutional-layers" title="Permalink to this heading">#</a></h3>
<p>In the realm of CNNs, the input image undergoes a transformative journey through a sequence of convolutional layers. Each of these layers harnesses a collection of filters, featuring small, learnable weights that convolve across the spatial dimensions of the input. This convolutional operation adeptly extracts local patterns, ranging from elementary features like edges to more intricate textures. The progressive movement through these layers facilitates the capturing of hierarchical features, contributing to the network’s ability to discern and comprehend complex structures inherent in the input data.</p>
<p>A critical element within the convolutional layer’s architecture involves the systematic traversal of a convolutional filter across an input matrix.</p>
<p><font color='Blue'><b>Example:</b></font> Consider a 3x3 convolutional filter denoted by the matrix [[0,1,0], [1,0,1], [0,1,0]]. In this convolutional layer, nine distinct convolutional operations unfold, with each operation meticulously applied to a 5x5 input matrix. Crucially, each operation is directed at a unique 3x3 segment of the input matrix. The culmination of these operations results in the creation of a consolidated 3x3 matrix, encapsulating the collective outcomes of the nine individual convolutional processes <span id="id17">[<a class="reference internal" href="../References.html#id44" title="Google Developers. Machine learning glossary. https://developers.google.com/machine-learning/glossary, 2023. [Online; accessed 01-October-2023].">Google Developers, 2023</a>]</span>.</p>
<p>Let’s delve into the mathematical steps involved:</p>
<ol class="arabic">
<li><p><strong>Input Matrix (5x5):</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} I = \begin{bmatrix}
   1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\
   6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\
   11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 \\
   16 &amp; 17 &amp; 18 &amp; 19 &amp; 20 \\
   21 &amp; 22 &amp; 23 &amp; 24 &amp; 25 \\
   \end{bmatrix}
   \end{equation*}\]</div>
</li>
<li><p><strong>Convolutional Filter (3x3):</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} F = \begin{bmatrix}
   0 &amp; 1 &amp; 0 \\
   1 &amp; 0 &amp; 1 \\
   0 &amp; 1 &amp; 0 \\
   \end{bmatrix}
   \end{equation*}\]</div>
</li>
<li><p><strong>Convolution Operation at (1,1):</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} \text{Result}(1,1) &amp; = (1 \times 0) + (2 \times 1) + (3 \times 0) + (6 \times 1) + (7 \times 0) + (8 \times 1) \\ &amp; + (11 \times 0) + (12 \times 1) + (13 \times 0) = 28 \end{align*}\]</div>
</li>
<li><p><strong>Result Matrix (Feature Map):</strong>
The feature map is composed of the results of each convolution operation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Feature Map} = \begin{bmatrix}
   28 &amp; \text{Result}(1,2) &amp; \text{Result}(1,3) \\
   \text{Result}(2,1) &amp; \text{Result}(2,2) &amp; \text{Result}(2,3) \\
   \text{Result}(3,1) &amp; \text{Result}(3,2) &amp; \text{Result}(3,3) \\
   \end{bmatrix}
   \end{equation*}\]</div>
<p>Continue this process for each position in the matrix.</p>
</li>
</ol>
<p>These numerical values represent the outcome of the convolutional operations, vividly illustrating how the convolutional filter systematically processes the input matrix to yield a discerning and informative feature map.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Conv2D</span></code> layer in TensorFlow is a core building block for convolutional neural networks (CNNs). It performs 2D convolutions on input data, typically used for processing images or spatial data. Convolutional layers are essential for extracting hierarchical features from input data, enabling the network to learn hierarchical representations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import numpy for array operations</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the input matrix as a numpy array</span>
<span class="n">input_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">]])</span>

<span class="c1"># Define the convolutional filter as a numpy array</span>
<span class="n">conv_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Define the output matrix as an empty numpy array</span>
<span class="n">output_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Loop over the input matrix with a stride of 1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="c1"># Extract a 3x3 segment of the input matrix</span>
        <span class="n">segment</span> <span class="o">=</span> <span class="n">input_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span>
        <span class="c1"># Perform element-wise multiplication and summation</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">segment</span> <span class="o">*</span> <span class="n">conv_filter</span><span class="p">)</span>
        <span class="c1"># Store the result in the output matrix</span>
        <span class="n">output_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>

<span class="c1"># Print the output matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[28. 32. 36.]
 [48. 52. 56.]
 [68. 72. 76.]]
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id20">
<a class="reference internal image-reference" href="../_images/CNN_Layers.gif"><img alt="../_images/CNN_Layers.gif" src="../_images/CNN_Layers.gif" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.17 </span><span class="caption-text">A visual representation of the above example.</span><a class="headerlink" href="#id20" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">Conv2D</a> layer in TensorFlow is used for 2D convolutions, applying a set of filters or kernels to input data. These filters slide over the input, performing element-wise multiplication and summing the results to produce feature maps. This process captures local patterns and spatial relationships in the input, making convolutional layers effective for image-related tasks.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer takes several parameters, such as:</p>
<ul class="simple">
<li><p><strong>filters:</strong> The number of filters or kernels to use in the convolutional layer. This determines the depth or the number of channels of the output feature map. Each filter can learn to detect a different feature from the input data.</p></li>
<li><p><strong>kernel_size:</strong> The size or shape of the filters or kernels to use in the convolutional layer. This determines the height and width of the region of the input that each filter covers. A common choice is a 3x3 kernel, which can capture local patterns and features effectively.</p></li>
<li><p><strong>strides:</strong> The number of pixels to move the filters or kernels across the input in each dimension. This determines the spatial resolution or the height and width of the output feature map. A common choice is a stride of 1, which means that the filters move one pixel at a time, preserving the spatial resolution of the input. A larger stride can reduce the spatial resolution and the computational complexity of the layer, but it may also lose some information from the input.</p></li>
<li><p><strong>padding:</strong> The way to handle the edges of the input in the convolutional layer. This determines whether to add zeros or to ignore the edges of the input when applying the filters or kernels. A common choice is a padding of ‘same’, which means that the input is padded with zeros such that the output feature map has the same spatial resolution as the input. Another choice is a padding of ‘valid’, which means that the input is not padded and the output feature map has a smaller spatial resolution than the input.</p></li>
<li><p><strong>activation:</strong> The activation function to apply to the output feature map of the convolutional layer. This determines how the output values are transformed into a non-linear range. A common choice is a ReLU activation function, which sets any negative value to zero and preserves any positive value. This can introduce some sparsity and non-linearity in the output feature map, enhancing the learning ability of the layer.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Assume input_data is an image tensor with shape (batch_size, height, width, channels)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Define the Conv2D layer with 64 filters, 3x3 kernel size, 1 stride, same padding, and ReLU activation</span>
<span class="n">conv2d_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>

<span class="c1"># Apply Conv2D to the input_data</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">conv2d_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Display the shape of the input and output data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input Data Shape:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Output Data Shape:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input Data Shape:
(32, 28, 28, 3)

Output Data Shape:
(32, 28, 28, 64)
</pre></div>
</div>
</div>
</div>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer is applied to the input_data tensor, which has a shape of (32, 28, 28, 3). This means that the input_data consists of 32 images, each with a height of 28 pixels, a width of 28 pixels, and 3 channels (RGB). The <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer uses 64 filters, each with a size of 3x3, a stride of 1, a padding of ‘same’, and a ReLU activation function. The output_data tensor has a shape of (32, 28, 28, 64). This means that the output_data consists of 32 feature maps, each with a height of 28 pixels, a width of 28 pixels, and 64 channels. Each channel corresponds to a different filter that has learned to detect a different feature from the input_data. The output_data can be fed into the next layer of the network, such as a pooling layer or another convolutional layer.</p>
</section>
<section id="pooling-layers">
<h3><span class="section-number">12.9.2.3. </span>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Permalink to this heading">#</a></h3>
<p>Pooling layers, integral components of convolutional neural network (CNN) architectures, play a crucial role in downsampling spatial dimensions and enhancing computational efficiency while preserving key features. Two prevalent pooling methods, max pooling and average pooling, contribute to this process. In max pooling, the maximum value within a defined region is retained, emphasizing prominent features, while average pooling calculates the average value within a region, providing a smoother representation.</p>
<p>Pooling layers systematically select representative values from specific regions of the input, effectively summarizing the presence of essential features. This strategic downsampling contributes to model efficiency by maintaining crucial information while reducing computational complexity <span id="id18">[<a class="reference internal" href="../References.html#id44" title="Google Developers. Machine learning glossary. https://developers.google.com/machine-learning/glossary, 2023. [Online; accessed 01-October-2023].">Google Developers, 2023</a>]</span>.</p>
<p><font color='Blue'><b>Example:</b></font> Consider a 2x2 max pooling operation applied to a 4x4 feature map:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Feature Map} = \begin{bmatrix}
   2 &amp; 4 &amp; 1 &amp; 7 \\
   6 &amp; 8 &amp; 3 &amp; 5 \\
   9 &amp; 12 &amp; 11 &amp; 10 \\
   14 &amp; 13 &amp; 15 &amp; 16 \\
\end{bmatrix} \end{equation*}\]</div>
<p>The pooling operation selects the maximum value within each 2x2 region, resulting in a downsampled 2x2 pooled map:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \text{Pooled Map} = \begin{bmatrix}
   8 &amp; 7 \\
   14 &amp; 16 \\
\end{bmatrix} \end{equation*}\]</div>
<p>This process vividly demonstrates how pooling layers efficiently condense the information in the feature map, retaining essential details for subsequent layers in the neural network <span id="id19">[<a class="reference internal" href="../References.html#id44" title="Google Developers. Machine learning glossary. https://developers.google.com/machine-learning/glossary, 2023. [Online; accessed 01-October-2023].">Google Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import numpy for array operations</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the feature map as a numpy array</span>
<span class="n">feature_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">]])</span>

<span class="c1"># Define the pooled map as an empty numpy array</span>
<span class="n">pooled_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Loop over the feature map with a stride of 2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Extract a 2x2 region of the feature map</span>
        <span class="n">region</span> <span class="o">=</span> <span class="n">feature_map</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span>
        <span class="c1"># Select the maximum value within the region</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">region</span><span class="p">)</span>
        <span class="c1"># Store the result in the pooled map</span>
        <span class="n">pooled_map</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>

<span class="c1"># Print the pooled map</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pooled_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 8.  7.]
 [14. 16.]]
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id21">
<a class="reference internal image-reference" href="../_images/Pooling_Layers.gif"><img alt="../_images/Pooling_Layers.gif" src="../_images/Pooling_Layers.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.18 </span><span class="caption-text">A visual representation of the above example.</span><a class="headerlink" href="#id21" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.MaxPooling2D</span></code> layer in TensorFlow is a pooling layer commonly used in convolutional neural networks (CNNs) for downsampling spatial dimensions. Pooling layers help reduce the spatial resolution of the input data, leading to a more compact representation while retaining essential features. Max pooling is a specific pooling operation where the maximum value within each pooling window is retained.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D">MaxPooling2D</a> layer performs 2D max pooling, where it downsamples the spatial dimensions of the input data by selecting the maximum value within each pooling window. This operation is applied independently to different channels of the input data.</p>
<p>he MaxPooling2D layer takes several parameters, such as:</p>
<ul class="simple">
<li><p><strong>pool_size:</strong> The size of the pooling window, typically specified as a tuple (height, width).</p></li>
<li><p><strong>strides:</strong> The step size for sliding the pooling window over the input.</p></li>
<li><p><strong>padding:</strong> “valid” (no padding) or “same” (zero-padding to maintain spatial dimensions).</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a MaxPooling2D layer with a 2x2 pooling window and &#39;valid&#39; padding</span>
<span class="n">maxpool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>

<span class="c1"># Assuming input_data is a 4D tensor representing an image (batch_size, height, width, channels)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">]],</span>
                           <span class="p">[[</span><span class="mf">5.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">8.0</span><span class="p">]],</span>
                           <span class="p">[[</span><span class="mf">9.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">12.0</span><span class="p">]],</span>
                           <span class="p">[[</span><span class="mf">13.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">14.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">15.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">16.0</span><span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Apply MaxPooling2D to the input data</span>
<span class="n">output_data</span> <span class="o">=</span> <span class="n">maxpool_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Display the original and pooled data shapes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data Shape:&quot;</span><span class="p">,</span> <span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pooled Data Shape:&quot;</span><span class="p">,</span> <span class="n">output_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Data Shape: (1, 4, 4, 1)
Pooled Data Shape: (1, 2, 2, 1)
</pre></div>
</div>
</div>
</div>
<p>In this example, a <code class="docutils literal notranslate"><span class="pre">MaxPooling2D</span></code> layer is defined with a 2x2 pooling window and ‘valid’ padding. The layer is then applied to an example input data tensor, resulting in a downsampled output. Adjust the parameters based on the requirements of your specific CNN architecture. The <code class="docutils literal notranslate"><span class="pre">pool_size</span></code> determines the size of the pooling window, and <code class="docutils literal notranslate"><span class="pre">padding</span></code> influences whether zero-padding is applied to maintain spatial dimensions.</p>
<p>However, max pooling is not the only pooling operation that can be applied to input data. Depending on the task and the data, other pooling layers may be more suitable or necessary. Some of the other pooling layers that can be used in TensorFlow are:</p>
<ul class="simple">
<li><p><strong>AveragePooling2D:</strong> A layer that performs 2D average pooling, where it downsamples the spatial dimensions of the input data by selecting the average value within each pooling window. This operation is applied independently to different channels of the input data. Average pooling can provide a smoother representation of the input data, but it may also lose some important features that max pooling can preserve.</p></li>
<li><p><strong>GlobalMaxPooling2D:</strong> A layer that performs global max pooling, where it downsamples the spatial dimensions of the input data by selecting the maximum value within the entire input. This operation is applied independently to different channels of the input data. Global max pooling can reduce the spatial resolution of the input data to a single value per channel, which can be useful for classification or regression tasks.</p></li>
<li><p><strong>GlobalAveragePooling2D:</strong> A layer that performs global average pooling, where it downsamples the spatial dimensions of the input data by selecting the average value within the entire input. This operation is applied independently to different channels of the input data. Global average pooling can also reduce the spatial resolution of the input data to a single value per channel, which can be useful for classification or regression tasks.</p></li>
</ul>
<p>These are some of the pooling layers that can be used in TensorFlow to downsample and summarize the input data. Depending on the task and the data, different combinations and sequences of pooling layers can be applied to achieve the best results. Pooling is an essential step in convolutional neural networks, as it can affect the quality and efficiency of the learning process.</p>
</section>
<section id="dropout-layers">
<h3><span class="section-number">12.9.2.4. </span>Dropout Layers<a class="headerlink" href="#dropout-layers" title="Permalink to this heading">#</a></h3>
<p>The “Dropout Layer” is a regularization technique used in neural networks, and it specifically refers to the Dropout layer in the context of deep learning models. This layer plays a crucial role in preventing overfitting, a common issue where a model learns to perform exceptionally well on the training data but struggles to generalize to unseen data.</p>
<p>Overfitting occurs when a model becomes too complex and starts memorizing patterns specific to the training data, including noise or outliers, rather than learning the underlying patterns that generalize well to new data. Dropout is a regularization method designed to address this problem.</p>
<p>In a Dropout Layer, during training, a random fraction of neurons (nodes) in the layer is “dropped out” or deactivated. This means that their outputs are set to zero, effectively removing their contribution to the forward pass and backward pass during that specific iteration of training. The fraction of neurons to drop out is a hyperparameter and is typically set between 0.2 and 0.5.</p>
<p>The key idea behind dropout is that it introduces noise and variability during training. By randomly removing neurons, the model becomes less reliant on specific neurons for making predictions, forcing it to learn more robust and generalizable features. This process is akin to training multiple different subnetworks on different subsets of the data, and it helps prevent the model from memorizing noise in the training set.</p>
<p>During inference or prediction, when the model is not being trained, all neurons are active, but their outputs are scaled by the dropout rate. This scaling ensures that the expected output during inference remains similar to what the model learned during training.</p>
<p>The Dropout Layer is a regularization technique that helps prevent overfitting by randomly deactivating a fraction of neurons during training. This introduces robustness and improves the generalization ability of the model, making it more effective on unseen data.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dropout</span></code> layer in TensorFlow is a regularization technique commonly used in neural networks, including convolutional neural networks (CNNs). Dropout helps prevent overfitting by randomly setting a fraction of input units to zero during training. This introduces a form of noise to the network, which encourages the network to be more robust and prevents reliance on specific neurons.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout">Dropout</a> layer in TensorFlow is a regularization layer that randomly sets a fraction of input units to zero at each update during training time. This random “dropping out” of units helps prevent overfitting by reducing the reliance on specific neurons. It introduces a level of uncertainty during training, making the network more resilient and improving its generalization to unseen data.</p>
<p><strong>Key Parameter</strong>:</p>
<ul class="simple">
<li><p><strong>rate:</strong> The fraction of input units to drop during training. It is a float value between 0 and 1, representing the dropout rate.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a Dropout layer with a dropout rate of 0.2</span>
<span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Assuming input_data is a tensor representing the output from a previous layer</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Apply Dropout to the input data during training</span>
<span class="n">training_output</span> <span class="o">=</span> <span class="n">dropout_layer</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Display the original and dropout-applied data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data:&quot;</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Output (with Dropout):&quot;</span><span class="p">,</span> <span class="n">training_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Data: tf.Tensor([[1. 2. 3. 4. 5.]], shape=(1, 5), dtype=float32)
Training Output (with Dropout): tf.Tensor([[0.   2.5  0.   0.   6.25]], shape=(1, 5), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this example, a <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer is defined with a dropout rate of 0.2. The layer is then applied to an example input data tensor during training (indicated by the <code class="docutils literal notranslate"><span class="pre">training</span></code> argument). The output data shows that some of the input units have been randomly set to zero, while the remaining units have been scaled by a factor of 1.25 (the inverse of the dropout rate). This scaling ensures that the expected output during inference remains similar to what the model learned during training.</p>
<p>However, dropout is not the only regularization technique that can be applied to neural networks. Depending on the task and the data, other regularization techniques may be more suitable or necessary. Some of the other regularization techniques that can be used in TensorFlow are:</p>
<ul class="simple">
<li><p><strong>L1 and L2 regularization:</strong> These are techniques that add a penalty term to the loss function based on the magnitude of the weights of the network. L1 regularization adds a penalty proportional to the absolute value of the weights, while L2 regularization adds a penalty proportional to the square of the weights. These penalties discourage the network from having large weights, which can lead to overfitting and instability. L1 and L2 regularization can be applied to any layer that has weights, such as <code class="docutils literal notranslate"><span class="pre">Dense</span></code> or <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code>, by using the <code class="docutils literal notranslate"><span class="pre">kernel_regularizer</span></code> argument and specifying the type and amount of regularization.</p></li>
<li><p><strong>Batch normalization:</strong> This is a technique that normalizes the input or output of a layer by subtracting the mean and dividing by the standard deviation of the batch. Batch normalization helps reduce the internal covariate shift, which is the change in the distribution of the inputs or outputs of a layer due to the updates of the previous layers. Batch normalization can improve the stability and performance of the network, as it makes the network less sensitive to the initialization and learning rate of the weights. Batch normalization can be applied to any layer by adding a <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer before or after the layer.</p></li>
<li><p><strong>Early stopping:</strong> This is a technique that stops the training process when the validation loss stops improving or starts increasing. Early stopping helps prevent overfitting by avoiding training the network for too long, which can cause the network to memorize the training data and lose its generalization ability. Early stopping can be implemented in TensorFlow by using a <code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code> callback, which monitors the validation loss and stops the training when a certain condition is met.</p></li>
</ul>
</section>
<section id="flatten-layers">
<h3><span class="section-number">12.9.2.5. </span>Flatten Layers<a class="headerlink" href="#flatten-layers" title="Permalink to this heading">#</a></h3>
<p>The “Flatten Layer” is a fundamental component in neural networks, particularly in the context of convolutional neural networks (CNNs). This layer serves the purpose of transforming the multi-dimensional output from preceding convolutional or pooling layers into a one-dimensional (flat) vector. The primary objective is to prepare the data for the transition from convolutional operations to fully connected layers.</p>
<p>In convolutional and pooling layers, the data is processed in a grid-like fashion, extracting hierarchical features from different spatial regions. However, fully connected layers require a one-dimensional input where each neuron is connected to every neuron in the preceding and succeeding layers. The Flatten Layer acts as a bridge between these two types of layers, reshaping the data into a format suitable for fully connected operations.</p>
<p>The operation of the Flatten Layer is conceptually straightforward. It takes the multi-dimensional array output from the preceding layers and rearranges it into a linear sequence. For example, in the case of a 2D input with dimensions (height, width), the Flatten Layer would transform it into a 1D array by concatenating the rows or columns.</p>
<p>This flattening process is essential because fully connected layers are typically used for learning complex relationships and making predictions based on the extracted features. The Flatten Layer ensures that the information captured by convolutional and pooling layers can be effectively utilized by fully connected layers for tasks such as image classification.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Flatten()</span></code> layer in TensorFlow is a simple layer that is commonly used in neural networks, especially when transitioning from convolutional layers to fully connected layers. It serves the purpose of flattening the input tensor, converting it from a multi-dimensional tensor into a one-dimensional tensor. This operation is necessary when moving from spatial hierarchies (e.g., image grids) to a linear representation, which is a prerequisite for fully connected layers.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten">Flatten()</a> layer in TensorFlow is applied to flatten the input tensor, converting it from a multi-dimensional tensor into a one-dimensional tensor. This is particularly useful when transitioning from convolutional layers, which operate on spatial hierarchies like images, to fully connected layers that require a linear input.</p>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Assume conv_output is a tensor representing the output from a convolutional layer</span>
<span class="n">conv_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>
                           <span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define a Flatten layer</span>
<span class="n">flatten_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

<span class="c1"># Apply Flatten to the convolutional output</span>
<span class="n">flattened_output</span> <span class="o">=</span> <span class="n">flatten_layer</span><span class="p">(</span><span class="n">conv_output</span><span class="p">)</span>

<span class="c1"># Display the original and flattened data shapes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data Shape:&quot;</span><span class="p">,</span> <span class="n">conv_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened Data Shape:&quot;</span><span class="p">,</span> <span class="n">flattened_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Data Shape: (2, 2, 3)
Flattened Data Shape: (2, 6)
</pre></div>
</div>
</div>
</div>
<p>In this example, a <code class="docutils literal notranslate"><span class="pre">Flatten()</span></code> layer is applied to a hypothetical convolutional output tensor. The <code class="docutils literal notranslate"><span class="pre">conv_output</span></code> tensor has a shape representing a 3D grid. The <code class="docutils literal notranslate"><span class="pre">Flatten()</span></code> layer converts this tensor into a one-dimensional tensor, which is crucial for passing the data to subsequent fully connected layers.</p>
<p>However, flattening is not the only reshaping operation that can be applied to input data. Depending on the task and the data, other reshaping layers may be more suitable or necessary. Some of the other reshaping layers that can be used in TensorFlow are:</p>
<ul class="simple">
<li><p><strong>Reshape:</strong> A layer that reshapes the input tensor into a specified shape, preserving the number of elements. This layer can be useful for changing the dimensionality or the order of the input data, such as converting a 2D matrix into a 3D tensor or vice versa.</p></li>
<li><p><strong>Transpose:</strong> A layer that transposes the input tensor, swapping the order of the axes. This layer can be useful for changing the orientation or the alignment of the input data, such as flipping the rows and columns of a matrix or the height and width of an image.</p></li>
<li><p><strong>Permute:</strong> A layer that permutes the input tensor, rearranging the order of the axes according to a specified pattern. This layer can be useful for changing the format or the layout of the input data, such as converting between different data formats like channels first or channels last.</p></li>
</ul>
<p>These are some of the reshaping layers that can be used in TensorFlow to transform and manipulate the input data. Depending on the task and the data, different combinations and sequences of reshaping layers can be applied to achieve the best results. Reshaping is an essential step in neural networks, as it can affect the structure and the representation of the data.</p>
</section>
<section id="dense-layers-fully-connected-layers">
<h3><span class="section-number">12.9.2.6. </span>Dense Layers (Fully Connected Layers)<a class="headerlink" href="#dense-layers-fully-connected-layers" title="Permalink to this heading">#</a></h3>
<p>Dense layers, also known as fully connected layers, form a fundamental component in the architecture of neural networks, particularly in feedforward neural networks. In a dense layer, each neuron is connected to every neuron in the preceding and succeeding layers, creating a dense network of interconnected nodes. These layers play a crucial role in learning complex representations and capturing intricate relationships within the input data.</p>
<p>The mathematical operation in a dense layer involves the weighted sum of the input features, followed by the application of an activation function. The weights associated with these connections are learnable parameters, adapted during the training process through optimization algorithms such as gradient descent. This adaptability allows dense layers to learn and extract hierarchical features, enabling the network to understand intricate patterns and make informed predictions.</p>
<p><font color='Blue'><b>Example:</b></font> Consider a simple example of a dense layer with three input features (neurons) and two output neurons. The weights associated with each connection are represented by the matrix <span class="math notranslate nohighlight">\( W \)</span>, and the biases for the two neurons are denoted by the vector <span class="math notranslate nohighlight">\( b \)</span>. The output <span class="math notranslate nohighlight">\( O \)</span> is computed as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5ab78ba-7ccd-4236-bf2f-7bfac69d26b9">
<span class="eqno">(12.42)<a class="headerlink" href="#equation-f5ab78ba-7ccd-4236-bf2f-7bfac69d26b9" title="Permalink to this equation">#</a></span>\[\begin{equation} O = \text{Activation}(W \cdot X + b) \end{equation}\]</div>
<p>Where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the input vector <span class="math notranslate nohighlight">\([x_1, x_2, x_3]\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\( W \)</span> is the weight matrix</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc115000-f972-4e31-a965-ba72d064f592">
<span class="eqno">(12.43)<a class="headerlink" href="#equation-bc115000-f972-4e31-a965-ba72d064f592" title="Permalink to this equation">#</a></span>\[\begin{equation} W = \begin{bmatrix}
  w_{11} &amp; w_{12} \\
  w_{21} &amp; w_{22} \\
  w_{31} &amp; w_{32} \\
  \end{bmatrix} \end{equation}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\( b \)</span> is the bias vector <span class="math notranslate nohighlight">\([b_1, b_2]\)</span>,</p></li>
<li><p>The dot product <span class="math notranslate nohighlight">\( \cdot \)</span> denotes matrix multiplication.</p></li>
</ul>
<p>The activation function introduces non-linearity to the model, allowing it to learn and represent complex relationships in the data. Some common choices of activation functions are sigmoid, tanh, relu, softmax, etc. The choice of activation function depends on the type and range of output desired, as well as the characteristics of the input data. For example, sigmoid is often used for binary classification, softmax is used for multi-class classification, relu is used for hidden layers, etc. Different activation functions can have different effects on the output of the layer, such as scaling, shifting, clipping, etc.</p>
<p>The following diagram illustrates the concept of a dense layer and how it connects to the previous and next layers:</p>
<p>I’ll try to create that.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> layer in TensorFlow is a fundamental building block in neural networks, particularly in fully connected or densely connected layers. It represents a layer where each neuron or unit is connected to every neuron in the previous layer. The <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer performs a weighted sum of its input, applies an activation function, and produces an output.</p>
<p><strong>Key Parameters</strong>:</p>
<ul class="simple">
<li><p><strong>units:</strong> The number of neurons in the layer.</p></li>
<li><p><strong>activation:</strong> The activation function applied to the layer’s output.</p></li>
<li><p><strong>use_bias:</strong> Boolean, indicating whether the layer uses a bias term.</p></li>
<li><p><strong>kernel_initializer:</strong> The initializer for the weights.</p></li>
<li><p><strong>bias_initializer:</strong> The initializer for the bias terms.</p></li>
</ul>
<p><font color='Blue'><b>Example:</b></font></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Assume flattened_input is a tensor representing the flattened output from a previous layer</span>
<span class="n">flattened_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>

<span class="c1"># Define a Dense layer with 4 units and &#39;relu&#39; activation</span>
<span class="n">dense_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>

<span class="c1"># Apply Dense to the flattened input</span>
<span class="n">dense_output</span> <span class="o">=</span> <span class="n">dense_layer</span><span class="p">(</span><span class="n">flattened_input</span><span class="p">)</span>

<span class="c1"># Display the original and dense layer output</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened Input Shape:&quot;</span><span class="p">,</span> <span class="n">flattened_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dense Output Shape:&quot;</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dense Output:&quot;</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Flattened Input Shape: (1, 3)
Dense Output Shape: (1, 4)
Dense Output: [[0.         0.         0.86555624 0.05669618]]
</pre></div>
</div>
</div>
</div>
<p>In this example, a <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer is defined with 4 units and ‘relu’ activation. The layer is then applied to a hypothetical flattened input tensor. The weights and biases are automatically initialized, and the activation function (‘relu’ in this case) is applied to the output. The output shape is (1, 4), indicating that there are 4 neurons in the layer. The output values are all zero, because the relu activation function clips any negative values to zero.</p>
<p>Note that the weight matrix for this example is a 3x4 matrix that contains the weights associated with each connection between the input and the output neurons. The weight matrix is randomly initialized when the layer is created, and it is updated during the training process to minimize the loss function. The weight matrix is one of the learnable parameters of the layer, along with the bias vector. You can access the weight matrix of the layer by using the <code class="docutils literal notranslate"><span class="pre">get_weights()</span></code> method, which returns a list of two elements: the weight matrix and the bias vector. For example, you can print the weight matrix as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the weights of the layer</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">dense_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># The first element of the list is the weight matrix</span>
<span class="n">weight_matrix</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Print the weight matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weight Matrix:&quot;</span><span class="p">,</span> <span class="n">weight_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weight Matrix: [[-0.20255494 -0.06871521  0.20407951  0.5212424 ]
 [ 0.0613094  -0.86144495 -0.44219363  0.8067714 ]
 [-0.5033858  -0.86037374  0.515288   -0.69269633]]
</pre></div>
</div>
</div>
</div>
<p>The weight matrix, denoted as (W), exhibits a dimensionality of (3, 4), signifying the presence of 3 input features and 4 output neurons within the layer. Each element within (W) encapsulates the weight of the connection linking an input feature to an output neuron. The computational process of the <code class="docutils literal notranslate"><span class="pre">dense_output</span></code> unfolds as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the input vector as a numpy array</span>
<span class="n">input_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="c1"># Define the weight matrix as a numpy array</span>
<span class="c1"># (Same as the aforementioned matrix)</span>

<span class="c1"># Define the bias vector as a numpy array</span>
<span class="n">bias_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The initial step involves computing the dot product between the input vector and the weight matrix, executed as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the dot product of the input vector and the weight matrix</span>
<span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">input_vector</span><span class="p">,</span> <span class="n">weight_matrix</span><span class="p">)</span>

<span class="c1"># Print the dot product</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dot Product:&quot;</span><span class="p">,</span> <span class="n">dot_product</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dot Product: [-1.59009349 -4.37272632  0.86555624  0.05669618]
</pre></div>
</div>
</div>
</div>
<p>Subsequently, the second step necessitates the addition of the bias term to each element of the vector. This is accomplished through the element-wise addition operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the bias term to each element of the vector</span>
<span class="n">biased_sum</span> <span class="o">=</span> <span class="n">dot_product</span> <span class="o">+</span> <span class="n">bias_vector</span>

<span class="c1"># Print the biased sum</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Biased Sum:&quot;</span><span class="p">,</span> <span class="n">biased_sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Biased Sum: [-1.59009349 -4.37272632  0.86555624  0.05669618]
</pre></div>
</div>
</div>
</div>
<p>Lastly, the third and final step involves the application of the activation function to each element of the vector. In this instance, the activation function is the rectified linear unit (ReLU), defined as the maximum of zero and the input value. The <code class="docutils literal notranslate"><span class="pre">np.maximum()</span></code> function facilitates this element-wise maximum operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the activation function to each element of the vector</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">biased_sum</span><span class="p">)</span>  <span class="c1"># relu</span>

<span class="c1"># Print the activation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Activation:&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Activation: [0.         0.         0.86555624 0.05669618]
</pre></div>
</div>
</div>
</div>
<p>The output of the layer is the final vector obtained after applying the activation function. The output shape is (4,), indicating that there are 4 neurons in the layer.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C12S08.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.8. </span>Multilayer Perceptron (MLP)</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C12S10.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12.10. </span>Image classification with TensorFlow</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-between-tensorflow-and-pytorch">12.9.1. Choice between TensorFlow and PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-cnn">12.9.2. Convolutional Neural Network (CNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rescaling-layer">12.9.2.1. Rescaling Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers">12.9.2.2. Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">12.9.2.3. Pooling Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-layers">12.9.2.4. Dropout Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flatten-layers">12.9.2.5. Flatten Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dense-layers-fully-connected-layers">12.9.2.6. Dense Layers (Fully Connected Layers)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>