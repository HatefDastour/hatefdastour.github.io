
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12.2. Fundamentals of Neural Networks &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_12/ENGG680_C12S02';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.3. TensorFlow Basics" href="ENGG680_C12S03.html" />
    <link rel="prev" title="12.1. Understanding Deep Learning" href="ENGG680_C12S01.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ENGG680_C09.html">9. An Introduction to Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S02.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S04.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ENGG680_C10.html">10. Tree-Based Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S02.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S03.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C12.html">12. Introduction to Deep Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S03.html">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fundamentals of Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-their-role-in-computational-modeling">12.2.1. Neurons and Their Role in Computational Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-elements-of-an-artificial-neuron">12.2.1.1. Key Elements of an Artificial Neuron:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-and-learning-in-neural-networks">12.2.1.2. Computation and Learning in Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">12.2.2. Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-activation-functions">12.2.2.1. The Role of Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-step-function">12.2.2.2. Binary Step function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">12.2.2.3. ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">12.2.2.4. Leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">12.2.2.5. Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent">12.2.2.6. Tanh (Hyperbolic Tangent)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">12.2.2.7. Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-linear-unit-elu">12.2.2.8. Exponential Linear Unit (ELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward-and-backpropagation-concepts">12.2.3. Feedforward and Backpropagation Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">12.2.3.1. Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">12.2.3.2. Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions-in-deep-learning">12.2.4. Loss Functions in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter">12.2.4.1. Why Loss Functions Matter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-functions-help-the-model">12.2.4.2. How Loss Functions Help the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-loss-functions">12.2.4.3. Popular Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-optimization-in-deep-learning">12.2.5. Training and Optimization in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-process-unraveling-the-steps">12.2.5.1. The Training Process: Unraveling the Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrating-the-process-a-flowchart-perspective">12.2.5.2. Illustrating the Process: A Flowchart Perspective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.2.6. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="fundamentals-of-neural-networks">
<h1><span class="section-number">12.2. </span>Fundamentals of Neural Networks<a class="headerlink" href="#fundamentals-of-neural-networks" title="Link to this heading">#</a></h1>
<section id="neurons-and-their-role-in-computational-modeling">
<h2><span class="section-number">12.2.1. </span>Neurons and Their Role in Computational Modeling<a class="headerlink" href="#neurons-and-their-role-in-computational-modeling" title="Link to this heading">#</a></h2>
<p>Neurons are like the building blocks of artificial neural networks (ANNs), and they’re inspired by how our brains work. Just like how our own brain cells process and share information, artificial neurons do a similar job. When it comes to ANNs, neurons act as the workers that take in data and turn it into something useful <span id="id1">[<a class="reference internal" href="../References.html#id3" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id141" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id215" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>. Think of these neurons as tiny processors. They take the information given to them, adjust its importance, do some calculations, and then give out a result. This process happens layer by layer, and it’s really good at finding patterns in data. This is why neural networks are great at things like recognizing pictures and understanding language. They’re like the brains behind some really smart computer programs.</p>
<section id="key-elements-of-an-artificial-neuron">
<h3><span class="section-number">12.2.1.1. </span>Key Elements of an Artificial Neuron:<a class="headerlink" href="#key-elements-of-an-artificial-neuron" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Inputs:</strong> Neurons receive data from the preceding layer or directly from the input dataset. Each piece of input data represents various attributes, features, or facets of the information under scrutiny.</p></li>
<li><p><strong>Weights:</strong> Think of weights as significance labels for the input data. These weights determine the importance of each piece of information in shaping the neuron’s output. During the learning process, these weights are adjusted to optimize the neuron’s predictive abilities.</p></li>
<li><p><strong>Bias:</strong> A bias serves as an additional factor that enhances the neuron’s adaptability and flexibility. Visualize it as an adjustable knob fine-tuning the neuron’s behavior, enabling it to better comprehend intricate relationships within the data.</p></li>
<li><p><strong>Summation:</strong> All the weighted inputs and the bias are aggregated, much like assembling puzzle pieces to reveal a larger picture. This summation, executed in a specific manner, illustrates how all the individual input components interrelate.</p></li>
<li><p><strong>Activation Function:</strong> The aggregated result from the previous step undergoes an activation function. This function determines the neuron’s response pattern, akin to its “disposition.” The disposition can range from positive to negative or fall somewhere in between, playing a crucial role in processing diverse data patterns.</p></li>
<li><p><strong>Output:</strong> The activation function yields the neuron’s “decision” or interpretation of the data. It articulates, “I believe this is the meaning of the data.” This decision can then propagate to other neurons, progressively constructing a deeper comprehension of the information. Alternatively, if it belongs to the final layer, it can constitute the network’s ultimate conclusion or prediction.</p></li>
</ol>
<p>In essence, an artificial neuron operates as a miniature decision-maker, ingesting information, evaluating its importance, and delivering its perspective on the matter. When numerous neurons collaborate, they exhibit remarkable capabilities, such as pattern recognition and deciphering intricate data structures <span id="id2">[<a class="reference internal" href="../References.html#id3" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id141" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id215" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>.</p>
<!-- ### Components of an Artificial Neuron:

1. **Inputs:** Neurons take in information from the layer before them or directly from the data being worked with. These bits of information can stand for different characteristics, qualities, or parts of the data under consideration.

2. **Weights:** Think of weights as labels that say how important each piece of information is. These labels matter because they decide how much each input contributes to what the neuron ultimately says. When the network learns, it adjusts these labels to make the best predictions.

3. **Bias:** A bias is like a little extra something added to the mix. It helps the neuron adapt and be more flexible. Imagine it as an adjustment knob that fine-tunes the neuron's behavior, making it better at understanding complicated connections.

4. **Summation:** Now, all the weighted inputs and the bias get added up together. It's kind of like combining puzzle pieces to get a bigger picture. This addition, done in a certain way, shows how all the input parts fit together.

5. **Activation Function:** The summed-up result from before gets sent through an activation function. This function gives the neuron a certain way of reacting. It's like the neuron's "mood." This mood can be positive, negative, or somewhere in between, and it's important for dealing with all sorts of data patterns.

6. **Output:** The activation function gives us the neuron's "decision." It says, "Okay, I think this is what the data means." This decision can then pass on to other neurons, step by step, building up a bigger understanding of the data. Or, if it's the final layer, it can be the network's ultimate answer or prediction.

In a nutshell, an artificial neuron is like a little decision-maker that takes in information, weighs it up, and gives its own take on what's going on. When lots of these neurons work together, they can do some pretty clever stuff, like recognizing patterns and making sense out of complex data {cite:p}`mehlig2021machine,ye2022geometry,aggarwal2023neural`. -->
<figure class="align-center" id="id53">
<a class="reference internal image-reference" href="../_images/Elements_ANN.jpg"><img alt="../_images/Elements_ANN.jpg" src="../_images/Elements_ANN.jpg" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.1 </span><span class="caption-text">The main usage of the Activation Function is to convert the aggregated and weighted input from a node into an output value that can be propagated to the next hidden layer or used as an output.</span><a class="headerlink" href="#id53" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><font color='Blue'><b>Example:</b></font> A linear transformation stands as a pivotal building block, forming the basis upon which more complex architectures are constructed. At its core, a linear transformation takes input data and performs a weighted combination of its features, ultimately yielding an output. This process is encapsulated by the mathematical equation <span class="math notranslate nohighlight">\(z = W.X + B\)</span>, where <span class="math notranslate nohighlight">\(z\)</span> signifies the resulting output <span id="id3">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<p><strong>Components of a Linear Transformation:</strong></p>
<ul class="simple">
<li><p><strong>Output (<span class="math notranslate nohighlight">\(z\)</span>):</strong>
The output <span class="math notranslate nohighlight">\(z\)</span> represents the result of the linear transformation. It encapsulates the synthesized information that arises from the combination of input features according to learned weights and biases. While this transformation might seem simplistic, its elegance lies in its foundation—a direct linear relationship between inputs and outputs <span id="id4">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Weight Matrix (<span class="math notranslate nohighlight">\(W\)</span>)</strong>
The weight matrix <span class="math notranslate nohighlight">\(W\)</span> is the crux of the linear transformation. This matrix encompasses the learned weights assigned to each feature in the input data. In essence, it determines the influence that each feature wields in the process of generating the output. The arrangement of these weights in the matrix permits a holistic view of the interplay between input features and their corresponding importance <span id="id5">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Input Matrix (<span class="math notranslate nohighlight">\(X\)</span>):</strong>
The input matrix <span class="math notranslate nohighlight">\(X\)</span> forms the bedrock of the transformation. Comprising rows of input samples and columns of distinct features, <span class="math notranslate nohighlight">\(X\)</span> encapsulates the raw information that the model processes. Each column of the matrix corresponds to a specific feature, and each row corresponds to an individual sample. This representation allows the linear transformation to operate simultaneously on multiple samples, making it a vectorized and efficient computation <span id="id6">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
<li><p><strong>Bias Matrix (<span class="math notranslate nohighlight">\(B\)</span>):</strong>
While the weight matrix captures the interactions between input features, the bias matrix <span class="math notranslate nohighlight">\(B\)</span> introduces an additional degree of freedom. It is a vector that is added element-wise to the weighted sum of features before producing the final output. This term permits the model to account for inherent offsets or baseline values that are not directly captured by the weighted features alone <span id="id7">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p></li>
</ul>
</section>
<section id="computation-and-learning-in-neural-networks">
<h3><span class="section-number">12.2.1.2. </span>Computation and Learning in Neural Networks<a class="headerlink" href="#computation-and-learning-in-neural-networks" title="Link to this heading">#</a></h3>
<p>The inner workings of an artificial neuron closely emulate the information processing mechanisms of biological neurons. Yet, the true potential of artificial neurons is unlocked when they are orchestrated into layered configurations, interconnected to establish a neural network. Within this network, neurons cooperate in a concerted effort to acquire the ability to convert raw input data into coherent and meaningful representations. This collective learning process equips the network to make predictions and execute various tasks founded upon the acquired knowledge of patterns <span id="id8">[<a class="reference internal" href="../References.html#id3" title="C.C. Aggarwal. Neural Networks and Deep Learning: A Textbook. Springer International Publishing, 2023. ISBN 9783031296420. URL: https://books.google.ca/books?id=0-rIEAAAQBAJ.">Aggarwal, 2023</a>, <a class="reference internal" href="../References.html#id141" title="B. Mehlig. Machine Learning with Neural Networks: An Introduction for Scientists and Engineers. Cambridge University Press, 2021. ISBN 9781108494939. URL: https://books.google.ca/books?id=cE49zgEACAAJ.">Mehlig, 2021</a>, <a class="reference internal" href="../References.html#id215" title="J.C. Ye. Geometry of Deep Learning: A Signal Processing Perspective. Mathematics in Industry. Springer Nature Singapore, 2022. ISBN 9789811660467. URL: https://books.google.ca/books?id=fd5XEAAAQBAJ.">Ye, 2022</a>]</span>.</p>
</section>
</section>
<section id="activation-functions">
<h2><span class="section-number">12.2.2. </span>Activation Functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h2>
<p>As we journey from simple linear models to the complex landscapes of deep learning, activation functions step forward as vital agents of transformation. Their essence lies in imparting neural networks with the power to capture non-linear phenomena, opening pathways for representing intricate data relationships. By transcending the boundaries of linear operations, activation functions unlock the network’s potential to tackle tasks that involve subtle patterns and complex interactions.</p>
<!-- Activation functions introduce non-linearity to the neural network, enabling it to capture complex patterns in data. Some common activation functions include: -->
<section id="the-role-of-activation-functions">
<h3><span class="section-number">12.2.2.1. </span>The Role of Activation Functions<a class="headerlink" href="#the-role-of-activation-functions" title="Link to this heading">#</a></h3>
<p>Deep learning activation functions are mathematical functions that are applied to the output of a layer of a neural network. They determine how the output of the layer is transformed into the input for the next layer, or the final prediction of the model. Activation functions are essential for deep learning because they introduce non-linearity into the network, which allows it to learn complex patterns from data <span id="id9">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>.</p>
<figure class="align-center" id="id54">
<a class="reference internal image-reference" href="../_images/Elements_ANN.jpg"><img alt="../_images/Elements_ANN.jpg" src="../_images/Elements_ANN.jpg" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.2 </span><span class="caption-text">The main usage of the Activation Function is to convert the aggregated and weighted input from a node into an output value that can be propagated to the next hidden layer or used as an output.</span><a class="headerlink" href="#id54" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There are many types of activation functions that have different properties and effects on the network performance. Some of the most common activation functions are:</p>
</section>
<section id="binary-step-function">
<h3><span class="section-number">12.2.2.2. </span>Binary Step function<a class="headerlink" href="#binary-step-function" title="Link to this heading">#</a></h3>
<p>The Binary Step function is a simple activation function used in binary classification tasks, where the output of a neuron or a model needs to be binary, often representing classes like 0 and 1. Mathematically, the Binary Step function is defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-db411931-ff91-4719-9df4-cf23282eb34c">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-db411931-ff91-4719-9df4-cf23282eb34c" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
0, &amp; \text{if } x &lt; 0 \\
1, &amp; \text{if } x \geq 0
\end{cases}
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x\)</span> is the input to the function. The Binary Step function returns 0 if the input is less than 0, and it returns 1 if the input is greater than or equal to 0. It essentially provides a threshold-based output where any positive input is mapped to the value 1 and any negative input is mapped to the value 0.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../mystyle.mplstyle&#39;</span><span class="p">)</span>

<span class="c1"># Define the Binary Step function</span>
<span class="k">def</span> <span class="nf">binary_step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Binary Step function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">binary_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Binary Step&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Binary Step Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/9aac8a24821b6add39bf2715a4ec65fabd9a0ab5f35605b17862a806fe124e0b.png" src="../_images/9aac8a24821b6add39bf2715a4ec65fabd9a0ab5f35605b17862a806fe124e0b.png" />
</div>
</div>
</section>
<section id="relu-rectified-linear-unit">
<h3><span class="section-number">12.2.2.3. </span>ReLU (Rectified Linear Unit)<a class="headerlink" href="#relu-rectified-linear-unit" title="Link to this heading">#</a></h3>
<p>The Rectified Linear Unit, or ReLU for short, is a simple but powerful activation function that is widely used in deep learning models. It is defined as a piecewise linear function that outputs zero for negative inputs and the input itself for positive inputs. Mathematically, the ReLU function is expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5a7052b8-ab3a-41ba-87f8-5eb40031b319">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-5a7052b8-ab3a-41ba-87f8-5eb40031b319" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \max(0, x)\end{equation}\]</div>
<p>ReLU has several advantages over other activation functions, such as sigmoid or tanh. First, it is easy to compute and implement, which reduces the computational cost and complexity of the model. Second, it helps to mitigate the vanishing gradient problem, which occurs when the gradients of the activation function become too small to effectively update the weights of the model. This problem can hamper the learning process and lead to poor performance. ReLU avoids this issue by having a constant gradient of one for positive inputs, which ensures a steady flow of error signals during backpropagation. Third, it introduces non-linearity and sparsity into the model, which can enhance the expressive power and generalization ability of the model. ReLU can create sparse representations by setting some of the outputs to zero, which can reduce the correlation and redundancy among the features <span id="id10">[<a class="reference internal" href="../References.html#id60" title="Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep learning: a comprehensive survey and benchmark. Neurocomputing, 503:92-108, 2022. doi:10.1016/j.neucom.2022.06.111.">Dubey <em>et al.</em>, 2022</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id89" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 1026–1034. 2015.">He <em>et al.</em>, 2015</a>]</span>.</p>
<p>However, ReLU is not without drawbacks. One of the main challenges of using ReLU is the dying ReLU problem, which occurs when some of the neurons become inactive and stop producing any output. This can happen when the inputs to the ReLU function are negative for a long period of time, which causes the gradients to be zero and prevents any weight updates. As a result, the neurons can get stuck in a state where they always output zero, regardless of the input. This can reduce the effective capacity of the model and lead to poor performance. Several variants of ReLU have been proposed to address this problem, such as Leaky ReLU, Parametric ReLU, and Exponential Linear Unit (ELU) <span id="id11">[<a class="reference internal" href="../References.html#id33" title="Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.">Clevert <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id89" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 1026–1034. 2015.">He <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id129" title="Andrew L Maas, Awni Y Hannun, Andrew Y Ng, and others. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, 3. Atlanta, GA, 2013.">Maas <em>et al.</em>, 2013</a>]</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the ReLU function</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the ReLU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;ReLU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/28ecef1f280f2b9cff4bd2ecef11f60b38c6e10c57d2344336711c55fc4e63ad.png" src="../_images/28ecef1f280f2b9cff4bd2ecef11f60b38c6e10c57d2344336711c55fc4e63ad.png" />
</div>
</div>
</section>
<section id="leaky-relu">
<h3><span class="section-number">12.2.2.4. </span>Leaky ReLU<a class="headerlink" href="#leaky-relu" title="Link to this heading">#</a></h3>
<p>Leaky ReLU is a variant of the Rectified Linear Unit (ReLU) activation function that aims to overcome the drawbacks of the standard ReLU function. Unlike ReLU, which outputs zero for negative inputs, Leaky ReLU allows a small, non-zero output for negative inputs. This can prevent the neurons from becoming inactive and improve the performance of the neural network. Mathematically, Leaky ReLU is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ca73f5ff-df0d-4015-ad0c-75a6f3376594">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-ca73f5ff-df0d-4015-ad0c-75a6f3376594" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
x, &amp; \text{if } x &gt; 0 \\
\alpha x, &amp; \text{if } x \leq 0
\end{cases}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a small positive constant (typically 0.01). The function can be interpreted as follows:</p>
<ul class="simple">
<li><p>For positive inputs, Leaky ReLU behaves like the identity function, meaning it preserves the input value without any change.</p></li>
<li><p>For negative inputs, Leaky ReLU scales the input by a factor of <span class="math notranslate nohighlight">\(\alpha\)</span>, resulting in a linear function with a slope of <span class="math notranslate nohighlight">\(\alpha\)</span>. This ensures that the output is not zero and the gradient can flow through the function.</p></li>
</ul>
<p>Leaky ReLU has several benefits over ReLU, such as <span id="id12">[<a class="reference internal" href="../References.html#id129" title="Andrew L Maas, Awni Y Hannun, Andrew Y Ng, and others. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, 3. Atlanta, GA, 2013.">Maas <em>et al.</em>, 2013</a>, <a class="reference internal" href="../References.html#id212" title="Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.">Xu <em>et al.</em>, 2015</a>]</span>:</p>
<ul class="simple">
<li><p>It avoids the dying ReLU problem, where some neurons can stop learning due to zero gradients. By allowing a small output for negative inputs, Leaky ReLU ensures that the neurons remain active and responsive to the input variations.</p></li>
<li><p>It introduces a slight asymmetry and non-linearity into the function, which can enhance the expressive power and generalization ability of the neural network. Leaky ReLU can create sparse and diverse representations by setting some of the outputs to a small value, which can reduce the correlation and redundancy among the features.</p></li>
<li><p>It is easy to implement and computationally efficient, which makes it suitable for large-scale and complex models.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Leaky ReLU function</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Leaky ReLU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Leaky ReLU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Leaky ReLU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/82145422731eb5b8c944b16c81ea88e457eb46927e38189e6c5c9171b1c0316a.png" src="../_images/82145422731eb5b8c944b16c81ea88e457eb46927e38189e6c5c9171b1c0316a.png" />
</div>
</div>
</section>
<section id="sigmoid-function">
<h3><span class="section-number">12.2.2.5. </span>Sigmoid Function<a class="headerlink" href="#sigmoid-function" title="Link to this heading">#</a></h3>
<p>The Sigmoid function is a classic activation function that is widely used in machine learning and deep learning models. It is a smooth and continuous function that maps any real-valued input to a value between 0 and 1. This property makes it suitable for tasks that require output probabilities, such as binary classification or logistic regression. Mathematically, the Sigmoid function is expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4af21fe1-ddf1-4f31-9044-9ab0e0d75672">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-4af21fe1-ddf1-4f31-9044-9ab0e0d75672" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \frac{1}{1 + e^{-x}}\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(e\)</span> is the base of the natural logarithm. The function can be interpreted as follows:</p>
<ul class="simple">
<li><p>For large positive inputs, the Sigmoid function approaches 1, meaning it assigns a high probability to the positive class.</p></li>
<li><p>For large negative inputs, the Sigmoid function approaches 0, meaning it assigns a low probability to the positive class.</p></li>
<li><p>For inputs close to zero, the Sigmoid function outputs a value close to 0.5, meaning it assigns an equal probability to both classes.</p></li>
</ul>
<p>The Sigmoid function has several advantages, such as <span id="id13">[<a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>:</p>
<ul class="simple">
<li><p>It is easy to implement and understand, which makes it a popular choice for beginners and practitioners.</p></li>
<li><p>It is differentiable and has a simple derivative, which facilitates the gradient-based optimization methods.</p></li>
<li><p>It introduces non-linearity and saturation into the model, which can enhance the expressive power and robustness of the model. Sigmoid can create bounded and smooth representations by compressing the input range to a fixed interval, which can reduce the sensitivity and variance of the output.</p></li>
</ul>
<p>However, the Sigmoid function also has some drawbacks, such as <span id="id14">[<a class="reference internal" href="../References.html#id79" title="Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, 315–323. JMLR Workshop and Conference Proceedings, 2011.">Glorot <em>et al.</em>, 2011</a>, <a class="reference internal" href="../References.html#id111" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.">Krizhevsky <em>et al.</em>, 2017</a>, <a class="reference internal" href="../References.html#id151" title="Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), 807–814. 2010.">Nair and Hinton, 2010</a>]</span>:</p>
<ul class="simple">
<li><p>It suffers from the vanishing gradient problem, which occurs when the gradients of the activation function become too small to effectively update the weights of the model. This problem can hamper the learning process and lead to poor performance. Sigmoid is prone to this issue because it has a very flat region at both ends of the curve, where the gradient is almost zero.</p></li>
<li><p>It is not zero-centered, meaning it does not have a mean of zero. This can cause a shift in the distribution of the inputs to the next layer, which can affect the learning dynamics and convergence of the model. Sigmoid can create unbalanced and biased representations by shifting the input mean to a positive value, which can increase the correlation and redundancy among the features.</p></li>
<li><p>It is computationally expensive, meaning it requires more time and resources to calculate and evaluate. This can limit the scalability and efficiency of the model. Sigmoid involves an exponential operation, which is more costly than a linear or piecewise linear operation.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Sigmoid function</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Sigmoid function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sigmoid&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Sigmoid Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/645c58193a769e004019360828b3bfbd429409d7a402fdceecc84acb2c62cada.png" src="../_images/645c58193a769e004019360828b3bfbd429409d7a402fdceecc84acb2c62cada.png" />
</div>
</div>
</section>
<section id="tanh-hyperbolic-tangent">
<h3><span class="section-number">12.2.2.6. </span>Tanh (Hyperbolic Tangent)<a class="headerlink" href="#tanh-hyperbolic-tangent" title="Link to this heading">#</a></h3>
<p>The Hyperbolic Tangent, or Tanh for short, is a smooth and continuous activation function that is widely used in machine learning and deep learning models. It is a scaled and shifted version of the Sigmoid function, meaning it maps any real-valued input to a value between -1 and 1. This property makes it suitable for tasks that require output values to be centered around zero, such as regression or sentiment analysis. Mathematically, the Tanh function is expressed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1cea6ff0-c2f1-41f3-b05e-faf02cb9e587">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-1cea6ff0-c2f1-41f3-b05e-faf02cb9e587" title="Permalink to this equation">#</a></span>\[\begin{equation}\varphi(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(e\)</span> is the base of the natural logarithm. The function can be interpreted as follows:</p>
<ul class="simple">
<li><p>For large positive inputs, the Tanh function approaches 1, meaning it assigns a high positive value to the input.</p></li>
<li><p>For large negative inputs, the Tanh function approaches -1, meaning it assigns a high negative value to the input.</p></li>
<li><p>For inputs close to zero, the Tanh function outputs a value close to zero, meaning it assigns a neutral value to the input.</p></li>
</ul>
<p>The Tanh function has several advantages over the Sigmoid function, such as <span id="id15">[<a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span>:</p>
<ul class="simple">
<li><p>It is zero-centered, meaning it has a mean of zero. This can prevent a shift in the distribution of the inputs to the next layer, which can improve the learning dynamics and convergence of the model. Tanh can create balanced and symmetrical representations by mapping the input range to a symmetric interval, which can reduce the correlation and redundancy among the features.</p></li>
<li><p>It is steeper than the Sigmoid function, meaning it has a larger gradient for a given input. This can speed up the learning process and lead to better performance. Tanh can create more distinct and diverse representations by separating the input values more clearly, which can increase the sensitivity and variance of the output.</p></li>
</ul>
<p>However, the Tanh function also has some drawbacks, such as <span id="id16">[<a class="reference internal" href="../References.html#id11" title="Y. Bengio. Learning Deep Architectures for AI. Foundations and trends in machine learning. Now, 2009. ISBN 9781601982940. URL: https://books.google.ca/books?id=cq5ewg7FniMC.">Bengio, 2009</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>:</p>
<ul class="simple">
<li><p>It still suffers from the vanishing gradient problem, although to a lesser extent than the Sigmoid function. This occurs when the gradients of the activation function become too small to effectively update the weights of the model. This problem can hamper the learning process and lead to poor performance. Tanh is prone to this issue because it still has a very flat region at both ends of the curve, where the gradient is almost zero.</p></li>
<li><p>It is not optimal for tasks that require output probabilities, such as binary classification or logistic regression. This is because it outputs values between -1 and 1, which are not interpretable as probabilities. Tanh can create ambiguous and misleading representations by assigning negative values to some of the inputs, which can confuse the model and the user.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Hyperbolic Tangent (Tanh) function</span>
<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Tanh function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Tanh&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Tanh Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/b408a1072a7f80deb6541442692ccf42822ad36f578c16718f50bf2af4004a83.png" src="../_images/b408a1072a7f80deb6541442692ccf42822ad36f578c16718f50bf2af4004a83.png" />
</div>
</div>
</section>
<section id="softmax">
<h3><span class="section-number">12.2.2.7. </span>Softmax<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h3>
<p>The Softmax function is a popular activation function that is often used in the output layer of a neural network. It is a generalization of the logistic function, meaning it can handle multiple classes instead of just two. It is also known as the normalized exponential function, because it normalizes the input values by using the exponential function. Mathematically, the Softmax function is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-06d1eca9-710f-4217-8e62-9724af860e64">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-06d1eca9-710f-4217-8e62-9724af860e64" title="Permalink to this equation">#</a></span>\[\begin{equation}
p_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i\)</span> is the output probability for the <span class="math notranslate nohighlight">\(i\)</span>-th class, <span class="math notranslate nohighlight">\(x_i\)</span> is the input score or logit for the <span class="math notranslate nohighlight">\(i\)</span>-th class, <span class="math notranslate nohighlight">\(e\)</span> is the base of the natural logarithm, and <span class="math notranslate nohighlight">\(n\)</span> is the total number of classes. The function can be interpreted as follows:</p>
<ul class="simple">
<li><p>For each class, the Softmax function computes the exponential of the input score, which can amplify the difference between the scores and make the largest score more dominant.</p></li>
<li><p>For each class, the Softmax function divides the exponential of the input score by the sum of the exponentials of all the input scores, which can ensure that the output probabilities sum up to one and form a valid probability distribution.</p></li>
<li><p>The Softmax function outputs a vector of probabilities, where each element represents the likelihood of the input belonging to a certain class. The class with the highest probability is the predicted class for the input.</p></li>
</ul>
<p>The Softmax function has several advantages, such as <span id="id17">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>:</p>
<ul class="simple">
<li><p>It is differentiable and has a simple derivative, which facilitates the gradient-based optimization methods.</p></li>
<li><p>It is compatible with the cross-entropy loss function, which is a common choice for measuring the discrepancy between the predicted probabilities and the true labels.</p></li>
<li><p>It is invariant to scaling, meaning it does not change the output probabilities if the input scores are multiplied by a constant factor. This can prevent numerical issues and improve the stability of the model.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the Softmax function</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Subtracting the maximum value for numerical stability</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the Softmax function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Softmax&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Softmax Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/0b5defaf43fbf04363965019ee9c72d1ab3a15e70284b00e377b161403e8ce05.png" src="../_images/0b5defaf43fbf04363965019ee9c72d1ab3a15e70284b00e377b161403e8ce05.png" />
</div>
</div>
</section>
<section id="exponential-linear-unit-elu">
<h3><span class="section-number">12.2.2.8. </span>Exponential Linear Unit (ELU)<a class="headerlink" href="#exponential-linear-unit-elu" title="Link to this heading">#</a></h3>
<p>The Exponential Linear Unit, or ELU for short, is a novel activation function that was proposed to improve the performance and robustness of neural networks. It is a modified version of the Rectified Linear Unit (ReLU) function, meaning it retains the advantages of ReLU while addressing some of its drawbacks. Mathematically, ELU is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0c436aba-d2c6-4833-887e-983c8a73b5bb">
<span class="eqno">(12.7)<a class="headerlink" href="#equation-0c436aba-d2c6-4833-887e-983c8a73b5bb" title="Permalink to this equation">#</a></span>\[\begin{equation}
\varphi(x) = \begin{cases}
x, &amp; \text{if } x &gt; 0 \\
\alpha (e^x - 1), &amp; \text{if } x \leq 0
\end{cases}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the function, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a positive constant (typically 1) that controls the behavior of the function for negative inputs. The function can be interpreted as follows:</p>
<ul class="simple">
<li><p>For positive inputs, ELU behaves like the identity function, meaning it preserves the input value without any change.</p></li>
<li><p>For negative inputs, ELU outputs a small negative value that depends on the exponential of the input, resulting in a smooth and non-linear function with a slope of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
</ul>
<p>ELU has several benefits over ReLU, such as <span id="id18">[<a class="reference internal" href="../References.html#id33" title="Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.">Clevert <em>et al.</em>, 2015</a>]</span>:</p>
<ul class="simple">
<li><p>It avoids the dying ReLU problem, where some neurons can stop learning due to zero outputs and gradients. By allowing a small negative output for negative inputs, ELU ensures that the neurons remain active and responsive to the input variations.</p></li>
<li><p>It is closer to zero-centered, meaning it has a mean closer to zero. This can prevent a shift in the distribution of the inputs to the next layer, which can improve the learning dynamics and convergence of the model. ELU can create more balanced and stable representations by reducing the input bias to a negative value, which can decrease the correlation and redundancy among the features.</p></li>
<li><p>It has a faster learning rate, meaning it can achieve better results in less time and with fewer resources. This is because ELU has a larger and more consistent gradient for both positive and negative inputs, which can accelerate the gradient descent process and reduce the overfitting risk.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the ELU function</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply the ELU function to x values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create the plot using subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ELU&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\varphi(x)$&#39;</span><span class="p">,</span>
           <span class="n">title</span><span class="o">=</span><span class="s1">&#39;ELU Activation Function&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/a91e6a328e4a0ebebc8bb95ffbb6e82f9c561c00070c81ff5e262f38bc21b703.png" src="../_images/a91e6a328e4a0ebebc8bb95ffbb6e82f9c561c00070c81ff5e262f38bc21b703.png" />
</div>
</div>
</section>
</section>
<section id="feedforward-and-backpropagation-concepts">
<h2><span class="section-number">12.2.3. </span>Feedforward and Backpropagation Concepts<a class="headerlink" href="#feedforward-and-backpropagation-concepts" title="Link to this heading">#</a></h2>
<p>Neural networks, like the intricate workings of our own brains, have a central role in modern machine learning and artificial intelligence. Their ability to process information and learn hinges on the coordinated interplay of two key steps: feedforward and backpropagation.</p>
<section id="feedforward">
<h3><span class="section-number">12.2.3.1. </span>Feedforward<a class="headerlink" href="#feedforward" title="Link to this heading">#</a></h3>
<p><strong>Feedforward</strong>, much like how our senses process information, takes the first step in a neural network’s journey. This process involves these important stages <span id="id19">[<a class="reference internal" href="../References.html#id67" title="Okan Erkaymaz. Resilient back-propagation approach in small-world feed-forward neural network topology based on newman–watts algorithm. Neural Computing and Applications, 32(20):16279-16289, Oct 2020. doi:10.1007/s00521-020-05161-6.">Erkaymaz, 2020</a>, <a class="reference internal" href="../References.html#id136" title="M. Maynard. Neural Networks: Introduction to Artificial Neurons, Backpropagation and Multilayer Feedforward Neural Networks with Real-World Applications. Advanced Data Analytics. Independently Published, 2020. ISBN 9798642783528. URL: https://books.google.ca/books?id=82F5zQEACAAJ.">Maynard, 2020</a>, <a class="reference internal" href="../References.html#id217" title="Xinghuo Yu, M Onder Efe, and Okyay Kaynak. A general backpropagation algorithm for feedforward neural networks learning. IEEE transactions on neural networks, 13(1):251–254, 2002.">Yu <em>et al.</em>, 2002</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Input Layer:</strong> At the beginning, the input data resides in the input layer. Each neuron here represents a specific feature or characteristic of the data.</p></li>
<li><p><strong>Hidden Layers:</strong> The real magic unfolds as the data travels through hidden layers. Neurons in these layers interpret inputs from the prior layer, assigning them weights and passing them through an activation function. This transformation introduces non-linear changes, allowing the network to decipher intricate data relationships.</p></li>
<li><p><strong>Output Layer:</strong> The journey concludes in the output layer, where transformed data takes its final shape—prediction. In tasks like classification, each neuron embodies a class, with the one activated most indicating the network’s decision.</p></li>
</ol>
<p>This dance between feedforward and backpropagation embodies the essence of neural network learning, propelling machines to understand, learn, and predict much like our own minds do.</p>
<figure class="align-center" id="id55">
<a class="reference internal image-reference" href="../_images/DeepNeuralNetwork.jpg"><img alt="../_images/DeepNeuralNetwork.jpg" src="../_images/DeepNeuralNetwork.jpg" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.3 </span><span class="caption-text">An example of a Neural Metwork: Input, hidden, and output layers.</span><a class="headerlink" href="#id55" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="backpropagation">
<h3><span class="section-number">12.2.3.2. </span>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h3>
<p><strong>Backpropagation</strong> is a powerful algorithm that enables a neural network to learn from its own mistakes and improve its performance over time. It works in tandem with the feedforward process, which passes the input data through the network and generates the output predictions. Backpropagation follows these steps <span id="id20">[<a class="reference internal" href="../References.html#id67" title="Okan Erkaymaz. Resilient back-propagation approach in small-world feed-forward neural network topology based on newman–watts algorithm. Neural Computing and Applications, 32(20):16279-16289, Oct 2020. doi:10.1007/s00521-020-05161-6.">Erkaymaz, 2020</a>, <a class="reference internal" href="../References.html#id136" title="M. Maynard. Neural Networks: Introduction to Artificial Neurons, Backpropagation and Multilayer Feedforward Neural Networks with Real-World Applications. Advanced Data Analytics. Independently Published, 2020. ISBN 9798642783528. URL: https://books.google.ca/books?id=82F5zQEACAAJ.">Maynard, 2020</a>, <a class="reference internal" href="../References.html#id217" title="Xinghuo Yu, M Onder Efe, and Okyay Kaynak. A general backpropagation algorithm for feedforward neural networks learning. IEEE transactions on neural networks, 13(1):251–254, 2002.">Yu <em>et al.</em>, 2002</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Loss Calculation:</strong> The first step is to measure how well the network is doing by comparing the output predictions with the actual target values. This comparison is done by using a loss function, which quantifies the difference or error between the two. The loss function acts as a feedback signal, indicating how much the network needs to adjust its parameters (weights and biases) to reduce the error.</p></li>
<li><p><strong>Gradient Descent:</strong> The second step is to find the direction and magnitude of the adjustment for each parameter. This is done by using the gradient, which is the derivative or slope of the loss function with respect to the parameter. The gradient tells us how the loss function changes when we change the parameter slightly. By following the negative gradient, we can find the optimal value of the parameter that minimizes the loss function.</p></li>
<li><p><strong>Weight Updates:</strong> The third step is to apply the adjustment to each parameter according to the gradient. This is done by using a learning rate, which is a small positive constant that controls the size of the adjustment. The learning rate determines how fast or slow the network learns. By multiplying the gradient by the learning rate, we can obtain the amount of change for each parameter. By subtracting this change from the current value of the parameter, we can update the parameter to a new value that reduces the error.</p></li>
<li><p><strong>Iterative Process:</strong> The final step is to repeat the above steps for each input-output pair in the data set, and for multiple rounds or epochs. This is an iterative process that gradually improves the network’s accuracy and generalization. With each iteration, the network’s predictions become closer to the target values. The loss function decreases, reflecting the network’s progress and performance.</p></li>
</ol>
<p>Through this iterative process of feedforward and backpropagation, the neural network learns to recognize the patterns and relationships hidden in the data. This learning process enables the network to perform various tasks, such as classification, regression, image recognition, natural language processing, and more. As the network learns, it becomes more intelligent and capable, transforming itself from a simple mathematical model to a powerful artificial intelligence system.</p>
</section>
</section>
<section id="loss-functions-in-deep-learning">
<h2><span class="section-number">12.2.4. </span>Loss Functions in Deep Learning<a class="headerlink" href="#loss-functions-in-deep-learning" title="Link to this heading">#</a></h2>
<p>Loss functions are essential components of deep learning models, as they provide a way to measure the performance and guide the learning process of the neural networks. They are mathematical functions that compare the output predictions of the network with the actual target values of the data, and quantify the difference or error between them. This error reflects how well or poorly the network is doing its job, and serves as a feedback signal that the network uses to adjust its parameters (weights and biases) and improve its accuracy and generalization <span id="id21">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>.</p>
<section id="why-loss-functions-matter">
<h3><span class="section-number">12.2.4.1. </span>Why Loss Functions Matter<a class="headerlink" href="#why-loss-functions-matter" title="Link to this heading">#</a></h3>
<p>Deep learning is the process of creating models that can learn from data and make predictions for new, unseen data. A loss function is a crucial component of this process, as it provides a way to evaluate the quality and accuracy of the model’s predictions. A loss function compares the model’s output predictions with the actual target values of the data, and calculates the difference or error between them. This error reflects how well or poorly the model is performing its task, and serves as a feedback signal that the model uses to improve itself. By using a loss function, the model can adjust its parameters (weights and biases) based on the error, and try to minimize it. Therefore, a loss function acts as a guide that helps the model move in the right direction and achieve better results <span id="id22">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>.</p>
</section>
<section id="how-loss-functions-help-the-model">
<h3><span class="section-number">12.2.4.2. </span>How Loss Functions Help the Model<a class="headerlink" href="#how-loss-functions-help-the-model" title="Link to this heading">#</a></h3>
<p>Loss functions are essential components of deep learning models, as they provide a way to measure the performance and guide the learning process of the neural networks. They are mathematical functions that compare the output predictions of the network with the actual target values of the data, and quantify the difference or error between them. This error reflects how well or poorly the network is performing its task, and serves as a feedback signal that the network uses to improve itself. By using a loss function, the network can adjust its parameters (weights and biases) based on the error, and try to minimize it. Therefore, a loss function acts as a guide that helps the network move in the right direction and achieve better results. <span id="id23">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p>
<p>During training, the network keeps changing its parameters based on the feedback from the loss function. This is done by using an optimization algorithm, such as gradient descent, which calculates the gradient of the loss function with respect to the parameters, and updates the parameters in the opposite direction of the gradient. This process helps the network reduce the error and increase the accuracy of its predictions. As the network trains and updates itself, it gets closer and closer to making predictions that match the target values. <span id="id24">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p>
<p>In this way, loss functions help models improve over time and learn from their own mistakes. They are crucial for the success and performance of deep learning models, as they define the objective and the direction of the learning process. Different types of loss functions are suitable for different types of tasks and outputs, such as regression, classification, image recognition, natural language processing, and more. <span id="id25">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>.</p>
</section>
<section id="popular-loss-functions">
<h3><span class="section-number">12.2.4.3. </span>Popular Loss Functions<a class="headerlink" href="#popular-loss-functions" title="Link to this heading">#</a></h3>
<p>Loss functions are mathematical functions that measure the difference or error between the output predictions of a neural network and the actual target values of the data. They provide a way to evaluate the performance and guide the learning process of the network. Different types of loss functions are suitable for different types of tasks and outputs, such as regression, classification, image recognition, natural language processing, and more. Some of the most common loss functions are <span id="id26">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>Mean Squared Error (MSE)</strong>: This loss function is applied to regression tasks, where the aim is to predict continuous numerical values. It calculates the average of the squared differences between the predicted and actual values. Mathematically, it’s expressed as <span id="id27">[<a class="reference internal" href="../References.html#id42" title="M.P. Deisenroth, A.A. Faisal, and C.S. Ong. Mathematics for Machine Learning. Cambridge University Press, 2020. ISBN 9781108470049. URL: https://books.google.ca/books?id=pFjPDwAAQBAJ.">Deisenroth <em>et al.</em>, 2020</a>, <a class="reference internal" href="../References.html#id77" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-e6758149-e3df-4f6f-8d5f-ace366f1d4da">
<span class="eqno">(12.8)<a class="headerlink" href="#equation-e6758149-e3df-4f6f-8d5f-ace366f1d4da" title="Permalink to this equation">#</a></span>\[\begin{equation}MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of data points.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> represents the actual value for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value for the <span class="math notranslate nohighlight">\(i\)</span>th data point.</p></li>
</ul>
<p>MSE penalizes large errors more than small errors, and aims to minimize the variance of the predictions.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Binary Cross-Entropy Loss</strong>: Used for binary classification tasks, where the aim is to predict the probability of an instance belonging to one of two classes. It quantifies the difference between predicted probabilities and the actual binary labels. Mathematically, it’s given by <span id="id28">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-f0e5a6df-0564-4e13-aee4-aad01a0d79d3">
<span class="eqno">(12.9)<a class="headerlink" href="#equation-f0e5a6df-0564-4e13-aee4-aad01a0d79d3" title="Permalink to this equation">#</a></span>\[\begin{equation}BCE = -\frac{1}{n} \sum_{i=1}^{n} \left(y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i)\right),\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the actual binary label for the <span class="math notranslate nohighlight">\(i\)</span>th instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i\)</span> is the predicted probability of the positive class for the <span class="math notranslate nohighlight">\(i\)</span>th instance.</p></li>
</ul>
<p>BCE penalizes incorrect predictions more than correct predictions, and aims to maximize the likelihood of the predictions.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Categorical Cross-Entropy Loss</strong>: Suited for multi-class classification problems, where the aim is to predict the probability of an instance belonging to one of several classes. It measures the difference between predicted class probabilities and the true class labels. The formula is <span id="id29">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-33bdb1b0-74e5-45f4-a8bc-c7fc21923c02">
<span class="eqno">(12.10)<a class="headerlink" href="#equation-33bdb1b0-74e5-45f4-a8bc-c7fc21923c02" title="Permalink to this equation">#</a></span>\[\begin{equation}CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \cdot \log(p_{ij}),\end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of classes.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{ij}\)</span> is an indicator (0 or 1) if instance <span class="math notranslate nohighlight">\(i\)</span> belongs to class <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{ij}\)</span> is the predicted probability of instance <span class="math notranslate nohighlight">\(i\)</span> belonging to class <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<p>CCE penalizes incorrect predictions more than correct predictions, and aims to maximize the likelihood of the predictions.</p>
<ol class="arabic" start="4">
<li><p><strong>Sparse Categorical Cross-Entropy Loss</strong>: This variation of categorical cross-entropy is useful when true labels are provided as integers, instead of one-hot encoded vectors. It still measures the difference between predicted and actual class probabilities. Mathematically, it’s defined as <span id="id30">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7faeebbc-4f84-4b0f-819b-f87cb788bdbb">
<span class="eqno">(12.11)<a class="headerlink" href="#equation-7faeebbc-4f84-4b0f-819b-f87cb788bdbb" title="Permalink to this equation">#</a></span>\[\begin{equation} SCCE = -\frac{1}{n} \sum_{i=1}^{n} \log(p_{i, y_i}) \end{equation}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n \)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\( p_{i, y_i} \)</span> is the predicted probability of instance <span class="math notranslate nohighlight">\( i \)</span> belonging to its true class <span class="math notranslate nohighlight">\( y_i \)</span>.</p></li>
</ul>
</li>
</ol>
<p>SCCE is computationally more efficient than CCE, as it does not require converting the labels to one-hot vectors.</p>
<ol class="arabic simple" start="5">
<li><p><strong>Hinge Loss (SVM Loss)</strong>: Primarily used for support vector machines (SVMs) and classification tasks in neural networks. It encourages the correct class scores to be higher than the incorrect class scores by a predefined margin. The hinge loss can be defined as <span id="id31">[<a class="reference internal" href="../References.html#id35" title="Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273-297, Sep 1995. doi:10.1007/BF00994018.">Cortes and Vapnik, 1995</a>, <a class="reference internal" href="../References.html#id175" title="Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss functions all the same? Neural computation, 16(5):1063–1076, 2004.">Rosasco <em>et al.</em>, 2004</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-f50564f9-6bc7-44a7-b1c7-2ece85545bab">
<span class="eqno">(12.12)<a class="headerlink" href="#equation-f50564f9-6bc7-44a7-b1c7-2ece85545bab" title="Permalink to this equation">#</a></span>\[\begin{equation}HingeLoss = \max(0, 1 - y_i \cdot \hat{y}_i),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual label (either -1 or 1), and <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted score for class <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Hinge loss penalizes misclassified instances more than correctly classified instances, and aims to maximize the margin between the classes.</p>
<ol class="arabic simple" start="6">
<li><p><strong>Huber Loss</strong>: A robust loss function applied in regression tasks. It offers a balance between mean squared error and mean absolute error, which makes it less sensitive to outliers. Mathematically, it’s defined as <span id="id32">[<a class="reference internal" href="../References.html#id65" title="Charles Elkan. The foundations of cost-sensitive learning. In International joint conference on artificial intelligence, volume 17, 973–978. Lawrence Erlbaum Associates Ltd, 2001.">Elkan, 2001</a>, <a class="reference internal" href="../References.html#id93" title="Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics: Methodology and distribution, pages 492–518. Springer, 1992.">Huber, 1992</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-4dfef07f-74fa-4fc4-9303-b44b5972bccd">
<span class="eqno">(12.13)<a class="headerlink" href="#equation-4dfef07f-74fa-4fc4-9303-b44b5972bccd" title="Permalink to this equation">#</a></span>\[\begin{equation}HuberLoss = \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2 &amp; \text{if } |y_i - \hat{y}_i| \leq \delta \\ \delta |y_i - \hat{y}_i| - \frac{1}{2}\delta^2 &amp; \text{otherwise} \end{cases}\end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\delta\)</span> is a hyperparameter that determines the point where the loss transitions from quadratic to linear behavior.</p>
<p>Huber loss is more robust to outliers than MSE, as it does not square the errors for large values.</p>
<ol class="arabic simple" start="7">
<li><p><strong>Kullback-Leibler Divergence (KL Divergence)</strong>: Commonly used in generative models like variational autoencoders. It measures the difference between two probability distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> <span id="id33">[<a class="reference internal" href="../References.html#id37" title="T.M. Cover and J.A. Thomas. Elements of Information Theory. Wiley, 2012. ISBN 9781118585771. URL: https://books.google.ca/books?id=VWq5GG6ycxMC.">Cover and Thomas, 2012</a>, <a class="reference internal" href="../References.html#id113" title="Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1):79–86, 1951.">Kullback and Leibler, 1951</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-565e1fc1-7466-4cdf-b8f1-1fc21bbb131c">
<span class="eqno">(12.14)<a class="headerlink" href="#equation-565e1fc1-7466-4cdf-b8f1-1fc21bbb131c" title="Permalink to this equation">#</a></span>\[\begin{equation}KLD(P \parallel Q) = \sum_{i} P(i) \cdot \log\left(\frac{P(i)}{Q(i)}\right)\end{equation}\]</div>
<p>KL divergence quantifies how much information is lost when using <span class="math notranslate nohighlight">\(Q\)</span> to approximate <span class="math notranslate nohighlight">\(P\)</span>. It is also known as the relative entropy.</p>
<ol class="arabic simple" start="8">
<li><p><strong>Triplet Loss</strong>: This loss is used in tasks like face recognition and similarity learning. It ensures that embeddings of similar examples are closer in the embedding space than embeddings of dissimilar examples. Mathematically, for a triplet of examples <span class="math notranslate nohighlight">\((a, p, n)\)</span> <span id="id34">[<a class="reference internal" href="../References.html#id30" title="Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image similarity through ranking. Journal of Machine Learning Research, 2010.">Chechik <em>et al.</em>, 2010</a>, <a class="reference internal" href="../References.html#id185" title="Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: a unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, 815–823. 2015.">Schroff <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-4016b73b-79b5-4914-b7f4-7f30aebeee27">
<span class="eqno">(12.15)<a class="headerlink" href="#equation-4016b73b-79b5-4914-b7f4-7f30aebeee27" title="Permalink to this equation">#</a></span>\[\begin{equation}TripletLoss = \max(d(a, p) - d(a, n) + \alpha, 0),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(d(x, y)\)</span> measures the distance between embeddings <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(\alpha\)</span> is a margin parameter.</p>
<p>Triplet loss minimizes the distance between an anchor example and a positive example, while maximizing the distance between the anchor example and a negative example.</p>
<ol class="arabic simple" start="9">
<li><p><strong>Focal Loss</strong>: This loss is used for imbalanced classification problems, where some classes are more frequent than others. It modifies the cross-entropy loss by adding a weighting factor that reduces the loss contribution of easy and well-classified examples, and increases the loss contribution of hard and misclassified examples. Mathematically, for a binary classification problem, it is defined as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-a8200454-ab32-44aa-adc5-53f220552d42">
<span class="eqno">(12.16)<a class="headerlink" href="#equation-a8200454-ab32-44aa-adc5-53f220552d42" title="Permalink to this equation">#</a></span>\[\begin{equation}FocalLoss = -\alpha (1 - p)^{\gamma} \log(p),\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the predicted probability of the positive class, <span class="math notranslate nohighlight">\(\alpha\)</span> is a balancing factor, and <span class="math notranslate nohighlight">\(\gamma\)</span> is a focusing parameter.</p>
<p>Focal loss reduces the dominance of the majority class and boosts the learning of the minority class. <span id="id35">[<a class="reference internal" href="../References.html#id125" title="Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, 2980–2988. 2017.">Lin <em>et al.</em>, 2017</a>]</span></p>
<ol class="arabic simple" start="10">
<li><p><strong>Dice Loss</strong>: This loss is used for semantic segmentation tasks, where the aim is to assign a class label to each pixel in an image. It measures the similarity between the predicted and actual segmentation masks, using the Dice coefficient. Mathematically, it is defined as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-ca0836e2-5298-440e-94ca-29f6b88f1295">
<span class="eqno">(12.17)<a class="headerlink" href="#equation-ca0836e2-5298-440e-94ca-29f6b88f1295" title="Permalink to this equation">#</a></span>\[\begin{equation}DiceLoss = 1 - \frac{2 \sum_{i=1}^{n} y_i \hat{y}_i + \epsilon}{\sum_{i=1}^{n} y_i^2 + \sum_{i=1}^{n} \hat{y}_i^2 + \epsilon},\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the actual binary label for pixel <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted probability for pixel <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(n\)</span> is the number of pixels, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant to avoid division by zero.</p>
<p>Dice loss is robust to class imbalance and can capture the spatial overlap between the masks. <span id="id36">[<a class="reference internal" href="../References.html#id145" title="Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), 565–571. Ieee, 2016.">Milletari <em>et al.</em>, 2016</a>]</span></p>
<ol class="arabic simple" start="11">
<li><p><strong>Contrastive Loss</strong>: This loss is used for metric learning tasks, where the aim is to learn a distance metric that can measure the similarity or dissimilarity between pairs of examples. It encourages the distance between similar examples to be smaller than the distance between dissimilar examples by a predefined margin. Mathematically, for a pair of examples <span class="math notranslate nohighlight">\((x_1, x_2)\)</span> with a binary label <span class="math notranslate nohighlight">\(y\)</span> indicating their similarity (0 for dissimilar, 1 for similar), it is defined as:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-0f349100-83c6-4ea3-b5bf-b9dce6790597">
<span class="eqno">(12.18)<a class="headerlink" href="#equation-0f349100-83c6-4ea3-b5bf-b9dce6790597" title="Permalink to this equation">#</a></span>\[\begin{equation}ContrastiveLoss = \frac{1}{2} y d^2 + \frac{1}{2} (1 - y) \max(0, m - d)^2,\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the Euclidean distance between the embeddings of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, and <span class="math notranslate nohighlight">\(m\)</span> is the margin parameter.</p>
<p>Contrastive loss can learn meaningful and discriminative embeddings that can be used for tasks like face verification, image retrieval, and clustering. <span id="id37">[<a class="reference internal" href="../References.html#id84" title="Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), volume 2, 1735–1742. IEEE, 2006.">Hadsell <em>et al.</em>, 2006</a>]</span></p>
</section>
</section>
<section id="training-and-optimization-in-deep-learning">
<h2><span class="section-number">12.2.5. </span>Training and Optimization in Deep Learning<a class="headerlink" href="#training-and-optimization-in-deep-learning" title="Link to this heading">#</a></h2>
<p>Training and optimizing a neural network is a complex and iterative process that involves adjusting the network’s parameters to learn from data and improve its performance on a specific task. This process is essential for building effective neural networks that can make accurate and meaningful predictions, classifications, and outputs. In this section, we will explore the key steps involved in training and optimizing neural networks. <span id="id38">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p>
<section id="the-training-process-unraveling-the-steps">
<h3><span class="section-number">12.2.5.1. </span>The Training Process: Unraveling the Steps<a class="headerlink" href="#the-training-process-unraveling-the-steps" title="Link to this heading">#</a></h3>
<p>The journey of training a neural network consists of several fundamental stages, each contributing to the network’s ability to comprehend complex patterns within the data it encounters. These stages can be summarized as follows:</p>
<ol class="arabic simple">
<li><p><strong>Data Collection &amp; Preprocessing:</strong> The process begins with the collection of relevant and appropriately labeled data. High-quality training data is crucial for ensuring the network generalizes well to unseen examples. Data preprocessing, which involves tasks like normalization, scaling, and handling missing values, prepares the data for effective learning. <span id="id39">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p></li>
<li><p><strong>Model Architecture Design:</strong> Designing an appropriate model architecture is critical. This step involves defining the structure of the neural network, including the number and arrangement of layers, the type of activation functions used, and how information flows through the network. <span id="id40">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p></li>
<li><p><strong>Initialize Model Parameters:</strong> Before training begins, the model’s parameters (weights and biases) are initialized. Proper initialization can set the stage for faster and more stable convergence during optimization. <span id="id41">[<a class="reference internal" href="../References.html#id78" title="Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 249–256. JMLR Workshop and Conference Proceedings, 2010.">Glorot and Bengio, 2010</a>, <a class="reference internal" href="../References.html#id89" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 1026–1034. 2015.">He <em>et al.</em>, 2015</a>]</span></p></li>
<li><p><strong>Forward Pass:</strong> During the forward pass, input data is fed into the network, and the model generates predictions. This pass involves applying the defined transformations and activation functions layer by layer. <span id="id42">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p></li>
<li><p><strong>Calculate Loss:</strong> The generated predictions are compared with the ground truth labels using a predefined loss function. This function quantifies the discrepancy between predictions and actual values. <span id="id43">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>, <a class="reference internal" href="../References.html#id196" title="Juan Terven, Diana M Cordova-Esparza, Alfonzo Ramirez-Pedraza, and Edgar A Chavez-Urbiola. Loss functions and metrics in deep learning. a review. arXiv preprint arXiv:2307.02694, 2023.">Terven <em>et al.</em>, 2023</a>]</span></p></li>
<li><p><strong>Backpropagation:</strong> Backpropagation involves calculating the gradients of the loss with respect to the model’s parameters. This step enables the network to understand how changes in each parameter affect the overall loss. <span id="id44">[<a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id178" title="David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.">Rumelhart <em>et al.</em>, 1986</a>]</span></p></li>
<li><p><strong>Update Model Parameters:</strong> Using the calculated gradients, optimization algorithms like gradient descent are employed to update the model’s parameters. These updates aim to minimize the loss and steer the network towards better performance. <span id="id45">[<a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>, <a class="reference internal" href="../References.html#id177" title="Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.">Ruder, 2016</a>]</span></p></li>
<li><p><strong>Check Convergence:</strong> The training process is performed iteratively. After each iteration (epoch), the convergence of the model is evaluated by monitoring the changes in the loss and other performance metrics. If the model shows signs of convergence or meets predefined criteria, training can stop. <span id="id46">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p></li>
</ol>
<p>Through this iterative process of forward pass, loss calculation, backpropagation, and parameter update, the neural network learns to recognize the patterns and relationships hidden in the data. This learning process enables the network to perform various tasks, such as regression, classification, image recognition, natural language processing, and more. <span id="id47">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span>.</p>
</section>
<section id="illustrating-the-process-a-flowchart-perspective">
<h3><span class="section-number">12.2.5.2. </span>Illustrating the Process: A Flowchart Perspective<a class="headerlink" href="#illustrating-the-process-a-flowchart-perspective" title="Link to this heading">#</a></h3>
<p>To better visualize the intricacies of training and optimization, consider the following flowchart that outlines the sequential steps involved:</p>
<figure class="align-center" id="id56">
<a class="reference internal image-reference" href="../_images/DL_Flowchart.jpg"><img alt="../_images/DL_Flowchart.jpg" src="../_images/DL_Flowchart.jpg" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.4 </span><span class="caption-text">The Structure of Training and Optimization in Deep Learning.</span><a class="headerlink" href="#id56" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In this flowchart:</p>
<ol class="arabic simple">
<li><p><strong>Data Collection &amp; Preprocessing:</strong> Gathering and preparing the training data.</p></li>
<li><p><strong>Model Architecture Design:</strong> Defining the structure of the neural network.</p></li>
<li><p><strong>Initialize Model Parameters:</strong> Setting initial weights and biases.</p></li>
<li><p><strong>Forward Pass:</strong> Propagating input data through the network to generate predictions.</p></li>
<li><p><strong>Calculate Loss:</strong> Comparing predictions to actual values and computing a loss/error metric.</p></li>
<li><p><strong>Backpropagation:</strong> Calculating gradients of the loss with respect to model parameters.</p></li>
<li><p><strong>Update Model Parameters:</strong> Adjusting weights and biases using optimization algorithms (e.g., gradient descent).</p></li>
<li><p><strong>Check Convergence:</strong> Evaluating whether the training process has converged or met a stopping criterion.</p></li>
<li><p><strong>Repeat Backpropagation &amp; Update:</strong> If convergence is not reached, repeat the backpropagation and parameter update steps.</p></li>
<li><p><strong>Converged:</strong> Once convergence is achieved, the model is considered trained.</p></li>
<li><p><strong>Trained Model:</strong> The neural network with optimized parameters.</p></li>
<li><p><strong>End:</strong> End of the training process.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Please note that deep learning training involves many details and variations, so this flowchart is a simplified representation. Depending on the context and level of detail you want to provide, you can expand each step and include more specific information.</p>
</div>
<p>I can help you refine your paragraph. Here is a possible improved version:</p>
</section>
</section>
<section id="conclusion">
<h2><span class="section-number">12.2.6. </span>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We have reached the end of our exploration of linear models within the realm of deep learning. Linear models offer us a starting point, shedding light on how neural networks make predictions based on data. Yet, as we peer further, it’s clear that life’s complexities demand more than just linear thinking.</p>
<p>To go beyond linear models, we need to understand three crucial components of deep learning: activation functions, loss functions, and optimization techniques. Activation functions add twists and turns to our neural pathways, enabling networks to decipher the intricate patterns hidden in data. Loss functions act as guiding lights, revealing how much our predictions deviate from the truth. Optimization techniques act like sculptors, refining the network’s inner workings and molding it into a more proficient predictor. <span id="id48">[<a class="reference internal" href="../References.html#id15" title="C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer New York, 2016. ISBN 9781493938438. URL: https://books.google.ca/books?id=kOXDtAEACAAJ.">Bishop, 2016</a>, <a class="reference internal" href="../References.html#id80" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive Computation and Machine Learning series. MIT Press, 2016. ISBN 9780262337373. URL: https://books.google.ca/books?id=omivDQAAQBAJ.">Goodfellow <em>et al.</em>, 2016</a>]</span></p>
<p>However, our journey is not confined to linear routes. Deep learning ushers us into the realm of more intricate architectures. These architectures, filled with layers of computations, transform and amplify the information they receive. This expansion leads us to exciting domains – from understanding visual data with convolutional networks <span id="id49">[<a class="reference internal" href="../References.html#id118" title="Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.">LeCun <em>et al.</em>, 2015</a>]</span> to conversing with computers through recurrent networks <span id="id50">[<a class="reference internal" href="../References.html#id31" title="Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2014. doi:10.3115/v1/d14-1179.">Cho <em>et al.</em>, 2014</a>]</span> . Moreover, we delve into creative territories, crafting new content with generative models <span id="id51">[<a class="reference internal" href="../References.html#id81" title="Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 2014.">Goodfellow <em>et al.</em>, 2014</a>]</span> and mastering the art of making informed decisions with reinforcement learning <span id="id52">[<a class="reference internal" href="../References.html#id193" title="R.S. Sutton and A.G. Barto. Reinforcement Learning, second edition: An Introduction. Adaptive Computation and Machine Learning series. MIT Press, 2018. ISBN 9780262352703. URL: https://books.google.ca/books?id=uWV0DwAAQBAJ.">Sutton and Barto, 2018</a>]</span> .</p>
<p>As we conclude this phase of our exploration, it’s important to recognize that our current understanding is but a stepping stone. The landscape of deep learning continues to expand, offering us a plethora of challenges and opportunities. By grasping the essentials, we equip ourselves to navigate this ever-evolving field with confidence and curiosity.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C12S01.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.1. </span>Understanding Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C12S03.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12.3. </span>TensorFlow Basics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-their-role-in-computational-modeling">12.2.1. Neurons and Their Role in Computational Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-elements-of-an-artificial-neuron">12.2.1.1. Key Elements of an Artificial Neuron:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-and-learning-in-neural-networks">12.2.1.2. Computation and Learning in Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">12.2.2. Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-role-of-activation-functions">12.2.2.1. The Role of Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-step-function">12.2.2.2. Binary Step function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit">12.2.2.3. ReLU (Rectified Linear Unit)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">12.2.2.4. Leaky ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function">12.2.2.5. Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent">12.2.2.6. Tanh (Hyperbolic Tangent)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">12.2.2.7. Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-linear-unit-elu">12.2.2.8. Exponential Linear Unit (ELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward-and-backpropagation-concepts">12.2.3. Feedforward and Backpropagation Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">12.2.3.1. Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">12.2.3.2. Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions-in-deep-learning">12.2.4. Loss Functions in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter">12.2.4.1. Why Loss Functions Matter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-loss-functions-help-the-model">12.2.4.2. How Loss Functions Help the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-loss-functions">12.2.4.3. Popular Loss Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-optimization-in-deep-learning">12.2.5. Training and Optimization in Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-process-unraveling-the-steps">12.2.5.1. The Training Process: Unraveling the Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrating-the-process-a-flowchart-perspective">12.2.5.2. Illustrating the Process: A Flowchart Perspective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.2.6. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>