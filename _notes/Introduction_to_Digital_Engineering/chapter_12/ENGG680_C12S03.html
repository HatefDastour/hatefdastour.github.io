
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12.3. TensorFlow Basics &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_12/ENGG680_C12S03';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.4. Introduction to Variables" href="ENGG680_C12S04.html" />
    <link rel="prev" title="12.2. Fundamentals of Neural Networks" href="ENGG680_C12S02.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Digital Engineering - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Digital Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ENGG680_C01.html">1. Introduction to Python Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S01.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S02.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG680_C01S03.html">1.3. Functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ENGG680_C02.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S01.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG680_C02S02.html">2.2. Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ENGG680_C03.html">3. Data Structures and File Handling in Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S01.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S02.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S03.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S04.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S05.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG680_C03S06.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ENGG680_C04.html">4. Classes and Objects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S01.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S02.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S03.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S04.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S05.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG680_C04S06.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ENGG680_C05.html">5. Introduction to NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S01.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S02.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG680_C05S03.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ENGG680_C06.html">6. Working with Data using Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S01.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S02.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S03.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S04.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S05.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG680_C06S06.html">6.6. Aggregation and Grouping in Pandas</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ENGG680_C07.html">7. Data Visualization using Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S01.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S02.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S03.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S04.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S05.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG680_C07S06.html">7.6. Python Plotting Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ENGG680_C08.html">8. An Introduction to Computer Vision</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S01.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S02.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S03.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S04.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG680_C08S05.html">8.5. Drawing Functions (Optional Section)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ENGG680_C09.html">9. An Introduction to Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S01.html">9.1. Prologue: Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S02.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S03.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S04.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S05.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S06.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG680_C09S07.html">9.7. Support Vector Machines</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ENGG680_C10.html">10. Tree-Based Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S01.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S02.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S03.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S04.html">10.4. Regression Trees and Linear Models (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S05.html">10.5. Enhancing Decision Trees with Bagging: An Introduction (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S06.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG680_C10S07.html">10.7. Gradient Boosting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ENGG680_C11.html">11. Dimensionality Reduction and Feature Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S01.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S02.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S03.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S04.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S05.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG680_C11S06.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ENGG680_C12.html">12. Introduction to Deep Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S01.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S02.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.3. TensorFlow Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S04.html">12.4. Introduction to Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S05.html">12.5. Tensors in Various Operations (Ops)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S06.html">12.6. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S07.html">12.7. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S08.html">12.8. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S09.html">12.9. Deep Learning Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S10.html">12.10. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S11.html">12.11. Image Augmentations with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S12.html">12.12. Enhancing Image Classification Precision Through TensorFlow and Data Augmentation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG680_C12S13.html">12.13. Brief Overview of Additional Topics</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">13. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>TensorFlow Basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#array-and-tensor-connection">12.3.1. Array and Tensor Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-ties-to-numpy-arrays">12.3.2. Strong Ties to NumPy Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-or-rank-0-tensor">12.3.3. Scalar or Rank-0 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-or-rank-1-tensor">12.3.4. Vector or Rank-1 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-or-rank-2-tensor">12.3.5. Matrix or Rank-2 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-tensors-to-numpy-arrays">12.3.6. Converting Tensors to NumPy Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types-in-tensorflow">12.3.7. Data Types in TensorFlow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">12.3.8. Broadcasting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shapes-and-indexing">12.3.9. Tensor Shapes and Indexing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-shapes-with-tf-tensorshape-objects">12.3.9.1. Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-tensors">12.3.10. Indexing Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-axis-indexing">12.3.10.1. Single-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-axis-indexing">12.3.10.2. Multi-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensor-shapes">12.3.10.3. Reshaping Tensor Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ragged-tensors">12.3.11. Ragged Tensors</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tensorflow-basics">
<h1><span class="section-number">12.3. </span>TensorFlow Basics<a class="headerlink" href="#tensorflow-basics" title="Link to this heading">#</a></h1>
<p>Deep learning is a powerful field of artificial intelligence that uses neural networks to make sense of data and perform tasks like recognizing images, understanding natural language, and generating speech. These neural networks consist of intricate layers of nodes that perform complex calculations on data inputs and outputs. To efficiently handle this data and speed up these calculations, deep learning relies on a special data structure called <strong>tensors</strong>.</p>
<p>Tensors can be thought of as a more advanced version of the familiar <strong>arrays</strong> we often use in programming and mathematics. Arrays are collections of elements organized in rows and columns, and tensors share a strong connection with them. Let’s explore how they are similar <span id="id1">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<section id="array-and-tensor-connection">
<h2><span class="section-number">12.3.1. </span>Array and Tensor Connection<a class="headerlink" href="#array-and-tensor-connection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Multidimensional Flexibility:</strong> Arrays, with their ability to have multiple dimensions, lay the foundation for tensors. In arrays, dimensions are like the arrangement of data in a musical score, with rows and columns. Tensors take this idea further, extending dimensions into a multi-axis arrangement that can adapt to any kind of data structure.</p></li>
<li><p><strong>Indexing and Access:</strong> Just like you use indices to access elements in arrays, tensors work the same way for retrieving data. Elements inside both arrays and tensors are accessed and manipulated using indices, making data handling consistent.</p></li>
<li><p><strong>Mathematical Aptitude:</strong> Arrays have a well-established reputation for their mathematical capabilities, crucial for performing various operations. Tensors, as their successors, continue this mathematical legacy, serving as the conduits through which deep learning networks perform complex numerical operations.</p></li>
<li><p><strong>Data Representation:</strong> Similar to how arrays are used to represent data, tensors take center stage in deep learning. They excel at accurately encapsulating data, whether it’s images, text, or sound. These versatile data structures channel the essence of information into the neural network’s computational fabric.</p></li>
<li><p><strong>Transformations and Operations:</strong> Both arrays and tensors are alike in their transformative nature. They gracefully adapt within their dimensions to accommodate a range of mathematical operations, such as addition, multiplication, and transformation. Just as arrays change during calculations, tensors efficiently absorb operations, enhancing the capabilities of neural networks.</p></li>
</ul>
</section>
<section id="strong-ties-to-numpy-arrays">
<h2><span class="section-number">12.3.2. </span>Strong Ties to NumPy Arrays<a class="headerlink" href="#strong-ties-to-numpy-arrays" title="Link to this heading">#</a></h2>
<p>While tensors may seem like a novel concept in the context of deep learning, it’s important to recognize their strong connection to arrays, which have long been fundamental in programming and mathematics. Arrays can be seen as simplified versions of tensors, often with fixed dimensions and data types. Tensors, on the other hand, are more flexible and dynamic, allowing for variable dimensions and data types. Tensors also have some additional features and functionalities that make them more suitable for deep learning, such as automatic differentiation, GPU support, and distributed training. However, tensors and arrays are still compatible and interchangeable, thanks to the NumPy library, which provides a common interface for both data structures. NumPy is a popular Python library that offers a variety of tools and functions for working with arrays and tensors. It allows you to easily create, manipulate, and convert arrays and tensors, as well as perform various mathematical and statistical operations on them. NumPy is widely used in scientific computing and data analysis, and it serves as the backbone of many other libraries and frameworks, including TensorFlow. TensorFlow is a leading deep learning framework that provides a comprehensive set of tools and features for building, training, and deploying neural networks. TensorFlow uses tensors as its main data structure, and it relies on NumPy for creating and converting tensors from and to arrays. This makes it easy to integrate TensorFlow with other libraries and frameworks that use arrays, such as Pandas, Scikit-learn, and Matplotlib. By using NumPy and TensorFlow together, you can leverage the power and flexibility of tensors, while also benefiting from the simplicity and familiarity of arrays <span id="id2">[<a class="reference internal" href="../References.html#id164" title="K.B. Prakash and G.R. Kanagachidambaresan. Programming with TensorFlow: Solution for Edge Computing Applications. EAI/Springer Innovations in Communication and Computing. Springer International Publishing, 2021. ISBN 9783030570774. URL: https://books.google.ca/books?id=VcUWEAAAQBAJ.">Prakash and Kanagachidambaresan, 2021</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Definition</p>
<p><strong>Tensors</strong> are specialized data structures that resemble <strong>multi-dimensional arrays</strong>, but with some key differences and advantages. Tensors have the following characteristics <span id="id3">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p>They have a consistent data type, which is referred to as a ‘dtype.’ The available data types can be found in <code class="docutils literal notranslate"><span class="pre">tf.dtypes.DType</span></code>.</p></li>
<li><p>They are immutable, which means that you cannot modify the content of an existing tensor; instead, you create a new tensor when changes are needed. This is similar to how Python treats numbers and strings.</p></li>
<li><p>They can have variable dimensions and data types, which makes them more flexible and dynamic than arrays.</p></li>
<li><p>They have additional features and functionalities that make them more suitable for deep learning, such as automatic differentiation, GPU support, and distributed training.</p></li>
</ul>
<p>Tensors are the main data structure used by TensorFlow, a leading deep learning framework that provides a comprehensive set of tools and features for building, training, and deploying neural networks.</p>
</div>
<p>Let’s begin our journey into the world of TensorFlow by learning how to create basic tensors. Tensors are the fundamental data structures of TensorFlow, enabling us to work with data in a powerful and flexible way <span id="id4">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</section>
<section id="scalar-or-rank-0-tensor">
<h2><span class="section-number">12.3.3. </span>Scalar or Rank-0 Tensor<a class="headerlink" href="#scalar-or-rank-0-tensor" title="Link to this heading">#</a></h2>
<p>A scalar or rank-0 tensor is a special type of tensor that holds a single value and has no dimensions or axes <span id="id5">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>. It is the simplest form of a tensor, and it can be used to represent constants, such as numbers or strings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">31</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display text in bold with optional color.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - txt (str): The text to be displayed.</span>
<span class="sd">    - c (int): Color code for the text (default is 31 for red).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="c1"># Creating a Rank-0 Tensor (Scalar) with an Integer value</span>
<span class="c1"># By default, this will be an int32 tensor; more on &quot;dtypes&quot; below.</span>
<span class="n">rank_0_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">680</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-0 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-0 Tensor (Scalar) with Integer value:&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_0_tensor</span><span class="p">)</span>

<span class="c1"># Creating a Rank-0 Tensor (Scalar) with a Boolean value</span>
<span class="c1"># This will be a bool tensor with a specified dtype.</span>
<span class="n">rank_0_tensor_bool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-0 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-0 Tensor (Scalar) with Boolean value:&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_0_tensor_bool</span><span class="p">)</span>

<span class="c1"># Creating a Rank-0 Tensor (Scalar) with a String value</span>
<span class="c1"># This will be a string tensor with a specified dtype.</span>
<span class="n">rank_0_tensor_string</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s2">&quot;Hello, ENGG 680!&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-0 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-0 Tensor (Scalar) with String value:&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_0_tensor_string</span><span class="p">)</span>

<span class="c1"># Creating a Rank-0 Tensor (Scalar) with a Float value</span>
<span class="c1"># This will be a float64 tensor with a specified dtype.</span>
<span class="n">rank_0_tensor_float</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.14159</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-0 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-0 Tensor (Scalar) with Float value:&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_0_tensor_float</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Green">Rank-0 Tensor (Scalar) with Integer value:</span>
tf.Tensor(680, shape=(), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Green">Rank-0 Tensor (Scalar) with Boolean value:</span>
tf.Tensor(True, shape=(), dtype=bool)

<span class=" -Color -Color-Bold -Color-Bold-Green">Rank-0 Tensor (Scalar) with String value:</span>
tf.Tensor(b&#39;Hello, ENGG 680!&#39;, shape=(), dtype=string)

<span class=" -Color -Color-Bold -Color-Bold-Green">Rank-0 Tensor (Scalar) with Float value:</span>
tf.Tensor(3.14159, shape=(), dtype=float64)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id56">
<a class="reference internal image-reference" href="../_images/scalar.png"><img alt="../_images/scalar.png" src="../_images/scalar.png" style="width: 150px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.5 </span><span class="caption-text">Tensors with a scalar nature and a shape represented as an empty set of parentheses ().</span><a class="headerlink" href="#id56" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="vector-or-rank-1-tensor">
<h2><span class="section-number">12.3.4. </span>Vector or Rank-1 Tensor<a class="headerlink" href="#vector-or-rank-1-tensor" title="Link to this heading">#</a></h2>
<p>A vector or rank-1 tensor is a tensor that has only one dimension or axis. It can be seen as a sequence of values, similar to a list, that has a specific direction. For example, a vector can represent the coordinates of a point in a two-dimensional plane, or the velocity of an object in a three-dimensional space <span id="id6">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">31</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display text in bold with optional color.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - txt (str): The text to be displayed.</span>
<span class="sd">    - c (int): Color code for the text (default is 31 for red).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="c1"># Creating a Rank-1 Tensor (Vector) with integer values</span>
<span class="c1"># We specify float values to make it a float tensor.</span>
<span class="n">rank_1_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Displaying the Rank-1 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-1 Tensor (Vector) with Integer values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_1_tensor</span><span class="p">)</span>

<span class="c1"># Creating a Rank-1 Tensor (Vector) with string values</span>
<span class="c1"># We specify string values to make it a string tensor.</span>
<span class="n">rank_1_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s2">&quot;Hello&quot;</span><span class="p">,</span> <span class="s2">&quot;ENGG&quot;</span><span class="p">,</span> <span class="s2">&quot;680&quot;</span><span class="p">])</span>

<span class="c1"># Displaying the Rank-1 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-1 Tensor (Vector) with String values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_1_tensor</span><span class="p">)</span>

<span class="c1"># Creating a Rank-1 Tensor (Vector) with boolean values</span>
<span class="c1"># We specify boolean values to make it a boolean tensor.</span>
<span class="n">rank_1_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>

<span class="c1"># Displaying the Rank-1 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-1 Tensor (Vector) with Boolean values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_1_tensor</span><span class="p">)</span>

<span class="c1"># Creating a Rank-1 Tensor (Vector) with complex values</span>
<span class="c1"># We specify complex values to make it a complex tensor.</span>
<span class="n">rank_1_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">-</span> <span class="mi">4</span><span class="n">j</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">6</span><span class="n">j</span><span class="p">])</span>

<span class="c1"># Displaying the Rank-1 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-1 Tensor (Vector) with Complex values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_1_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-1 Tensor (Vector) with Integer values:</span>
tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Rank-1 Tensor (Vector) with String values:</span>
tf.Tensor([b&#39;Hello&#39; b&#39;ENGG&#39; b&#39;680&#39;], shape=(3,), dtype=string)

<span class=" -Color -Color-Bold -Color-Bold-Red">Rank-1 Tensor (Vector) with Boolean values:</span>
tf.Tensor([ True False  True], shape=(3,), dtype=bool)

<span class=" -Color -Color-Bold -Color-Bold-Red">Rank-1 Tensor (Vector) with Complex values:</span>
tf.Tensor([1.+2.j 3.-4.j 5.+6.j], shape=(3,), dtype=complex128)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id57">
<a class="reference internal image-reference" href="../_images/vector.png"><img alt="../_images/vector.png" src="../_images/vector.png" style="width: 220px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.6 </span><span class="caption-text">Tensors with a shape of 3 in the context of vector notation.</span><a class="headerlink" href="#id57" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="matrix-or-rank-2-tensor">
<h2><span class="section-number">12.3.5. </span>Matrix or Rank-2 Tensor<a class="headerlink" href="#matrix-or-rank-2-tensor" title="Link to this heading">#</a></h2>
<p>A matrix or rank-2 tensor is a tensor that has two dimensions or axes. It can be seen as a grid of values, similar to a two-dimensional array, that has a specific row and column structure. For example, a matrix can represent the pixels of an image, the coefficients of a linear system, or the features of a data set <span id="id7">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-2 Tensor (Matrix) with Specific Dtype</span>
<span class="c1"># We define a 2x2 matrix and set the data type to float16.</span>
<span class="n">rank_2_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">22</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">33</span><span class="p">,</span> <span class="mi">44</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">66</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-2 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-2 Tensor (Matrix) with Specific Dtype:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-2 Tensor (Matrix) with Specific Dtype:</span>
tf.Tensor(
[[11 22]
 [33 44]
 [55 66]], shape=(3, 2), dtype=int16)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id58">
<a class="reference internal image-reference" href="../_images/matrix.png"><img alt="../_images/matrix.png" src="../_images/matrix.png" style="width: 120px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.7 </span><span class="caption-text">A matrix with a shape of (3, 2).</span><a class="headerlink" href="#id58" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>One way to create rank-2 tensors or matrices in TensorFlow is to use the built-in functions that generate tensors with specific values. In this example, we will see how to create rank-2 tensors with random values, zeros, and ones using <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/random/uniform"><strong>tf.random.uniform</strong></a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/zeros"><strong>tf.zeros</strong></a>, and <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/ones"><strong>tf.ones</strong></a> respectively. These functions take a shape argument that specifies the number of rows and columns of the matrix. For example, shape=(3, 3) means a 3x3 matrix. We can then display the rank-2 tensors using the print function. Here is the code for this example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a Rank-2 Tensor (Matrix) with Random Values</span>
<span class="c1"># We use tf.random.uniform to generate a 3x3 matrix with values between 0 and 1.</span>
<span class="n">rank_2_tensor_random</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Displaying the Rank-2 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-2 Tensor (Matrix) with Random Values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor_random</span><span class="p">)</span>

<span class="c1"># Creating a Rank-2 Tensor (Matrix) with Zeros</span>
<span class="c1"># We use tf.zeros to create a 4x4 matrix with all zeros.</span>
<span class="n">rank_2_tensor_zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Displaying the Rank-2 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-2 Tensor (Matrix) with Zeros:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor_zeros</span><span class="p">)</span>

<span class="c1"># Creating a Rank-2 Tensor (Matrix) with Ones</span>
<span class="c1"># We use tf.ones to create a 5x5 matrix with all ones.</span>
<span class="n">rank_2_tensor_ones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Displaying the Rank-2 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rank-2 Tensor (Matrix) with Ones:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor_ones</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-2 Tensor (Matrix) with Random Values:</span>
tf.Tensor(
[[0.8024781  0.70768654 0.7383542 ]
 [0.16242695 0.703591   0.7325984 ]
 [0.7425693  0.40302432 0.04241037]], shape=(3, 3), dtype=float32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Rank-2 Tensor (Matrix) with Zeros:</span>
tf.Tensor(
[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Rank-2 Tensor (Matrix) with Ones:</span>
tf.Tensor(
[[1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]], shape=(5, 5), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Tensors Can Have More Axes: Here’s an Example with Three Axes <span id="id8">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a Rank-3 Tensor with Three Axes</span>
<span class="c1"># Here, we have a 3x2 grid of values, and</span>
<span class="c1"># there&#39;s an outer dimension encompassing them.</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">],</span> <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">]],</span>
                             <span class="p">])</span>

<span class="c1"># Displaying the Rank-3 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-3 Tensor with Three Axes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-3 Tensor with Three Axes:</span>
tf.Tensor(
[[[11 12 13 14]
  [15 16 17 18]]

 [[21 22 23 24]
  [25 26 27 28]]

 [[31 32 33 34]
  [35 36 37 38]]], shape=(3, 2, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id59">
<a class="reference internal image-reference" href="../_images/3d_tensor.png"><img alt="../_images/3d_tensor.png" src="../_images/3d_tensor.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.8 </span><span class="caption-text">Visualizing the above 3D Tensor.</span><a class="headerlink" href="#id59" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="converting-tensors-to-numpy-arrays">
<h2><span class="section-number">12.3.6. </span>Converting Tensors to NumPy Arrays<a class="headerlink" href="#converting-tensors-to-numpy-arrays" title="Link to this heading">#</a></h2>
<p>TensorFlow and NumPy are both libraries for scientific computing in Python that provide powerful array objects to store and manipulate multidimensional data. They have some similarities, such as:</p>
<ul class="simple">
<li><p>They both support various data types, such as int16, float32, bool, etc. In this example, we specify the data type of the tensor as int16 using the dtype argument <span id="id9">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id10">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
<li><p>They both have similar methods to create arrays with specific values, such as tf.ones, tf.zeros, np.ones, np.zeros, etc. In this example, we use tf.constant to create a tensor with constant values <span id="id11">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id12">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
<li><p>They both have similar attributes to access the shape and size of the arrays, such as tensor.shape, numpy_array.shape, tensor.size, numpy_array.size, etc. In this example, we print the shape of the tensor and the NumPy arrays using the shape attribute <span id="id13">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id14">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
<li><p>They both have similar methods to perform mathematical operations on the arrays, such as tf.add, tf.multiply, np.add, np.multiply, etc. In this example, we do not perform any operations, but we could do so if we wanted to <span id="id15">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id16">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
</ul>
<p>However, they also have some differences, such as:</p>
<ul class="simple">
<li><p>TensorFlow tensors are immutable, meaning that they cannot be modified once created. NumPy arrays are mutable, meaning that they can be modified in place. In this example, we cannot change the values of the tensor, but we can change the values of the NumPy arrays <span id="id17">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id18">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
<li><p>TensorFlow tensors can be used to build and run computational graphs that can perform automatic differentiation and optimization for machine learning models <span id="id19">[<a class="reference internal" href="../References.html#id77" title="A. Géron. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly, 2022. ISBN 9781098125974. URL: https://books.google.ca/books?id=bWD5zgEACAAJ.">Géron, 2022</a>]</span> . NumPy arrays do not have this capability, and are mainly used for numerical analysis and data manipulation. In this example, we do not use any computational graphs, but we could do so if we wanted to build a machine learning model using TensorFlow <span id="id20">[<a class="reference internal" href="../References.html#id2" title="Martín Abadi. Tensorflow: learning functions at scale. In Proceedings of the 21st ACM SIGPLAN international conference on functional programming, 1–1. 2016. doi:10.1145/2951913.2976746.">Abadi, 2016</a>]</span>.</p></li>
<li><p>TensorFlow tensors can be executed on different devices, such as CPUs, GPUs, or TPUs, to accelerate the computation <span id="id21">[<a class="reference internal" href="../References.html#id2" title="Martín Abadi. Tensorflow: learning functions at scale. In Proceedings of the 21st ACM SIGPLAN international conference on functional programming, 1–1. 2016. doi:10.1145/2951913.2976746.">Abadi, 2016</a>]</span>. NumPy arrays can only be executed on CPUs, and do not support parallelization or distribution. In this example, we do not specify any device, but we could do so if we wanted to use a different device for the tensor computation <span id="id22">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> <span id="id23">[<a class="reference internal" href="../References.html#id169" title="B. Ramsundar and R.B. Zadeh. TensorFlow for Deep Learning: From Linear Regression to Reinforcement Learning. O'Reilly Media, 2018. ISBN 9781491980408. URL: https://books.google.ca/books?id=GZ1ODwAAQBAJ.">Ramsundar and Zadeh, 2018</a>]</span> .</p></li>
</ul>
<p>To illustrate how to convert tensors to NumPy arrays, we will use the following code snippet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import NumPy and TensorFlow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">31</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display text in bold with optional color.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - txt (str): The text to be displayed.</span>
<span class="sd">    - c (int): Color code for the text (default is 31 for red).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1;</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">m&quot;</span> <span class="o">+</span> <span class="n">txt</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="c1"># Create a 2D array of shape (2, 3) with random values</span>
<span class="n">np_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;NumPy array:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">np_array</span><span class="p">)</span>

<span class="c1"># Convert the NumPy array to a TensorFlow tensor</span>
<span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">np_array</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TensorFlow tensor:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">tf_tensor</span><span class="p">)</span>

<span class="c1"># Perform element-wise operations on both the array and the tensor</span>
<span class="n">np_array</span> <span class="o">=</span> <span class="n">np_array</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Add 1 to each element</span>
<span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf_tensor</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Add 1 to each element</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NumPy array after adding 1:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">np_array</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TensorFlow tensor after adding 1:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">tf_tensor</span><span class="p">)</span>

<span class="n">np_array</span> <span class="o">=</span> <span class="n">np_array</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Multiply each element by 2</span>
<span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf_tensor</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Multiply each element by 2</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NumPy array after multiplying by 2:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">np_array</span><span class="p">)</span>

<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TensorFlow tensor after multiplying by 2:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">tf_tensor</span><span class="p">)</span>

<span class="c1"># Convert the TensorFlow tensor back to a NumPy array</span>
<span class="n">np_array2</span> <span class="o">=</span> <span class="n">tf_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NumPy array converted from TensorFlow tensor:&quot;</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">np_array2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">NumPy array:</span>
array([[0, 6, 5],
       [4, 2, 3]])

<span class=" -Color -Color-Bold -Color-Bold-Red">TensorFlow tensor:</span>
&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 6, 5],
       [4, 2, 3]])&gt;

<span class=" -Color -Color-Bold -Color-Bold-Red">NumPy array after adding 1:</span>
array([[1, 7, 6],
       [5, 3, 4]])

<span class=" -Color -Color-Bold -Color-Bold-Red">TensorFlow tensor after adding 1:</span>
&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 7, 6],
       [5, 3, 4]])&gt;

<span class=" -Color -Color-Bold -Color-Bold-Red">NumPy array after multiplying by 2:</span>
array([[ 2, 14, 12],
       [10,  6,  8]])

<span class=" -Color -Color-Bold -Color-Bold-Red">TensorFlow tensor after multiplying by 2:</span>
&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[ 2, 14, 12],
       [10,  6,  8]])&gt;

<span class=" -Color -Color-Bold -Color-Bold-Red">NumPy array converted from TensorFlow tensor:</span>
array([[ 2, 14, 12],
       [10,  6,  8]])
</pre></div>
</div>
</div>
</div>
<p><font color='Blue'><b>Example:</b></font> In this example,  we demonstrate that tensors are immutable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import NumPy and TensorFlow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a 2D tensor of shape (2, 3) with constant values and int16 data type</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow tensor:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>

<span class="c1"># Convert the tensor to a NumPy array using the tensor.numpy method</span>
<span class="n">array</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NumPy array converted from TensorFlow tensor:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">array</span><span class="p">)</span>

<span class="c1"># Try to modify the first element of the first row of both the array and the tensor</span>
<span class="n">array</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># This works, as NumPy arrays are mutable</span>
<span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># This fails, as TensorFlow tensors are immutable</span>
</pre></div>
</div>
<p>The output of this code is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TensorFlow</span> <span class="n">tensor</span><span class="p">:</span>
 <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int16</span><span class="p">)</span>
<span class="n">NumPy</span> <span class="n">array</span> <span class="n">converted</span> <span class="kn">from</span> <span class="nn">TensorFlow</span> <span class="n">tensor</span><span class="p">:</span>
 <span class="p">[[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span><span class="p">]]</span>
<span class="o">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span>                                 <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">30</span><span class="p">],</span> <span class="n">line</span> <span class="mi">12</span>
     <span class="mi">11</span> <span class="n">array</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># This works, as NumPy arrays are mutable</span>
<span class="o">---&gt;</span> <span class="mi">12</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># This fails, as TensorFlow tensors are immutable</span>

<span class="ne">TypeError</span><span class="p">:</span> <span class="s1">&#39;tensorflow.python.framework.ops.EagerTensor&#39;</span> <span class="nb">object</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">item</span> <span class="n">assignment</span>
</pre></div>
</div>
<p>As you can see, the NumPy array can be modified in place, but the TensorFlow tensor cannot. This is because TensorFlow tensors are immutable, meaning that they cannot be changed once created <span id="id24">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>. This is a design choice that allows TensorFlow to optimize the computation and memory usage of tensors, especially when they are used in computational graphs. If you want to change the values of a tensor, you need to create a new tensor with the updated values. For example, you can use the tf.assign function to assign new values to a variable that holds a tensor <span id="id25">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span> . Here is an example of how to do that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a variable that holds a 2D tensor of shape (2, 3) with constant values and int16 data type</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;TensorFlow Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

<span class="c1"># Try to modify the first element of the first row of the variable using the tf.assign function</span>
<span class="n">var</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># This works, as variables can be updated using the assign method</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TensorFlow Variable after assigning 100 to the first element of the first row:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">TensorFlow Variable:</span>
&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 3) dtype=int16, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int16)&gt;

<span class=" -Color -Color-Bold -Color-Bold-Red">TensorFlow Variable after assigning 100 to the first element of the first row:</span>
&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 3) dtype=int16, numpy=
array([[100,   2,   3],
       [  4,   5,   6]], dtype=int16)&gt;
</pre></div>
</div>
</div>
</div>
<p>As you can see, the variable can be updated using the assign method, which creates a new tensor with the updated values and assigns it to the variable. This is different from modifying the tensor directly, which is not possible. This example demonstrates how to update the values of a tensor using the tf.assign function <span id="id26">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</section>
<section id="data-types-in-tensorflow">
<h2><span class="section-number">12.3.7. </span>Data Types in TensorFlow<a class="headerlink" href="#data-types-in-tensorflow" title="Link to this heading">#</a></h2>
<p>To examine the data type of a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>, you can utilize the <code class="docutils literal notranslate"><span class="pre">Tensor.dtype</span></code> property. When generating a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> from a Python object, you have the option to specify the data type if desired <span id="id27">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>In cases where you don’t specify a data type, TensorFlow automatically selects an appropriate one to accommodate your data. For instance, TensorFlow converts Python integers to <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code> and Python floating-point numbers to <code class="docutils literal notranslate"><span class="pre">tf.float32</span></code>. In other scenarios, TensorFlow applies the same conventions as NumPy does when converting to arrays <span id="id28">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Moreover, TensorFlow permits you to perform type casting, enabling you to switch between different data types as needed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a tensor with float64 data type</span>
<span class="n">the_f64_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Original Float64 Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">the_f64_tensor</span><span class="p">)</span>

<span class="c1"># Cast the float64 tensor to float16</span>
<span class="n">the_f16_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">the_f64_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Float64 Tensor cast to Float16:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">the_f16_tensor</span><span class="p">)</span>

<span class="c1"># Cast the float16 tensor to uint8 (unsigned 8-bit integer)</span>
<span class="c1"># This will lead to loss of decimal precision</span>
<span class="n">the_u8_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">the_f16_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Float16 Tensor cast to Uint8:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">the_u8_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Original Float64 Tensor:</span>
tf.Tensor([2.2 3.3 4.4], shape=(3,), dtype=float64)

<span class=" -Color -Color-Bold -Color-Bold-Red">Float64 Tensor cast to Float16:</span>
tf.Tensor([2.2 3.3 4.4], shape=(3,), dtype=float16)

<span class=" -Color -Color-Bold -Color-Bold-Red">Float16 Tensor cast to Uint8:</span>
tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="broadcasting">
<h2><span class="section-number">12.3.8. </span>Broadcasting<a class="headerlink" href="#broadcasting" title="Link to this heading">#</a></h2>
<p>Broadcasting is a concept adopted from the <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">equivalent feature in NumPy</a>. In essence, broadcasting enables smaller tensors to be automatically expanded to match the dimensions of larger tensors when performing combined operations on them <span id="id29">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>The most straightforward and frequent scenario occurs when you try to multiply or add a tensor by a scalar value. In this instance, the scalar value is broadcasted, causing it to take on the same shape as the other tensor in the operation. This seamless extension of dimensions facilitates efficient computation of operations involving tensors with varying shapes <span id="id30">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a constant tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Original Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Define scalar and tensor constants</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Perform the same computation using different expressions</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Broadcasting scalar to tensor</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>  <span class="c1"># Broadcasting scalar to tensor using operator</span>
<span class="n">result3</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>  <span class="c1"># Broadcasting tensor to tensor</span>

<span class="c1"># Print the results</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result 1 (Scalar Broadcasting):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result1</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result 2 (Scalar Broadcasting using Operator):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Result 3 (Tensor Broadcasting):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Original Tensor:</span>
tf.Tensor([1 2 3], shape=(3,), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Result 1 (Scalar Broadcasting):</span>
tf.Tensor([2 4 6], shape=(3,), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Result 2 (Scalar Broadcasting using Operator):</span>
tf.Tensor([2 4 6], shape=(3,), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Result 3 (Tensor Broadcasting):</span>
tf.Tensor([2 4 6], shape=(3,), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Similarly, axes with a length of 1 can be extended to align with the dimensions of other arguments during operations. This extension can be applied to both operands simultaneously within a single computation <span id="id31">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>In the example, a 3x1 matrix is multiplied element-wise with a 1x4 matrix, resulting in a 3x4 matrix. It’s worth noting that the leading 1 in the matrix dimensions is not strictly required. For instance, the shape of matrix <code class="docutils literal notranslate"><span class="pre">y</span></code> can be expressed as <code class="docutils literal notranslate"><span class="pre">[4]</span></code>, highlighting that broadcasting effectively accounts for dimensions of length 1, regardless of whether they are explicitly stated <span id="id32">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Original Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Reshaped x:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create a tensor with values ranging from 1 to 4</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">y:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Perform element-wise multiplication using broadcasting</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Result of Element-wise Multiplication:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Original Tensor:</span>
tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Reshaped x:</span>
tf.Tensor(
[[1]
 [2]
 [3]
 [4]
 [5]], shape=(5, 1), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">y:</span>
tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)
<span class=" -Color -Color-Bold -Color-Bold-Red">Result of Element-wise Multiplication:</span>
tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]
 [ 4  8 12 16]
 [ 5 10 15 20]], shape=(5, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Let’s express the mathematical representation of the element-wise multiplication operation performed in the provided TensorFlow code:</p>
<p>Given the tensors <span class="math notranslate nohighlight">\( x \)</span> and <span class="math notranslate nohighlight">\( y \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1e349f6d-3f30-4e56-9a3e-7ad1bb4ca20e">
<span class="eqno">(12.19)<a class="headerlink" href="#equation-1e349f6d-3f30-4e56-9a3e-7ad1bb4ca20e" title="Permalink to this equation">#</a></span>\[\begin{equation} x = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{bmatrix} \end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-85a5b0e8-a52e-4483-89ec-7dc1b3019c46">
<span class="eqno">(12.20)<a class="headerlink" href="#equation-85a5b0e8-a52e-4483-89ec-7dc1b3019c46" title="Permalink to this equation">#</a></span>\[\begin{equation} y = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \end{bmatrix} \end{equation}\]</div>
<p>The element-wise multiplication <span class="math notranslate nohighlight">\( \text{result} \)</span> is obtained by broadcasting <span class="math notranslate nohighlight">\( y \)</span> across the columns of <span class="math notranslate nohighlight">\( x \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-06cacd01-05a2-4c92-a3b6-3963b7fbc932">
<span class="eqno">(12.21)<a class="headerlink" href="#equation-06cacd01-05a2-4c92-a3b6-3963b7fbc932" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{result} = \begin{bmatrix} 1 \times 1 &amp; 1 \times 2 &amp; 1 \times 3 &amp; 1 \times 4 \\ 2 \times 1 &amp; 2 \times 2 &amp; 2 \times 3 &amp; 2 \times 4 \\ 3 \times 1 &amp; 3 \times 2 &amp; 3 \times 3 &amp; 3 \times 4 \\ 4 \times 1 &amp; 4 \times 2 &amp; 4 \times 3 &amp; 4 \times 4 \\ 5 \times 1 &amp; 5 \times 2 &amp; 5 \times 3 &amp; 5 \times 4 \end{bmatrix} \end{equation}\]</div>
<p>Simplifying the multiplication results in the final element-wise product matrix:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dbcb6a6b-f733-46dd-8ac8-fa9d709d244d">
<span class="eqno">(12.22)<a class="headerlink" href="#equation-dbcb6a6b-f733-46dd-8ac8-fa9d709d244d" title="Permalink to this equation">#</a></span>\[\begin{equation} \text{result} = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 2 &amp; 4 &amp; 6 &amp; 8 \\ 3 &amp; 6 &amp; 9 &amp; 12 \\ 4 &amp; 8 &amp; 12 &amp; 16 \\ 5 &amp; 10 &amp; 15 &amp; 20 \end{bmatrix} \end{equation}\]</div>
<figure class="align-center" id="id60">
<a class="reference internal image-reference" href="../_images/tf_broadcast.png"><img alt="../_images/tf_broadcast.png" src="../_images/tf_broadcast.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.9 </span><span class="caption-text">A broadcasted addition occurs when a tensor with shape [5, 1] is combined with another tensor of shape [1, 4], resulting in a new tensor with shape [5, 4].</span><a class="headerlink" href="#id60" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Here is the same operation conducted without utilizing broadcasting:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define tensors for multiplication without broadcasting</span>
<span class="n">x_stretch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>

<span class="n">y_stretch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># Perform element-wise multiplication without broadcasting</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">x_stretch</span> <span class="o">*</span> <span class="n">y_stretch</span>

<span class="c1"># Print the result</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Element-wise Multiplication Without Broadcasting:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Operator overloading performs element-wise multiplication</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Element-wise Multiplication Without Broadcasting:</span>
tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]
 [ 4  8 12 16]
 [ 5 10 15 20]], shape=(5, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In the majority of cases, broadcasting proves to be efficient in terms of both time and space. This efficiency arises from the fact that the broadcast operation doesn’t physically create expanded tensors in memory <span id="id33">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>To observe the visual representation of broadcasting, you can use the <code class="docutils literal notranslate"><span class="pre">tf.broadcast_to</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import TensorFlow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Use tf.broadcast_to to demonstrate broadcasting</span>
<span class="n">broadcasted_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Print the broadcasted tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Broadcasted Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">broadcasted_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Broadcasted Tensor:</span>
tf.Tensor(
[[1 2 3]
 [1 2 3]
 [1 2 3]], shape=(3, 3), dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Unlike certain mathematical operations, the <code class="docutils literal notranslate"><span class="pre">broadcast_to</span></code> function doesn’t implement memory-saving mechanisms. In this case, you are essentially creating the expanded tensor in memory <span id="id34">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
</section>
<section id="tensor-shapes-and-indexing">
<h2><span class="section-number">12.3.9. </span>Tensor Shapes and Indexing<a class="headerlink" href="#tensor-shapes-and-indexing" title="Link to this heading">#</a></h2>
<p>Tensors have shapes that describe how many elements they contain along each dimension. To work with tensors effectively, we need to understand some concepts related to their shapes <span id="id35">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p><strong>Shape</strong>: The shape of a tensor is a vector that shows the length (number of elements) of each dimension. For example, a tensor with shape <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">3]</span></code> has two dimensions, and each dimension has three elements.</p></li>
<li><p><strong>Rank</strong>: The rank of a tensor is the number of dimensions it has. A scalar (a single number) has rank 0, a vector (a list of numbers) has rank 1, and a matrix (a table of numbers) has rank 2. The rank of a tensor is also the length of its shape vector.</p></li>
<li><p><strong>Axis</strong> or <strong>Dimension</strong>: An axis or a dimension of a tensor is a specific direction or dimension that the tensor has. For example, a matrix has two axes: the first axis (axis 0) is the rows, and the second axis (axis 1) is the columns. We can use the axis index to access or modify a slice of a tensor along that axis.</p></li>
<li><p><strong>Size</strong>: The size of a tensor is the total number of elements it contains. We can calculate the size of a tensor by multiplying the elements of its shape vector. For example, a tensor with shape <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">3]</span></code> has a size of 6.</p></li>
</ul>
<p>These concepts help us understand the structure and dimensions of tensors, which are essential for working with TensorFlow <span id="id36">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<section id="accessing-tensor-shapes-with-tf-tensorshape-objects">
<h3><span class="section-number">12.3.9.1. </span>Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects<a class="headerlink" href="#accessing-tensor-shapes-with-tf-tensorshape-objects" title="Link to this heading">#</a></h3>
<p>Both tensors and <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> objects have properties that allow us to access information about their shapes <span id="id37">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ndim</span></code></strong>: This property returns the rank of the tensor, which is the number of dimensions it has. For example, a scalar has <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of 0, a vector has <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of 1, and a matrix has <code class="docutils literal notranslate"><span class="pre">ndim</span></code> of 2.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">shape</span></code></strong>: This property gives a tuple of integers that represent the length of each dimension of the tensor. For example, a tensor with shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> has two dimensions, and each dimension has three elements.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">as_list()</span></code></strong>: This method converts the shape tuple to a Python list, which can be useful for manipulating or indexing the shape. For example, we can use <code class="docutils literal notranslate"><span class="pre">as_list()</span></code> to get the first element of the shape tuple, which is the size of the first dimension of the tensor.</p></li>
</ul>
<p>Let’s see how we can use these properties to access the shapes of tensors <span id="id38">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-4 Tensor</span>
<span class="n">rank_4_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Accessing shape information</span>
<span class="n">shape_object</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">ndim</span>
<span class="n">shape_list</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>

<span class="c1"># Accessing shape and other information</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of every element:&quot;</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of axes:&quot;</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of tensor:&quot;</span>  <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elements along axis 0 of tensor:&quot;</span>  <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elements along the last axis of tensor:&quot;</span>  <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total number of elements (3*2*4*5):&quot;</span>  <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">rank_4_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type of every element:				 &lt;dtype: &#39;float32&#39;&gt;
Number of axes:					 4
Shape of tensor:				 (3, 2, 4, 5)
Elements along axis 0 of tensor:		 3
Elements along the last axis of tensor:		 5
Total number of elements (3*2*4*5):		 120
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id61">
<a class="reference internal image-reference" href="../_images/4d_tensor.png"><img alt="../_images/4d_tensor.png" src="../_images/4d_tensor.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.10 </span><span class="caption-text">A rank-4 tensor, shape: [3, 2, 4, 5]. Image courtesy of TensorFlow documentation <span id="id39">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id61" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="indexing-tensors">
<h2><span class="section-number">12.3.10. </span>Indexing Tensors<a class="headerlink" href="#indexing-tensors" title="Link to this heading">#</a></h2>
<section id="single-axis-indexing">
<h3><span class="section-number">12.3.10.1. </span>Single-Axis Indexing<a class="headerlink" href="#single-axis-indexing" title="Link to this heading">#</a></h3>
<p>TensorFlow employs standard Python indexing conventions, akin to <a class="reference external" href="https://docs.python.org/3/tutorial/introduction.html#strings">indexing a list or a string in Python</a>, and similar to NumPy indexing <span id="id40">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Here are the fundamental rules <span id="id41">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p>Indexing begins at <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>Negative indices count in reverse from the end.</p></li>
<li><p>Slices are defined using colons, <code class="docutils literal notranslate"><span class="pre">:</span></code>, in the format <code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code>.</p></li>
</ul>
<p>These indexing principles facilitate efficient access and manipulation of tensor elements, enabling you to work with tensors just like you would with arrays or lists in Python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">201</span><span class="p">])</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Original Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Displaying the original tensor</span>

<span class="c1"># Indexing operations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>           <span class="c1"># Access the first element (index 0)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>          <span class="c1"># Access the second element (index 1)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Last:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>           <span class="c1"># Access the last element using negative index</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Everything:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>      <span class="c1"># Access all elements (the entire tensor)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before 4:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>       <span class="c1"># Access elements up to but not including index 4</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From 4 to the end:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access elements from index 4 to the end</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From 2, before 7:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access elements starting from index 2 up to but not including index 7</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Every other item:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access every other element (step size of 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reversed:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>     <span class="c1"># Access elements in reverse order</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Original Tensor:</span>
[  0   1   1   2   3   4   8  18  21  25 101 201]
First: 0
Second: 1
Last: 201
Everything: [  0   1   1   2   3   4   8  18  21  25 101 201]
Before 4: [0 1 1 2]
From 4 to the end: [  3   4   8  18  21  25 101 201]
From 2, before 7: [1 2 3 4 8]
Every other item: [  0   1   3   8  21 101]
Reversed: [201 101  25  21  18   8   4   3   2   1   1   0]
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[0].numpy()</span></code> retrieves the first element of the tensor, which is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[1].numpy()</span></code> retrieves the second element, which is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[-1].numpy()</span></code> retrieves the last element, which is <code class="docutils literal notranslate"><span class="pre">201</span></code>, using negative indexing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[:]</span></code> accesses the entire tensor, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">18,</span> <span class="pre">21,</span> <span class="pre">25,</span> <span class="pre">101,</span> <span class="pre">201]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[:4]</span></code> gets elements up to but not including index 4, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">2]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[4:]</span></code> retrieves elements from index 4 to the end, giving <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">18,</span> <span class="pre">21,</span> <span class="pre">25,</span> <span class="pre">101,</span> <span class="pre">201]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[2:7]</span></code> accesses elements starting from index 2 up to but not including index 7, yielding <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">8]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[::2]</span></code> retrieves every other element with a step size of 2, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">21,</span> <span class="pre">101]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[::-1]</span></code> accesses the elements in reverse order, giving <code class="docutils literal notranslate"><span class="pre">[201,</span> <span class="pre">101,</span> <span class="pre">25,</span> <span class="pre">21,</span> <span class="pre">18,</span> <span class="pre">8,</span> <span class="pre">4,</span> <span class="pre">3,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0]</span></code>.</p></li>
</ul>
</section>
<section id="multi-axis-indexing">
<h3><span class="section-number">12.3.10.2. </span>Multi-Axis Indexing<a class="headerlink" href="#multi-axis-indexing" title="Link to this heading">#</a></h3>
<p>For tensors with higher ranks (more dimensions), indexing involves passing multiple indices <span id="id42">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>. Crucially, the same rules as in the single-axis case apply to each axis independently. This means that you can apply the familiar indexing rules to each axis of the tensor, allowing for precise element selection within multi-dimensional structures <span id="id43">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a Rank-2 Tensor</span>
<span class="n">rank_2_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="c1"># Displaying the original tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Original Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="c1"># Indexing to retrieve a scalar value</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scalar Value at (1, 1):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="c1"># Indexing using a combination of integers and slices</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Second row:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Second column:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Last row:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First item in last column:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Skip the first row:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs [[4 5 6]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Original Tensor:</span>
[[1 2 3]
 [4 5 6]]

<span class=" -Color -Color-Bold -Color-Bold-Red">Scalar Value at (1, 1):</span>
5

<span class=" -Color -Color-Bold -Color-Bold-Red">Second row:</span>
[4 5 6]

<span class=" -Color -Color-Bold -Color-Bold-Red">Second column:</span>
[2 5]

<span class=" -Color -Color-Bold -Color-Bold-Red">Last row:</span>
[4 5 6]

<span class=" -Color -Color-Bold -Color-Bold-Red">First item in last column:</span>
3

<span class=" -Color -Color-Bold -Color-Bold-Red">Skip the first row:</span>
[[4 5 6]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a Rank-3 Tensor with Three Axes</span>
<span class="c1"># Here, we have a 3x2 grid of values, and</span>
<span class="c1"># there&#39;s an outer dimension encompassing them.</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">],</span> <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">]],</span>
                             <span class="p">])</span>

<span class="c1"># Displaying the Rank-3 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-3 Tensor with Three Axes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">)</span>

<span class="c1"># Indexing along all axes to retrieve a 2D tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2D Tensor along All Axes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-3 Tensor with Three Axes:</span>
tf.Tensor(
[[[11 12 13 14]
  [15 16 17 18]]

 [[21 22 23 24]
  [25 26 27 28]]

 [[31 32 33 34]
  [35 36 37 38]]], shape=(3, 2, 4), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">2D Tensor along All Axes:</span>
tf.Tensor(
[[14 18]
 [24 28]
 [34 38]], shape=(3, 2), dtype=int32)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id62">
<a class="reference internal image-reference" href="../_images/Multiaxis_indexing.png"><img alt="../_images/Multiaxis_indexing.png" src="../_images/Multiaxis_indexing.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.11 </span><span class="caption-text">Selecting the final feature from all 2d slices.</span><a class="headerlink" href="#id62" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="reshaping-tensor-shapes">
<h3><span class="section-number">12.3.10.3. </span>Reshaping Tensor Shapes<a class="headerlink" href="#reshaping-tensor-shapes" title="Link to this heading">#</a></h3>
<p>The ability to reshape a tensor is incredibly valuable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">101</span><span class="p">],</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="p">[</span><span class="mi">103</span><span class="p">],</span> <span class="p">[</span><span class="mi">104</span><span class="p">],</span> <span class="p">[</span><span class="mi">105</span><span class="p">]])</span>

<span class="c1"># Accessing shape information using shape property</span>
<span class="n">shape_object</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape as TensorShape object:&quot;</span><span class="p">,</span> <span class="n">shape_object</span><span class="p">)</span>

<span class="c1"># Converting TensorShape object to a Python list</span>
<span class="n">shape_list</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape as Python list:&quot;</span><span class="p">,</span> <span class="n">shape_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape as TensorShape object: (5, 1)
Shape as Python list: [5, 1]
</pre></div>
</div>
</div>
</div>
<p>“Facilitating the transformation of tensor structures into novel configurations is a task of inherent importance. Notably, the <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> operation stands out for its efficiency and resource efficiency, distinctively characterized by its avoidance of redundant data replication.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">101</span><span class="p">],</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="p">[</span><span class="mi">103</span><span class="p">],</span> <span class="p">[</span><span class="mi">104</span><span class="p">],</span> <span class="p">[</span><span class="mi">105</span><span class="p">]])</span>

<span class="c1"># Reshaping the tensor to a new shape</span>
<span class="n">reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Displaying original and reshaped shapes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Shape:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reshaped Shape:&quot;</span><span class="p">,</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Shape: (5, 1)
Reshaped Shape: (1, 5)
</pre></div>
</div>
</div>
</div>
<p>When reshaping a tensor in TensorFlow, the underlying data retains its original arrangement in memory. Instead of physically reorganizing the data, a new tensor with the desired shape is generated, while still referencing the same data in memory. TensorFlow follows a memory layout known as “row-major,” which means that the elements of a row are stored adjacently in memory. Consequently, incrementing the index on the rightmost side corresponds to traversing through memory in contiguous steps, effectively moving to the next element in the same row. This approach optimizes memory access patterns and aligns with the structure commonly used in C-style programming languages <span id="id44">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a Rank-3 Tensor with Three Axes</span>
<span class="c1"># Here, we have a 3x2 grid of values, and</span>
<span class="c1"># there&#39;s an outer dimension encompassing them.</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">]],</span>
                             <span class="p">[[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">],</span> <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">]],</span>
                             <span class="p">])</span>

<span class="c1"># Displaying the Rank-3 Tensor</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Rank-3 Tensor with Three Axes:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Rank-3 Tensor with Three Axes:</span>
tf.Tensor(
[[[11 12 13 14]
  [15 16 17 18]]

 [[21 22 23 24]
  [25 26 27 28]]

 [[31 32 33 34]
  [35 36 37 38]]], shape=(3, 2, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Flattening a tensor provides insight into the sequence in which its elements are organized within memory <span id="id45">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the tensor using &#39;-1&#39;</span>
<span class="c1"># The &#39;-1&#39; indicates TensorFlow should determine the size along this axis automatically</span>
<span class="n">flattened_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Print the flattened tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">flattened_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([11 12 13 14 15 16 17 18 21 22 23 24 25 26 27 28 31 32 33 34 35 36 37 38], shape=(24,), dtype=int32)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id63">
<a class="reference internal image-reference" href="../_images/flattening_rank_3_tensor.png"><img alt="../_images/flattening_rank_3_tensor.png" src="../_images/flattening_rank_3_tensor.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.12 </span><span class="caption-text">Flattening a rank 3 tensor.</span><a class="headerlink" href="#id63" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In this code, we’re using the -1 value in the shape argument of the tf.reshape function. This value essentially tells TensorFlow to calculate the size along that axis so that the total number of elements remains constant. The comments explain how the -1 value works and its purpose in the reshaping process.</p>
<p>Typically, the <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> function is most useful when you need to merge or split neighboring axes, or when you want to add or remove dimensions with a size of <code class="docutils literal notranslate"><span class="pre">1</span></code> <span id="id46">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>For the given 3x2x5 tensor, reshaping it into a (3x2)x5 format or into a 3x(2x5) format are both logical operations. This is because the reshaping maintains separate slices that do not overlap, allowing you to rearrange the data without mixing elements <span id="id47">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p>Reshaping to (3x2)x5: This results in three rows of two-by-five matrices, effectively combining the inner dimensions. Each new row holds a set of elements that originally resided in different inner matrices.</p></li>
<li><p>Reshaping to 3x(2x5): This keeps the outermost dimension (3) intact while splitting the second dimension (2x5) into separate inner matrices. Each new column of the reshaped tensor contains elements from one of the original 2x5 matrices. Both reshaping approaches maintain the data integrity and relationships between elements <span id="id48">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;Reshape to (6, 4):&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Reshape to (3, 8):&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">Reshape to (6, 4):</span>
tf.Tensor(
[[11 12 13 14]
 [15 16 17 18]
 [21 22 23 24]
 [25 26 27 28]
 [31 32 33 34]
 [35 36 37 38]], shape=(6, 4), dtype=int32)

<span class=" -Color -Color-Bold -Color-Bold-Red">Reshape to (3, 8):</span>
tf.Tensor(
[[11 12 13 14 15 16 17 18]
 [21 22 23 24 25 26 27 28]
 [31 32 33 34 35 36 37 38]], shape=(3, 8), dtype=int32)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="id64">
<a class="reference internal image-reference" href="../_images/tf_reshape.png"><img alt="../_images/tf_reshape.png" src="../_images/tf_reshape.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.13 </span><span class="caption-text">Visualizing reshaping <code class="docutils literal notranslate"><span class="pre">rank_3_tensor</span></code> tensor.</span><a class="headerlink" href="#id64" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Reshaping a tensor can be successfully performed for any new shape that maintains the same total count of elements. However, it’s important to note that reshaping won’t yield meaningful results if the arrangement of axes isn’t honored <span id="id49">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>If you intend to rearrange the order of axes, using <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> won’t suffice. Instead, for swapping or permuting axes, the appropriate function to use is <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code>. Reshaping merely alters the dimensions while preserving the sequence of elements, ensuring that you maintain the correct data relationships. Conversely, when you need to modify the order of the dimensions, such as swapping axes, <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code> is the suitable choice to achieve that specific transformation <span id="id50">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a Rank-3 tensor</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">]],</span>
<span class="p">])</span>

<span class="c1"># Examples of incorrect reshaping</span>

<span class="c1"># You can&#39;t reorder axes with reshape.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cannot reorder axes with reshape:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Incorrect dimensions specified.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Incorrect dimensions specified:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Incompatible reshape dimensions.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Incompatible reshape dimensions:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cannot reorder axes with reshape:
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]
  [10 11 12 13 14]]

 [[15 16 17 18 19]
  [20 21 22 23 24]
  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) 

Incorrect dimensions specified:
tf.Tensor(
[[ 0  1  2  3  4  5]
 [ 6  7  8  9 10 11]
 [12 13 14 15 16 17]
 [18 19 20 21 22 23]
 [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) 

Incompatible reshape dimensions:
InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]
</pre></div>
</div>
</div>
</div>
<p><span id="id51">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span></p>
<figure class="align-center" id="id65">
<a class="reference internal image-reference" href="../_images/tf_reshape_bad.png"><img alt="../_images/tf_reshape_bad.png" src="../_images/tf_reshape_bad.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.14 </span><span class="caption-text">Visualizing reshaping <code class="docutils literal notranslate"><span class="pre">rank_3_tensor</span></code> tensor (incorrect reshaping). Image courtesy of TensorFlow documentation <span id="id52">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id65" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="ragged-tensors">
<h2><span class="section-number">12.3.11. </span>Ragged Tensors<a class="headerlink" href="#ragged-tensors" title="Link to this heading">#</a></h2>
<p>A tensor that possesses varying numbers of elements along a specific axis is referred to as “ragged.” To handle such data, the <code class="docutils literal notranslate"><span class="pre">tf.ragged.RaggedTensor</span></code> class comes into play <span id="id53">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>For instance, the following data structure cannot be accurately represented using a standard tensor <span id="id54">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<figure class="align-center" id="id66">
<a class="reference internal image-reference" href="../_images/ragged.png"><img alt="../_images/ragged.png" src="../_images/ragged.png" style="width: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12.15 </span><span class="caption-text">A <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> with a shape of [4, None] aptly addresses this scenario.  Image courtesy of TensorFlow documentation <span id="id55">[<a class="reference internal" href="../References.html#id54" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id66" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a ragged list</span>
<span class="n">ragged_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">]]</span>

<span class="c1"># Attempt to create a tensor from the ragged list</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">ragged_list</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error: ValueError: Can&#39;t convert non-rectangular Python sequence to Tensor.
</pre></div>
</div>
</div>
</div>
<p>Instead create a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> using <code class="docutils literal notranslate"><span class="pre">tf.ragged.constant</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a ragged list</span>
<span class="n">ragged_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">]]</span>

<span class="c1"># Create a tf.RaggedTensor using tf.ragged.constant</span>
<span class="n">ragged_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">ragged_list</span><span class="p">)</span>

<span class="c1"># Print the ragged tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ragged Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ragged_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ragged Tensor:
&lt;tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]&gt;
</pre></div>
</div>
</div>
</div>
<p>The shape of a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> will contain some axes with unknown lengths:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ragged_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, None)
</pre></div>
</div>
</div>
</div>
<p>You can learn more about ragged tensors <a class="reference external" href="https://www.tensorflow.org/guide/ragged_tensor">here</a>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG680_C12S02.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.2. </span>Fundamentals of Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG680_C12S04.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12.4. </span>Introduction to Variables</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#array-and-tensor-connection">12.3.1. Array and Tensor Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-ties-to-numpy-arrays">12.3.2. Strong Ties to NumPy Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-or-rank-0-tensor">12.3.3. Scalar or Rank-0 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-or-rank-1-tensor">12.3.4. Vector or Rank-1 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-or-rank-2-tensor">12.3.5. Matrix or Rank-2 Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-tensors-to-numpy-arrays">12.3.6. Converting Tensors to NumPy Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types-in-tensorflow">12.3.7. Data Types in TensorFlow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">12.3.8. Broadcasting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shapes-and-indexing">12.3.9. Tensor Shapes and Indexing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-shapes-with-tf-tensorshape-objects">12.3.9.1. Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-tensors">12.3.10. Indexing Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-axis-indexing">12.3.10.1. Single-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-axis-indexing">12.3.10.2. Multi-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensor-shapes">12.3.10.3. Reshaping Tensor Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ragged-tensors">12.3.11. Ragged Tensors</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>