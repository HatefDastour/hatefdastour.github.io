

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12.3. Tensors and Variables &#8212; Introduction to Digital Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_12/ENGG_680_C12S3';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.4. Building a linear Regression Model" href="ENGG_680_C12S4.html" />
    <link rel="prev" title="12.2. Fundamentals of Neural Networks" href="ENGG_680_C12S2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/ReadMe.html">1. Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S1.html">1.1. Variable names, Expressions and statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S2.html">1.2. Fundamental Concepts and Operations in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/ENGG_680_C01S3.html">1.3. Functions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_02/ReadMe.html">2. Control Flow: Conditionals, Recursion, and Iteration</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S1.html">2.1. Conditionals and recursion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02/ENGG_680_C02S2.html">2.2. Iteration</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/ReadMe.html">3. Data Structures and File Handling in Python</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S1.html">3.1. Strings in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S2.html">3.2. Python Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S3.html">3.3. Python Dictionaries: Key-Value Pairs in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S4.html">3.4. Python Tuples: Immutable Sequences in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S5.html">3.5. Python Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/ENGG_680_C03S6.html">3.6. Files in Python (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/ReadMe.html">4. Classes and Objects</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S1.html">4.1. Introduction to Classes and Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S2.html">4.2. Inheritance and Polymorphism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S3.html">4.3. Encapsulation and Abstraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S4.html">4.4. Copying Objects (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S5.html">4.5. Magic Methods (Dunder Methods) in Python (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/ENGG_680_C04S6.html">4.6. Examples of Classes (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/ReadMe.html">5. Introduction to NumPy</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S1.html">5.1. Basics of Numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S2.html">5.2. NumPy Function Reference and Usage Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/ENGG_680_C05S3.html">5.3. Advanced Numpy Concepts (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_06/ReadMe.html">6. Working with Data using Pandas</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S1.html">6.1. An Introduction to Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S2.html">6.2. DataFrame and Series Indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S3.html">6.3. Pandas Data Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S4.html">6.4. Handling Missing Data in Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S5.html">6.5. Combining Datasets using Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06/ENGG_680_C06S6.html">6.6. Aggregation and Grouping in Pandas</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_07/ReadMe.html">7. Data Visualization using Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S1.html">7.1. Getting started with Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S2.html">7.2. Matplotlib Styles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S3.html">7.3. Matplotlib interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S4.html">7.4. Adjusting the Plot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S5.html">7.5. Seaborn plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_07/ENGG_680_C07S6.html">7.6. Python Plotting Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_08/ReadMe.html">8. An Introduction to Computer Vision</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S1.html">8.1. Getting Started with OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S2.html">8.2. Geometric Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S3.html">8.3. Image Thresholding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S4.html">8.4. Image Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S5.html">8.5. Drawing Functions (Optional Section)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08/ENGG_680_C08S6.html">8.6. Morphological Transformations (Optional Section)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_09/ReadMe.html">9. An Introduction to Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S1.html">9.1. Statistical Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S2.html">9.2. An Introduction to Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S3.html">9.3. Multiple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S4.html">9.4. Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S5.html">9.5. K-Nearest Neighbors (K-NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S6.html">9.6. Resampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_09/ENGG_680_C09S7.html">9.7. Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_10/ReadMe.html">10. Tree-Based Methods</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S1.html">10.1. Fundamental Structure of Decision Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S2.html">10.2. Regression Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S3.html">10.3. Classification Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S4.html">10.4. Regression Trees and Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S5.html">10.5. Enhancing Decision Trees with Bagging: An Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S6.html">10.6. Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_10/ENGG_680_C10S7.html">10.7. Gradient Boosting</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_11/ReadMe.html">11. Dimensionality Reduction and Feature Selection</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S1.html">11.1. Introduction to Dimensionality Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S2.html">11.2. Principal Components Analysis (PCA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S3.html">11.3. t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S4.html">11.4. Linear and Quadratic Discriminant Analyses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S5.html">11.5. Recursive Feature Elimination (RFE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_11/ENGG_680_C11S6.html">11.6. Practical Considerations in Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ReadMe.html">12. Introduction to Deep Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S1.html">12.1. Understanding Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S2.html">12.2. Fundamentals of Neural Networks</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.3. Tensors and Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S4.html">12.4. Building a linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S5.html">12.5. Building a Logistic Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S6.html">12.6. Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S7.html">12.7. Image classification with TensorFlow</a></li>
<li class="toctree-l2"><a class="reference internal" href="ENGG_680_C12S8.html">12.8. Image Augmentations with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendixes/ReadMe.html">13. Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendixes/SampleQuestions_1.html">13.1. Sample Questions 1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">14. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tensors and Variables</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-basics">12.3.1. TensorFlow Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#array-and-tensor-connection">12.3.1.1. Array and Tensor Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-ties-to-numpy-arrays">12.3.1.2. Strong Ties to NumPy Arrays</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-or-rank-0-tensor">12.3.1.3. “Scalar” or “Rank-0” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-or-rank-1-tensor">12.3.1.4. “Vector” or “Rank-1” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-or-rank-2-tensor">12.3.1.5. “Matrix” or “Rank-2” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-tensors-to-numpy-arrays">12.3.1.6. Converting Tensors to NumPy Arrays</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performing-basic-math-operations-on-tensors">12.3.1.7. Performing Basic Math Operations on Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-in-various-operations-ops">12.3.1.8. Tensors in Various Operations (Ops)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-conversion-flexibility">12.3.1.9. Tensor Conversion Flexibility</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shapes-and-indexing">12.3.2. Tensor Shapes and Indexing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-shapes-with-tf-tensorshape-objects">12.3.2.1. Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-tensors">12.3.3. Indexing Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-axis-indexing">12.3.3.1. Single-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-axis-indexing">12.3.3.2. Multi-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensor-shapes">12.3.3.3. Reshaping Tensor Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types-in-tensorflow">12.3.4. Data Types in TensorFlow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">12.3.4.1. Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-convert-to-tensor">12.3.4.2. tf.convert_to_tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ragged-tensors">12.3.4.3. Ragged Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#string-tensors">12.3.4.4. String Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-tensors">12.3.4.5. Sparse Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-variables">12.3.5. Introduction to Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-variable">12.3.5.1. Creating a Variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lifecycles-naming-and-watching">12.3.5.2. Lifecycles, Naming, and Watching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-names-saving-and-gradients">12.3.5.3. Variable Names, Saving, and Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#placing-variables-and-tensors">12.3.5.4. Placing Variables and Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-device-placement">12.3.5.5. Cross-Device Placement</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="tensors-and-variables">
<h1><span class="section-number">12.3. </span>Tensors and Variables<a class="headerlink" href="#tensors-and-variables" title="Permalink to this headline">#</a></h1>
<p>Deep learning is a powerful field of artificial intelligence that uses neural networks to make sense of data and perform tasks like recognizing images, understanding natural language, and generating speech. These neural networks consist of intricate layers of nodes that perform complex calculations on data inputs and outputs. To efficiently handle this data and speed up these calculations, deep learning relies on a special data structure called tensors.</p>
<p>Tensors can be thought of as a more advanced version of the familiar arrays we often use in programming and mathematics. Arrays are collections of elements organized in rows and columns, and tensors share a strong connection with them. Let’s explore how they are similar <span id="id1">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="section" id="tensorflow-basics">
<h2><span class="section-number">12.3.1. </span>TensorFlow Basics<a class="headerlink" href="#tensorflow-basics" title="Permalink to this headline">#</a></h2>
<div class="section" id="array-and-tensor-connection">
<h3><span class="section-number">12.3.1.1. </span>Array and Tensor Connection<a class="headerlink" href="#array-and-tensor-connection" title="Permalink to this headline">#</a></h3>
<p><strong>1. Multidimensional Flexibility:</strong> Arrays, with their ability to have multiple dimensions, lay the foundation for tensors. In arrays, dimensions are like the arrangement of data in a musical score, with rows and columns. Tensors take this idea further, extending dimensions into a multi-axis arrangement that can adapt to any kind of data structure.</p>
<p><strong>2. Indexing and Access:</strong> Just like you use indices to access elements in arrays, tensors work the same way for retrieving data. Elements inside both arrays and tensors are accessed and manipulated using indices, making data handling consistent.</p>
<p><strong>3. Mathematical Aptitude:</strong> Arrays have a well-established reputation for their mathematical capabilities, crucial for performing various operations. Tensors, as their successors, continue this mathematical legacy, serving as the conduits through which deep learning networks perform complex numerical operations.</p>
<p><strong>4. Data Representation:</strong> Similar to how arrays are used to represent data, tensors take center stage in deep learning. They excel at accurately encapsulating data, whether it’s images, text, or sound. These versatile data structures channel the essence of information into the neural network’s computational fabric.</p>
<p><strong>5. Transformations and Operations:</strong> Both arrays and tensors are alike in their transformative nature. They gracefully adapt within their dimensions to accommodate a range of mathematical operations, such as addition, multiplication, and transformation. Just as arrays change during calculations, tensors efficiently absorb operations, enhancing the capabilities of neural networks.</p>
</div>
<div class="section" id="strong-ties-to-numpy-arrays">
<h3><span class="section-number">12.3.1.2. </span>Strong Ties to NumPy Arrays<a class="headerlink" href="#strong-ties-to-numpy-arrays" title="Permalink to this headline">#</a></h3>
<p>While tensors may seem like a novel concept in the context of deep learning, it’s important to recognize their strong connection to arrays, which have long been fundamental in programming and mathematics. Arrays can be seen as simplified versions of tensors, often with fixed dimensions and data types.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Definition</p>
<p>Tensors are specialized data structures that resemble multi-dimensional arrays, and they come with a consistent data type, which is referred to as a ‘dtype.’ The available data types can be found in <code class="docutils literal notranslate"><span class="pre">tf.dtypes.DType</span></code>. An important trait of tensors is their immutability, which is akin to how Python treats numbers and strings. This means that you cannot modify the content of an existing tensor; instead, you create a new tensor when changes are needed <span id="id2">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
<p>To start our journey into the world of TensorFlow, we’ll dive into the creation of basic tensors. Tensors serve as the building blocks of TensorFlow, empowering us to work with data in a powerful and flexible manner <span id="id3">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
<div class="section" id="scalar-or-rank-0-tensor">
<h3><span class="section-number">12.3.1.3. </span>“Scalar” or “Rank-0” Tensor<a class="headerlink" href="#scalar-or-rank-0-tensor" title="Permalink to this headline">#</a></h3>
<p>Introducing a “Scalar” or “Rank-0” Tensor: This special tensor type holds a solitary value and lacks any “axes” or dimensions <span id="id4">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-0 Tensor (Scalar)</span>
<span class="c1"># By default, this will be an int32 tensor; more on &quot;dtypes&quot; below.</span>
<span class="n">rank_0_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-0 Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_0_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(4, shape=(), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Here’s a breakdown of what’s happening:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.constant(4)</span></code>: This part uses TensorFlow’s <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code> function to create a tensor with a constant value. In this case, the value provided is 4. This function is used to define tensors that hold fixed values and cannot be changed later.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_0_tensor</span> <span class="pre">=</span> <span class="pre">...</span></code>: We assign the tensor created by <code class="docutils literal notranslate"><span class="pre">tf.constant(4)</span></code> to the variable named <code class="docutils literal notranslate"><span class="pre">rank_0_tensor</span></code>. This variable will be used to refer to this specific tensor in the code.</p></li>
</ol>
<p>So, the overall line of code is creating a scalar tensor with a value of 4 and giving it a name <code class="docutils literal notranslate"><span class="pre">rank_0_tensor</span></code> that we can use to access and manipulate this tensor in our code.</p>
<div class="figure align-center" id="id67">
<a class="reference internal image-reference" href="../_images/scalar.png"><img alt="../_images/scalar.png" src="../_images/scalar.png" style="width: 100px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.5 </span><span class="caption-text">A Scalar Tensor: Shape - []. Image courtesy of TensorFlow documentation <span id="id5">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id67" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="vector-or-rank-1-tensor">
<h3><span class="section-number">12.3.1.4. </span>“Vector” or “Rank-1” Tensor<a class="headerlink" href="#vector-or-rank-1-tensor" title="Permalink to this headline">#</a></h3>
<p>Introducing a “Vector” or “Rank-1” Tensor: Think of this tensor as a sequence of values, similar to a list. A vector possesses a single axis, giving it a structured direction <span id="id6">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-1 Tensor (Vector)</span>
<span class="c1"># We specify float values to make it a float tensor.</span>
<span class="n">rank_1_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>

<span class="c1"># Displaying the Rank-1 Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_1_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we utilize TensorFlow to craft a Rank-1 tensor, also referred to as a vector tensor. Vectors can be thought of as sequences of values, much like lists. Here, we’re creating a float tensor by providing float values within the <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code> function.</p>
<p>The line <code class="docutils literal notranslate"><span class="pre">rank_1_tensor</span> <span class="pre">=</span> <span class="pre">tf.constant([2.0,</span> <span class="pre">3.0,</span> <span class="pre">4.0])</span></code> creates the Rank-1 tensor, and then <code class="docutils literal notranslate"><span class="pre">print(rank_1_tensor)</span></code> is used to exhibit the contents of the tensor. This tensor, with a single axis, forms a structured direction for the sequence of values it holds.</p>
<div class="figure align-center" id="id68">
<a class="reference internal image-reference" href="../_images/vector.png"><img alt="../_images/vector.png" src="../_images/vector.png" style="width: 150px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.6 </span><span class="caption-text">A vector, shape: [3]. Image courtesy of TensorFlow documentation <span id="id7">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id68" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="matrix-or-rank-2-tensor">
<h3><span class="section-number">12.3.1.5. </span>“Matrix” or “Rank-2” Tensor<a class="headerlink" href="#matrix-or-rank-2-tensor" title="Permalink to this headline">#</a></h3>
<p>Introducing a “Matrix” or “Rank-2” Tensor: Envision this tensor as a grid of values, resembling a two-dimensional array. A matrix is defined by two axes, offering a clear row and column structure <span id="id8">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-2 Tensor (Matrix) with Specific Dtype</span>
<span class="c1"># We define a 2x2 matrix and set the data type to float16.</span>
<span class="n">rank_2_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                             <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="c1"># Displaying the Rank-2 Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[[1. 2.]
 [3. 4.]
 [5. 6.]], shape=(3, 2), dtype=float16)
</pre></div>
</div>
</div>
</div>
<div class="figure align-center" id="id69">
<a class="reference internal image-reference" href="../_images/matrix.png"><img alt="../_images/matrix.png" src="../_images/matrix.png" style="width: 150px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.7 </span><span class="caption-text">A matrix, shape: [3, 2]. Image courtesy of TensorFlow documentation <span id="id9">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id69" title="Permalink to this image">#</a></p>
</div>
<p>Tensors Can Have More Axes: Here’s an Example with Three Axes <span id="id10">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-3 Tensor with Three Axes</span>
<span class="c1"># Here, we have a 3x2 grid of values, and there&#39;s an outer dimension encompassing them.</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">]],</span>
<span class="p">])</span>

<span class="c1"># Displaying the Rank-3 Tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="figure align-center" id="id70">
<img alt="../_images/3d_tensor.png" src="../_images/3d_tensor.png" />
<p class="caption"><span class="caption-number">Fig. 12.8 </span><span class="caption-text">Visualizing Tensors with More Than Two Axes. Image courtesy of TensorFlow documentation <span id="id11">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id70" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="converting-tensors-to-numpy-arrays">
<h3><span class="section-number">12.3.1.6. </span>Converting Tensors to NumPy Arrays<a class="headerlink" href="#converting-tensors-to-numpy-arrays" title="Permalink to this headline">#</a></h3>
<p>You can easily convert a tensor into a NumPy array using either the <code class="docutils literal notranslate"><span class="pre">np.array</span></code> function or the <code class="docutils literal notranslate"><span class="pre">tensor.numpy</span></code> method <span id="id12">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Using np.array to convert to a NumPy array</span>
<span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

<span class="c1"># Using the tensor.numpy() method</span>
<span class="n">numpy_array_method</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Converted using np.array:&quot;</span><span class="p">,</span> <span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Converted using tensor.numpy():&quot;</span><span class="p">,</span> <span class="n">numpy_array_method</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Converted using np.array: [1 2 3]
Converted using tensor.numpy(): [1 2 3]
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we first create a tensor using TensorFlow’s <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code>. Then, we convert the tensor to a NumPy array using two approaches: the <code class="docutils literal notranslate"><span class="pre">np.array</span></code> function and the <code class="docutils literal notranslate"><span class="pre">tensor.numpy()</span></code> method. Both methods achieve the same result.</p>
<p>The output will show the NumPy arrays created from the tensor using both conversion methods. This conversion allows you to seamlessly use TensorFlow tensors and NumPy arrays together in your code.</p>
</div>
<div class="section" id="performing-basic-math-operations-on-tensors">
<h3><span class="section-number">12.3.1.7. </span>Performing Basic Math Operations on Tensors<a class="headerlink" href="#performing-basic-math-operations-on-tensors" title="Permalink to this headline">#</a></h3>
<p>Tensors enable you to perform various fundamental mathematical operations, such as addition, element-wise multiplication, and matrix multiplication <span id="id13">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating tensors</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>

<span class="c1"># Addition</span>
<span class="n">sum_result</span> <span class="o">=</span> <span class="n">tensor1</span> <span class="o">+</span> <span class="n">tensor2</span>

<span class="c1"># Element-wise multiplication</span>
<span class="n">elementwise_product</span> <span class="o">=</span> <span class="n">tensor1</span> <span class="o">*</span> <span class="n">tensor2</span>

<span class="c1"># Matrix multiplication</span>
<span class="n">matmul_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span> <span class="c1"># or tensor1 @ tensor2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum Result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sum_result</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Element-wise Product:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">elementwise_product</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrix Multiplication Result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">matmul_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sum Result:
 tf.Tensor(
[[ 6  8]
 [10 12]], shape=(2, 2), dtype=int32)
Element-wise Product:
 tf.Tensor(
[[ 5 12]
 [21 32]], shape=(2, 2), dtype=int32)
Matrix Multiplication Result:
 tf.Tensor(
[[19 22]
 [43 50]], shape=(2, 2), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this code example, two tensors <code class="docutils literal notranslate"><span class="pre">tensor1</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor2</span></code> are created using <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code>. We then perform basic mathematical operations:</p>
<ul class="simple">
<li><p><strong>Addition:</strong> We add the tensors element-wise using the <code class="docutils literal notranslate"><span class="pre">+</span></code> operator.</p></li>
<li><p><strong>Element-wise Multiplication:</strong> We multiply the tensors element-wise using the <code class="docutils literal notranslate"><span class="pre">*</span></code> operator.</p></li>
<li><p><strong>Matrix Multiplication:</strong> We use the <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code> function to perform matrix multiplication between the two tensors.</p></li>
</ul>
<p>The results of these operations are displayed using <code class="docutils literal notranslate"><span class="pre">print</span></code>. Tensors make it convenient to perform mathematical calculations, making them a fundamental component of deep learning and scientific computing.</p>
</div>
<div class="section" id="tensors-in-various-operations-ops">
<h3><span class="section-number">12.3.1.8. </span>Tensors in Various Operations (Ops)<a class="headerlink" href="#tensors-in-various-operations-ops" title="Permalink to this headline">#</a></h3>
<p>Tensors are central to a wide range of operations, often referred to as “Ops,” in TensorFlow. These operations cover tasks such as computation, manipulation, and transformation of data <span id="id14">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>. Here are some examples <span id="id15">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>

<span class="c1"># Find the largest value in the tensor</span>
<span class="n">max_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Largest Value:&quot;</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>

<span class="c1"># Find the index of the largest value in the tensor</span>
<span class="n">index_of_max</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index of Largest Value:&quot;</span><span class="p">,</span> <span class="n">index_of_max</span><span class="p">)</span>

<span class="c1"># Compute the softmax of the tensor</span>
<span class="n">softmax_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax Result:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">softmax_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Largest Value: tf.Tensor(10.0, shape=(), dtype=float32)
Index of Largest Value: tf.Tensor([1 0], shape=(2,), dtype=int64)
Softmax Result:
 tf.Tensor(
[[2.6894143e-01 7.3105860e-01]
 [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this section, we use the given tensor <code class="docutils literal notranslate"><span class="pre">c</span></code> to demonstrate different operations:</p>
<ul class="simple">
<li><p><strong>Finding the Largest Value:</strong> We use <code class="docutils literal notranslate"><span class="pre">tf.reduce_max</span></code> to find the largest value within the tensor.</p></li>
<li><p><strong>Finding the Index of the Largest Value:</strong> Using <code class="docutils literal notranslate"><span class="pre">tf.math.argmax</span></code>, we determine the index of the largest value in the tensor.</p></li>
<li><p><strong>Computing Softmax:</strong> The <code class="docutils literal notranslate"><span class="pre">tf.nn.softmax</span></code> function computes the softmax values for each element in the tensor, transforming them into a probability distribution.</p></li>
</ul>
<p>These operations showcase the versatility and utility of tensors in performing diverse calculations, a critical aspect of TensorFlow’s capabilities.</p>
</div>
<div class="section" id="tensor-conversion-flexibility">
<h3><span class="section-number">12.3.1.9. </span>Tensor Conversion Flexibility<a class="headerlink" href="#tensor-conversion-flexibility" title="Permalink to this headline">#</a></h3>
<p>TensorFlow offers a convenient feature: in most cases where a TensorFlow function requires a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> as input, you can also use anything that can be transformed into a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> using <code class="docutils literal notranslate"><span class="pre">tf.convert_to_tensor</span></code>. This flexibility simplifies the process of working with different data types <span id="id16">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Here’s an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># A NumPy array</span>
<span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="c1"># Converting the NumPy array to a TensorFlow tensor</span>
<span class="n">tensor_from_numpy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>

<span class="c1"># Performing a simple operation</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tensor_from_numpy</span> <span class="o">*</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original NumPy Array:&quot;</span><span class="p">,</span> <span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor from NumPy Array:&quot;</span><span class="p">,</span> <span class="n">tensor_from_numpy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result after Operation:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original NumPy Array: [1. 2. 3.]
Tensor from NumPy Array: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)
Result after Operation: tf.Tensor([2. 4. 6.], shape=(3,), dtype=float64)
</pre></div>
</div>
</div>
</div>
<p>In this example, we start with a NumPy array called <code class="docutils literal notranslate"><span class="pre">numpy_array</span></code>. By using <code class="docutils literal notranslate"><span class="pre">tf.convert_to_tensor</span></code>, we effortlessly convert this array into a TensorFlow tensor. Subsequently, we can perform a mathematical operation on the tensor, demonstrating the seamless compatibility between NumPy arrays and TensorFlow tensors.</p>
<p>This feature allows for smooth integration of different data types within TensorFlow functions, enhancing the convenience and versatility of the TensorFlow library.</p>
</div>
</div>
<div class="section" id="tensor-shapes-and-indexing">
<h2><span class="section-number">12.3.2. </span>Tensor Shapes and Indexing<a class="headerlink" href="#tensor-shapes-and-indexing" title="Permalink to this headline">#</a></h2>
<p>Tensors come with associated shapes, and certain terms help describe this concept <span id="id17">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p><strong>Shape</strong>: Refers to the length (count of elements) along each axis of a tensor.</p></li>
<li><p><strong>Rank</strong>: Indicates the number of tensor axes. For example, a scalar has rank 0, a vector is rank 1, and a matrix has rank 2.</p></li>
<li><p><strong>Axis</strong> or <strong>Dimension</strong>: Represents a specific direction or dimension of a tensor.</p></li>
<li><p><strong>Size</strong>: Denotes the total count of items in the tensor, achieved by multiplying the elements of the shape vector.</p></li>
</ul>
<p>These terms collectively help us grasp the structure and dimensions of tensors, essential aspects for working effectively with TensorFlow <span id="id18">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="section" id="accessing-tensor-shapes-with-tf-tensorshape-objects">
<h3><span class="section-number">12.3.2.1. </span>Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects<a class="headerlink" href="#accessing-tensor-shapes-with-tf-tensorshape-objects" title="Permalink to this headline">#</a></h3>
<p>Both tensors and <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> objects provide convenient properties for accessing this information <span id="id19">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">ndim</span></code></strong>: Returns the rank (number of axes) of the tensor.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">shape</span></code></strong>: Yields a tuple of integers representing the shape of the tensor.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">as_list()</span></code></strong>: Converts the shape to a Python list.</p></li>
</ul>
<p>Let’s explore how to use these properties <span id="id20">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-4 Tensor</span>
<span class="n">rank_4_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Accessing shape information</span>
<span class="n">shape_object</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">ndim</span>
<span class="n">shape_list</span> <span class="o">=</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>

<span class="c1"># Accessing shape and other information</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of every element:&quot;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of axes:&quot;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of tensor:&quot;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elements along axis 0 of tensor:&quot;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elements along the last axis of tensor:&quot;</span><span class="p">,</span> <span class="n">rank_4_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total number of elements (3*2*4*5):&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">rank_4_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type of every element: &lt;dtype: &#39;float32&#39;&gt;
Number of axes: 4
Shape of tensor: (3, 2, 4, 5)
Elements along axis 0 of tensor: 3
Elements along the last axis of tensor: 5
Total number of elements (3*2*4*5): 120
</pre></div>
</div>
</div>
</div>
<p>In this code snippet,</p>
<ul class="simple">
<li><p><strong>Type of Every Element</strong>: This prints the data type of each element in the tensor.</p></li>
<li><p><strong>Number of Axes</strong>: This reveals the rank (number of axes) of the tensor.</p></li>
<li><p><strong>Shape of Tensor</strong>: This displays the shape of the tensor as a tuple.</p></li>
<li><p><strong>Elements Along Axis 0</strong>: This specifies the number of elements along the first axis.</p></li>
<li><p><strong>Elements Along the Last Axis</strong>: This specifies the number of elements along the last axis.</p></li>
<li><p><strong>Total Number of Elements</strong>: This computes and prints the total number of elements in the tensor using <code class="docutils literal notranslate"><span class="pre">tf.size(rank_4_tensor)</span></code>.</p></li>
</ul>
<div class="figure align-center" id="id71">
<a class="reference internal image-reference" href="../_images/4d_tensor.png"><img alt="../_images/4d_tensor.png" src="../_images/4d_tensor.png" style="width: 450px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.9 </span><span class="caption-text">A rank-4 tensor, shape: [3, 2, 4, 5]. Image courtesy of TensorFlow documentation <span id="id21">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id71" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
<div class="section" id="indexing-tensors">
<h2><span class="section-number">12.3.3. </span>Indexing Tensors<a class="headerlink" href="#indexing-tensors" title="Permalink to this headline">#</a></h2>
<div class="section" id="single-axis-indexing">
<h3><span class="section-number">12.3.3.1. </span>Single-Axis Indexing<a class="headerlink" href="#single-axis-indexing" title="Permalink to this headline">#</a></h3>
<p>TensorFlow employs standard Python indexing conventions, akin to <a class="reference external" href="https://docs.python.org/3/tutorial/introduction.html#strings">indexing a list or a string in Python</a>, and similar to NumPy indexing <span id="id22">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Here are the fundamental rules <span id="id23">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p>Indexing begins at <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p>Negative indices count in reverse from the end.</p></li>
<li><p>Slices are defined using colons, <code class="docutils literal notranslate"><span class="pre">:</span></code>, in the format <code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code>.</p></li>
</ul>
<p>These indexing principles facilitate efficient access and manipulation of tensor elements, enabling you to work with tensors just like you would with arrays or lists in Python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">201</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Displaying the original tensor</span>

<span class="c1"># Indexing operations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>           <span class="c1"># Access the first element (index 0)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>          <span class="c1"># Access the second element (index 1)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Last:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>           <span class="c1"># Access the last element using negative index</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Everything:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>      <span class="c1"># Access all elements (the entire tensor)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before 4:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>       <span class="c1"># Access elements up to but not including index 4</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From 4 to the end:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access elements from index 4 to the end</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From 2, before 7:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access elements starting from index 2 up to but not including index 7</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Every other item:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Access every other element (step size of 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reversed:&quot;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>     <span class="c1"># Access elements in reverse order</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[  0   1   1   2   3   4   8  18  21  25 101 201]
First: 0
Second: 1
Last: 201
Everything: [  0   1   1   2   3   4   8  18  21  25 101 201]
Before 4: [0 1 1 2]
From 4 to the end: [  3   4   8  18  21  25 101 201]
From 2, before 7: [1 2 3 4 8]
Every other item: [  0   1   3   8  21 101]
Reversed: [201 101  25  21  18   8   4   3   2   1   1   0]
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[0].numpy()</span></code> retrieves the first element of the tensor, which is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[1].numpy()</span></code> retrieves the second element, which is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[-1].numpy()</span></code> retrieves the last element, which is <code class="docutils literal notranslate"><span class="pre">201</span></code>, using negative indexing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[:]</span></code> accesses the entire tensor, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">18,</span> <span class="pre">21,</span> <span class="pre">25,</span> <span class="pre">101,</span> <span class="pre">201]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[:4]</span></code> gets elements up to but not including index 4, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">2]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[4:]</span></code> retrieves elements from index 4 to the end, giving <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4,</span> <span class="pre">8,</span> <span class="pre">18,</span> <span class="pre">21,</span> <span class="pre">25,</span> <span class="pre">101,</span> <span class="pre">201]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[2:7]</span></code> accesses elements starting from index 2 up to but not including index 7, yielding <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">8]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[::2]</span></code> retrieves every other element with a step size of 2, resulting in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">21,</span> <span class="pre">101]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor[::-1]</span></code> accesses the elements in reverse order, giving <code class="docutils literal notranslate"><span class="pre">[201,</span> <span class="pre">101,</span> <span class="pre">25,</span> <span class="pre">21,</span> <span class="pre">18,</span> <span class="pre">8,</span> <span class="pre">4,</span> <span class="pre">3,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0]</span></code>.</p></li>
</ul>
</div>
<div class="section" id="multi-axis-indexing">
<h3><span class="section-number">12.3.3.2. </span>Multi-Axis Indexing<a class="headerlink" href="#multi-axis-indexing" title="Permalink to this headline">#</a></h3>
<p>For tensors with higher ranks (more dimensions), indexing involves passing multiple indices <span id="id24">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>. Crucially, the same rules as in the single-axis case apply to each axis independently. This means that you can apply the familiar indexing rules to each axis of the tensor, allowing for precise element selection within multi-dimensional structures <span id="id25">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a Rank-2 Tensor</span>
<span class="n">rank_2_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="c1"># Displaying the original tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="c1"># Indexing to retrieve a scalar value</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs 5</span>

<span class="c1"># Indexing using a combination of integers and slices</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second row:&quot;</span><span class="p">,</span> <span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs [4 5 6]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second column:&quot;</span><span class="p">,</span> <span class="n">rank_2_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs [2 5]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Last row:&quot;</span><span class="p">,</span> <span class="n">rank_2_tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs [4 5 6]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First item in last column:&quot;</span><span class="p">,</span> <span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs 3</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Skip the first row:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_2_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Outputs [[4 5 6]]</span>

<span class="c1"># Indexing a Rank-3 tensor</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">]],</span>
<span class="p">])</span>

<span class="c1"># Indexing along all axes to retrieve a 2D tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 2 3]
 [4 5 6]]
5
Second row: [4 5 6]
Second column: [2 5]
Last row: [4 5 6]
First item in last column: 3
Skip the first row:
[[4 5 6]]
tf.Tensor(
[[ 4  9]
 [14 19]
 [24 29]], shape=(3, 2), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this code example, we work with tensors of various ranks to showcase indexing operations:</p>
<ul class="simple">
<li><p>For a Rank-2 tensor (<code class="docutils literal notranslate"><span class="pre">rank_2_tensor</span></code>), we demonstrate scalar indexing, row and column extraction, and slicing.</p></li>
<li><p>For a Rank-3 tensor (<code class="docutils literal notranslate"><span class="pre">rank_3_tensor</span></code>), we retrieve a 2D tensor by indexing along all axes using <code class="docutils literal notranslate"><span class="pre">rank_3_tensor[:,</span> <span class="pre">:,</span> <span class="pre">4]</span></code>.</p></li>
</ul>
<p>The output demonstrates how indexing can be employed to access specific elements or sub-tensors within tensors of different dimensions.</p>
<div class="figure align-center" id="id72">
<a class="reference internal image-reference" href="../_images/Multiaxis_indexing.png"><img alt="../_images/Multiaxis_indexing.png" src="../_images/Multiaxis_indexing.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.10 </span><span class="caption-text">Selecting the final feature across all positions within each example in the batch. Image courtesy of TensorFlow documentation <span id="id26">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id72" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="reshaping-tensor-shapes">
<h3><span class="section-number">12.3.3.3. </span>Reshaping Tensor Shapes<a class="headerlink" href="#reshaping-tensor-shapes" title="Permalink to this headline">#</a></h3>
<p>The ability to reshape a tensor is incredibly valuable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">101</span><span class="p">],</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="p">[</span><span class="mi">103</span><span class="p">],</span> <span class="p">[</span><span class="mi">104</span><span class="p">],</span> <span class="p">[</span><span class="mi">105</span><span class="p">]])</span>

<span class="c1"># Accessing shape information using shape property</span>
<span class="n">shape_object</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape as TensorShape object:&quot;</span><span class="p">,</span> <span class="n">shape_object</span><span class="p">)</span>

<span class="c1"># Converting TensorShape object to a Python list</span>
<span class="n">shape_list</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape as Python list:&quot;</span><span class="p">,</span> <span class="n">shape_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape as TensorShape object: (5, 1)
Shape as Python list: [5, 1]
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we first create a tensor <code class="docutils literal notranslate"><span class="pre">x</span></code> using TensorFlow’s <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code>. We then access the shape information using the <code class="docutils literal notranslate"><span class="pre">shape</span></code> property. The <code class="docutils literal notranslate"><span class="pre">shape_object</span></code> will contain a <code class="docutils literal notranslate"><span class="pre">TensorShape</span></code> object that shows the size along each axis.</p>
<p>Additionally, the <code class="docutils literal notranslate"><span class="pre">shape_list</span></code> is obtained by converting the <code class="docutils literal notranslate"><span class="pre">TensorShape</span></code> object into a Python list using the <code class="docutils literal notranslate"><span class="pre">as_list()</span></code> method. This provides a more convenient format for understanding the shape of the tensor.</p>
<p>“Facilitating the transformation of tensor structures into novel configurations is a task of inherent importance. Notably, the <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> operation stands out for its efficiency and resource efficiency, distinctively characterized by its avoidance of redundant data replication.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Creating a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">101</span><span class="p">],</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="p">[</span><span class="mi">103</span><span class="p">],</span> <span class="p">[</span><span class="mi">104</span><span class="p">],</span> <span class="p">[</span><span class="mi">105</span><span class="p">]])</span>

<span class="c1"># Reshaping the tensor to a new shape</span>
<span class="n">reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Displaying original and reshaped shapes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Shape:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reshaped Shape:&quot;</span><span class="p">,</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Shape: (5, 1)
Reshaped Shape: (1, 5)
</pre></div>
</div>
</div>
</div>
<p>In this code snippet, we first create a tensor <code class="docutils literal notranslate"><span class="pre">x</span></code> using TensorFlow’s <code class="docutils literal notranslate"><span class="pre">tf.constant</span></code>. Next, we reshape the tensor using the <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> function, passing in the new shape as a list <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">5]</span></code>.</p>
<p>The output will display the shapes of both the original tensor and the reshaped tensor. Reshaping allows you to rearrange the elements of the tensor while maintaining the total number of elements, enabling you to adapt tensors for various operations and tasks.</p>
<p>When reshaping a tensor in TensorFlow, the underlying data retains its original arrangement in memory. Instead of physically reorganizing the data, a new tensor with the desired shape is generated, while still referencing the same data in memory. TensorFlow follows a memory layout known as “row-major,” which means that the elements of a row are stored adjacently in memory. Consequently, incrementing the index on the rightmost side corresponds to traversing through memory in contiguous steps, effectively moving to the next element in the same row. This approach optimizes memory access patterns and aligns with the structure commonly used in C-style programming languages <span id="id27">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a Rank-3 tensor</span>
<span class="c1"># This tensor has shape (3, 2, 5), representing three 2x5 matrices</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">]],</span>
<span class="p">])</span>

<span class="c1"># Print the Rank-3 tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this code, we’re creating a Rank-3 tensor using TensorFlow’s <code class="docutils literal notranslate"><span class="pre">constant</span></code> function. The tensor has a shape of (3, 2, 5), meaning it consists of three 2x5 matrices. The comments explain the structure of the tensor and its shape.</p>
<p>Flattening a tensor provides insight into the sequence in which its elements are organized within memory <span id="id28">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the tensor using &#39;-1&#39;</span>
<span class="c1"># The &#39;-1&#39; indicates TensorFlow should determine the size along this axis automatically</span>
<span class="n">flattened_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Print the flattened tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">flattened_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29], shape=(30,), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this code, we’re using the -1 value in the shape argument of the tf.reshape function. This value essentially tells TensorFlow to calculate the size along that axis so that the total number of elements remains constant. The comments explain how the -1 value works and its purpose in the reshaping process.</p>
<p>Typically, the <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> function is most useful when you need to merge or split neighboring axes, or when you want to add or remove dimensions with a size of <code class="docutils literal notranslate"><span class="pre">1</span></code> <span id="id29">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>For the given 3x2x5 tensor, reshaping it into a (3x2)x5 format or into a 3x(2x5) format are both logical operations. This is because the reshaping maintains separate slices that do not overlap, allowing you to rearrange the data without mixing elements <span id="id30">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<ul class="simple">
<li><p>Reshaping to (3x2)x5: This results in three rows of two-by-five matrices, effectively combining the inner dimensions. Each new row holds a set of elements that originally resided in different inner matrices.</p></li>
<li><p>Reshaping to 3x(2x5): This keeps the outermost dimension (3) intact while splitting the second dimension (2x5) into separate inner matrices. Each new column of the reshaped tensor contains elements from one of the original 2x5 matrices. Both reshaping approaches maintain the data integrity and relationships between elements <span id="id31">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]
 [25 26 27 28 29]], shape=(6, 5), dtype=int32) 

tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9]
 [10 11 12 13 14 15 16 17 18 19]
 [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p><span id="id32">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span></p>
<div class="figure align-center" id="id73">
<a class="reference internal image-reference" href="../_images/tf_reshape.png"><img alt="../_images/tf_reshape.png" src="../_images/tf_reshape.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.11 </span><span class="caption-text">Visualizing reshaping <code class="docutils literal notranslate"><span class="pre">rank_3_tensor</span></code> tensor. Image courtesy of TensorFlow documentation <span id="id33">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id73" title="Permalink to this image">#</a></p>
</div>
<p>Reshaping a tensor can be successfully performed for any new shape that maintains the same total count of elements. However, it’s important to note that reshaping won’t yield meaningful results if the arrangement of axes isn’t honored <span id="id34">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>If you intend to rearrange the order of axes, using <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> won’t suffice. Instead, for swapping or permuting axes, the appropriate function to use is <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code>. Reshaping merely alters the dimensions while preserving the sequence of elements, ensuring that you maintain the correct data relationships. Conversely, when you need to modify the order of the dimensions, such as swapping axes, <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code> is the suitable choice to achieve that specific transformation <span id="id35">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a Rank-3 tensor</span>
<span class="n">rank_3_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]],</span>
    <span class="p">[[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">]],</span>
<span class="p">])</span>

<span class="c1"># Examples of incorrect reshaping</span>

<span class="c1"># You can&#39;t reorder axes with reshape.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cannot reorder axes with reshape:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Incorrect dimensions specified.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Incorrect dimensions specified:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Incompatible reshape dimensions.</span>
<span class="k">try</span><span class="p">:</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rank_3_tensor</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Incompatible reshape dimensions:&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cannot reorder axes with reshape:
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]
  [10 11 12 13 14]]

 [[15 16 17 18 19]
  [20 21 22 23 24]
  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) 

Incorrect dimensions specified:
tf.Tensor(
[[ 0  1  2  3  4  5]
 [ 6  7  8  9 10 11]
 [12 13 14 15 16 17]
 [18 19 20 21 22 23]
 [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) 

Incompatible reshape dimensions:
InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]
</pre></div>
</div>
</div>
</div>
<p><span id="id36">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span></p>
<div class="figure align-center" id="id74">
<a class="reference internal image-reference" href="../_images/tf_reshape_bad.png"><img alt="../_images/tf_reshape_bad.png" src="../_images/tf_reshape_bad.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.12 </span><span class="caption-text">Visualizing reshaping <code class="docutils literal notranslate"><span class="pre">rank_3_tensor</span></code> tensor (incorrect reshaping). Image courtesy of TensorFlow documentation <span id="id37">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id74" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
<div class="section" id="data-types-in-tensorflow">
<h2><span class="section-number">12.3.4. </span>Data Types in TensorFlow<a class="headerlink" href="#data-types-in-tensorflow" title="Permalink to this headline">#</a></h2>
<p>To examine the data type of a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>, you can utilize the <code class="docutils literal notranslate"><span class="pre">Tensor.dtype</span></code> property. When generating a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> from a Python object, you have the option to specify the data type if desired <span id="id38">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>In cases where you don’t specify a data type, TensorFlow automatically selects an appropriate one to accommodate your data. For instance, TensorFlow converts Python integers to <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code> and Python floating-point numbers to <code class="docutils literal notranslate"><span class="pre">tf.float32</span></code>. In other scenarios, TensorFlow applies the same conventions as NumPy does when converting to arrays <span id="id39">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Moreover, TensorFlow permits you to perform type casting, enabling you to switch between different data types as needed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a tensor with float64 data type</span>
<span class="n">the_f64_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># Cast the float64 tensor to float16</span>
<span class="n">the_f16_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">the_f64_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="c1"># Cast the float16 tensor to uint8 (unsigned 8-bit integer)</span>
<span class="c1"># This will lead to loss of decimal precision</span>
<span class="n">the_u8_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">the_f16_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="c1"># Print the resulting uint8 tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">the_u8_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
</pre></div>
</div>
</div>
</div>
<div class="section" id="broadcasting">
<h3><span class="section-number">12.3.4.1. </span>Broadcasting<a class="headerlink" href="#broadcasting" title="Permalink to this headline">#</a></h3>
<p>Broadcasting is a concept adopted from the <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">equivalent feature in NumPy</a>. In essence, broadcasting enables smaller tensors to be automatically expanded to match the dimensions of larger tensors when performing combined operations on them <span id="id40">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>The most straightforward and frequent scenario occurs when you try to multiply or add a tensor by a scalar value. In this instance, the scalar value is broadcasted, causing it to take on the same shape as the other tensor in the operation. This seamless extension of dimensions facilitates efficient computation of operations involving tensors with varying shapes <span id="id41">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a constant tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Define scalar and tensor constants</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Perform the same computation using different expressions</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Broadcasting scalar to tensor</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>  <span class="c1"># Broadcasting scalar to tensor using operator</span>
<span class="n">result3</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>  <span class="c1"># Broadcasting tensor to tensor</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result 1:&quot;</span><span class="p">,</span> <span class="n">result1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result 2:&quot;</span><span class="p">,</span> <span class="n">result2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result 3:&quot;</span><span class="p">,</span> <span class="n">result3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Result 1: tf.Tensor([2 4 6], shape=(3,), dtype=int32)
Result 2: tf.Tensor([2 4 6], shape=(3,), dtype=int32)
Result 3: tf.Tensor([2 4 6], shape=(3,), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Similarly, axes with a length of 1 can be extended to align with the dimensions of other arguments during operations. This extension can be applied to both operands simultaneously within a single computation <span id="id42">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>In the provided example, a 3x1 matrix is multiplied element-wise with a 1x4 matrix, resulting in a 3x4 matrix. It’s worth noting that the leading 1 in the matrix dimensions is not strictly required. For instance, the shape of matrix <code class="docutils literal notranslate"><span class="pre">y</span></code> can be expressed as <code class="docutils literal notranslate"><span class="pre">[4]</span></code>, highlighting that broadcasting effectively accounts for dimensions of length 1, regardless of whether they are explicitly stated <span id="id43">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Reshape x into a 3x1 matrix</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create a tensor with values ranging from 1 to 4</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Perform element-wise multiplication using broadcasting</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the reshaped x, y, and the result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reshaped x:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result of element-wise multiplication:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reshaped x:
tf.Tensor(
[[1]
 [2]
 [3]], shape=(3, 1), dtype=int32) 

y:
tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) 

Result of element-wise multiplication:
tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]], shape=(3, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p><span id="id44">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span></p>
<div class="figure align-center" id="id75">
<a class="reference internal image-reference" href="../_images/3d_tensor.png"><img alt="../_images/3d_tensor.png" src="../_images/3d_tensor.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.13 </span><span class="caption-text">A broadcasted addition occurs when a tensor with shape [3, 1] is combined with another tensor of shape [1, 4], resulting in a new tensor with shape [3, 4]. This concept is illustrated below and is sourced from the TensorFlow documentation <span id="id45">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id75" title="Permalink to this image">#</a></p>
</div>
<p>Here is the same operation conducted without utilizing broadcasting:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define tensors for multiplication without broadcasting</span>
<span class="n">x_stretch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>

<span class="n">y_stretch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># Perform element-wise multiplication without broadcasting</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">x_stretch</span> <span class="o">*</span> <span class="n">y_stretch</span>

<span class="c1"># Print the result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Element-wise multiplication without broadcasting:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Operator overloading performs element-wise multiplication</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Element-wise multiplication without broadcasting:
tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]], shape=(3, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In the majority of cases, broadcasting proves to be efficient in terms of both time and space. This efficiency arises from the fact that the broadcast operation doesn’t physically create expanded tensors in memory <span id="id46">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>To observe the visual representation of broadcasting, you can use the <code class="docutils literal notranslate"><span class="pre">tf.broadcast_to</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Use tf.broadcast_to to demonstrate broadcasting</span>
<span class="n">broadcasted_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Print the broadcasted tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Broadcasted tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">broadcasted_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Broadcasted tensor:
tf.Tensor(
[[1 2 3]
 [1 2 3]
 [1 2 3]], shape=(3, 3), dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark</p>
<p>Unlike certain mathematical operations, the <code class="docutils literal notranslate"><span class="pre">broadcast_to</span></code> function doesn’t implement memory-saving mechanisms. In this case, you are essentially creating the expanded tensor in memory <span id="id47">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
</div>
<div class="section" id="tf-convert-to-tensor">
<h3><span class="section-number">12.3.4.2. </span>tf.convert_to_tensor<a class="headerlink" href="#tf-convert-to-tensor" title="Permalink to this headline">#</a></h3>
<p>The majority of operations in TensorFlow, such as <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code>, expect arguments to be instances of the class <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>. However, as seen in the example above, objects structured like tensors are also accepted <span id="id48">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>It’s important to note that many operations, although not all, invoke <code class="docutils literal notranslate"><span class="pre">convert_to_tensor</span></code> on arguments that are not already tensors. This conversion process relies on a registry containing various conversion functions. Common object classes like NumPy’s <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="docutils literal notranslate"><span class="pre">TensorShape</span></code>, Python lists, and even <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> instances are automatically converted to tensors <span id="id49">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>For more intricate details, you can refer to <code class="docutils literal notranslate"><span class="pre">tf.register_tensor_conversion_function</span></code>. If you’re dealing with a specific type that you wish to be automatically converted into a tensor, this function is worth exploring <span id="id50">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
<div class="section" id="ragged-tensors">
<h3><span class="section-number">12.3.4.3. </span>Ragged Tensors<a class="headerlink" href="#ragged-tensors" title="Permalink to this headline">#</a></h3>
<p>A tensor that possesses varying numbers of elements along a specific axis is referred to as “ragged.” To handle such data, the <code class="docutils literal notranslate"><span class="pre">tf.ragged.RaggedTensor</span></code> class comes into play <span id="id51">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>For instance, the following data structure cannot be accurately represented using a standard tensor <span id="id52">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>:</p>
<div class="figure align-center" id="id76">
<a class="reference internal image-reference" href="../_images/ragged.png"><img alt="../_images/ragged.png" src="../_images/ragged.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.14 </span><span class="caption-text">A <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> with a shape of [4, None] aptly addresses this scenario.  Image courtesy of TensorFlow documentation <span id="id53">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id76" title="Permalink to this image">#</a></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a ragged list</span>
<span class="n">ragged_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">]]</span>

<span class="c1"># Attempt to create a tensor from the ragged list</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">ragged_list</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error: ValueError: Can&#39;t convert non-rectangular Python sequence to Tensor.
</pre></div>
</div>
</div>
</div>
<p>Instead create a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> using <code class="docutils literal notranslate"><span class="pre">tf.ragged.constant</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a ragged list</span>
<span class="n">ragged_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">]]</span>

<span class="c1"># Create a tf.RaggedTensor using tf.ragged.constant</span>
<span class="n">ragged_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">ragged_list</span><span class="p">)</span>

<span class="c1"># Print the ragged tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ragged Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ragged_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ragged Tensor:
&lt;tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]&gt;
</pre></div>
</div>
</div>
</div>
<p>The shape of a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> will contain some axes with unknown lengths:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ragged_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, None)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="string-tensors">
<h3><span class="section-number">12.3.4.4. </span>String Tensors<a class="headerlink" href="#string-tensors" title="Permalink to this headline">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> data type allows you to represent data as strings, which are essentially variable-length byte arrays, within tensors.</p>
<p>It’s important to note that strings in TensorFlow are treated as atomic units and can’t be indexed in the same way as Python strings. Unlike traditional tensors, the length of a string is not considered as one of the tensor’s axes. If you need to manipulate string data within tensors, you can explore the functionalities provided by <code class="docutils literal notranslate"><span class="pre">tf.strings</span></code> for various string operations <span id="id54">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Here is a scalar string tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a scalar string tensor</span>
<span class="n">scalar_string</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s2">&quot;Hello, Calgary!&quot;</span><span class="p">)</span>

<span class="c1"># Print the scalar string tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Scalar String Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scalar_string</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Scalar String Tensor:
tf.Tensor(b&#39;Hello, Calgary!&#39;, shape=(), dtype=string)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a tensor of strings with different lengths</span>
<span class="n">tensor_of_strings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s2">&quot;Crimson apple&quot;</span><span class="p">,</span>
                                 <span class="s2">&quot;Sky-blue ocean&quot;</span><span class="p">,</span>
                                 <span class="s2">&quot;Emerald forest&quot;</span><span class="p">])</span>

<span class="c1"># Note that the shape is (3,), as the string lengths are not included</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tensor of Strings:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor_of_strings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor of Strings:
tf.Tensor([b&#39;Crimson apple&#39; b&#39;Sky-blue ocean&#39; b&#39;Emerald forest&#39;], shape=(3,), dtype=string)
</pre></div>
</div>
</div>
</div>
<p>In the above printout, the presence of the <code class="docutils literal notranslate"><span class="pre">b</span></code> prefix signifies that the <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> data type represents byte-strings, rather than Unicode strings. If you’re interested in understanding how to work with Unicode text within TensorFlow, you can refer to the <a class="reference external" href="https://www.tensorflow.org/tutorials/load_data/unicode">Unicode Tutorial</a>, which provides insights into this topic <span id="id55">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Fundamental string operations can be accessed via <code class="docutils literal notranslate"><span class="pre">tf.strings</span></code>, which includes functions such as <code class="docutils literal notranslate"><span class="pre">tf.strings.split</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a scalar string tensor</span>
<span class="n">scalar_string_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s2">&quot;Hello Calgary!&quot;</span><span class="p">)</span>

<span class="c1"># Use split to break a string into a set of tensors</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Splitting a scalar string tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">scalar_string_tensor</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">))</span>

<span class="c1"># However, when applied to a tensor of strings, split results in a `RaggedTensor`</span>
<span class="c1"># since each string may be divided into a different number of parts.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Splitting a tensor of strings:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor_of_strings</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Splitting a scalar string tensor:
tf.Tensor([b&#39;Hello&#39; b&#39;Calgary!&#39;], shape=(2,), dtype=string)

Splitting a tensor of strings:
&lt;tf.RaggedTensor [[b&#39;Crimson&#39;, b&#39;apple&#39;],
 [b&#39;Sky-blue&#39;, b&#39;ocean&#39;],
 [b&#39;Emerald&#39;, b&#39;forest&#39;]]&gt;
</pre></div>
</div>
</div>
</div>
<p>And <code class="docutils literal notranslate"><span class="pre">tf.strings.to_number</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a string containing numbers</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s2">&quot;1 101 201 301&quot;</span><span class="p">)</span>

<span class="c1"># Use split to separate the string into parts and then convert them to numbers</span>
<span class="n">converted_numbers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">to_number</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">))</span>

<span class="c1"># Print the converted numbers</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Converted Numbers:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">converted_numbers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Converted Numbers:
tf.Tensor([  1. 101. 201. 301.], shape=(4,), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this code, we’re demonstrating how to split a string containing numbers into parts and then convert those parts into numeric values using <code class="docutils literal notranslate"><span class="pre">tf.strings.to_number</span></code>.</p>
<p>While you cannot directly use <code class="docutils literal notranslate"><span class="pre">tf.cast</span></code> to convert a string tensor into numeric values, you can achieve this conversion by first transforming the string tensor into bytes and then proceeding to convert those bytes into numbers <span id="id56">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Define a string tensor</span>
<span class="n">string_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">,</span> <span class="s2">&quot;4&quot;</span><span class="p">])</span>

<span class="c1"># Convert string tensor to bytes</span>
<span class="n">bytes_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">bytes_split</span><span class="p">(</span><span class="n">string_tensor</span><span class="p">)</span>

<span class="c1"># Convert bytes tensor to numbers</span>
<span class="n">numbers_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">to_number</span><span class="p">(</span><span class="n">bytes_tensor</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Print the original string tensor, bytes tensor, and the converted numbers tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original String Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">string_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bytes Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bytes_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converted Numbers Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numbers_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original String Tensor:
tf.Tensor([b&#39;1&#39; b&#39;2&#39; b&#39;3&#39; b&#39;4&#39;], shape=(4,), dtype=string)

Bytes Tensor:
&lt;tf.RaggedTensor [[b&#39;1&#39;],
 [b&#39;2&#39;],
 [b&#39;3&#39;],
 [b&#39;4&#39;]]&gt;

Converted Numbers Tensor:
&lt;tf.RaggedTensor [[1],
 [2],
 [3],
 [4]]&gt;
</pre></div>
</div>
</div>
</div>
<p>In this example, we start with a string tensor containing numeric strings. We then convert this string tensor to a bytes tensor using <code class="docutils literal notranslate"><span class="pre">tf.strings.bytes_split</span></code>. Finally, we convert the bytes tensor to numbers using <code class="docutils literal notranslate"><span class="pre">tf.strings.to_number</span></code> with an <code class="docutils literal notranslate"><span class="pre">out_type</span></code> specified as <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code>. The original string tensor, the bytes tensor, and the converted numbers tensor are printed for comparison.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> data type serves as the representation for raw bytes data in TensorFlow. For all operations involving raw bytes, this data type is used. The <code class="docutils literal notranslate"><span class="pre">tf.io</span></code> module encompasses functions tailored for converting data to and from bytes. This module is especially useful for tasks such as decoding images and parsing CSV files, among other data-related operations <span id="id57">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
</div>
<div class="section" id="sparse-tensors">
<h3><span class="section-number">12.3.4.5. </span>Sparse Tensors<a class="headerlink" href="#sparse-tensors" title="Permalink to this headline">#</a></h3>
<p>In certain scenarios, your data might exhibit sparsity, such as in a wide embedding space. TensorFlow provides support for handling sparse data through the use of <code class="docutils literal notranslate"><span class="pre">tf.sparse.SparseTensor</span></code> and associated operations. These tools allow you to efficiently store and manipulate sparse data within your TensorFlow computations <span id="id58">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="figure align-center" id="id77">
<a class="reference internal image-reference" href="../_images/sparse.png"><img alt="../_images/sparse.png" src="../_images/sparse.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12.15 </span><span class="caption-text">A <code class="docutils literal notranslate"><span class="pre">tf.SparseTensor</span></code>, shape: [3, 4]. Image courtesy of TensorFlow documentation <span id="id59">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</span><a class="headerlink" href="#id77" title="Permalink to this image">#</a></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a sparse tensor storing values by index</span>
<span class="n">sparse_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                                       <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                                       <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Print the sparse tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sparse_tensor</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Convert sparse tensor to dense</span>
<span class="n">dense_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">sparse_tensor</span><span class="p">)</span>

<span class="c1"># Print the dense tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dense Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sparse Tensor:
SparseTensor(indices=tf.Tensor(
[[0 0]
 [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) 

Dense Tensor:
tf.Tensor(
[[1 0 0 0]
 [0 0 2 0]
 [0 0 0 0]], shape=(3, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this code, we’re showcasing the creation of a sparse tensor using <code class="docutils literal notranslate"><span class="pre">tf.sparse.SparseTensor</span></code> and the subsequent conversion of that sparse tensor to a dense tensor using <code class="docutils literal notranslate"><span class="pre">tf.sparse.to_dense</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a higher-dimensional sparse tensor</span>
<span class="n">sparse_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                                       <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                                       <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Print the sparse tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sparse_tensor</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Convert sparse tensor to dense</span>
<span class="n">dense_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">sparse_tensor</span><span class="p">)</span>

<span class="c1"># Print the dense tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dense Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dense_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sparse Tensor:
SparseTensor(indices=tf.Tensor(
[[0 0 0]
 [1 2 3]
 [2 1 2]], shape=(3, 3), dtype=int64), values=tf.Tensor([1 2 3], shape=(3,), dtype=int32), dense_shape=tf.Tensor([3 4 5], shape=(3,), dtype=int64)) 

Dense Tensor:
tf.Tensor(
[[[1 0 0 0 0]
  [0 0 0 0 0]
  [0 0 0 0 0]
  [0 0 0 0 0]]

 [[0 0 0 0 0]
  [0 0 0 0 0]
  [0 0 0 2 0]
  [0 0 0 0 0]]

 [[0 0 0 0 0]
  [0 0 3 0 0]
  [0 0 0 0 0]
  [0 0 0 0 0]]], shape=(3, 4, 5), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this example, we’re creating a sparse tensor with a shape of [3, 4, 5].</p>
</div>
</div>
<div class="section" id="introduction-to-variables">
<h2><span class="section-number">12.3.5. </span>Introduction to Variables<a class="headerlink" href="#introduction-to-variables" title="Permalink to this headline">#</a></h2>
<p>In TensorFlow, the <strong>variable</strong> mechanism is the recommended approach for representing shared and persistent state that your program interacts with. This guide provides an in-depth exploration of creating, modifying, and managing instances of <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> within TensorFlow <span id="id60">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>At its core, the <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> class plays a crucial role in both the establishment and supervision of variables. In essence, a <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> encapsulates a tensor whose value can be modified by executing operations. Specific operations cater to both reading and altering the values within this tensor. Notably, in higher-level frameworks like <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> is extensively utilized to house model parameters, underscoring its vital significance in various machine learning tasks <span id="id61">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="section" id="creating-a-variable">
<h3><span class="section-number">12.3.5.1. </span>Creating a Variable<a class="headerlink" href="#creating-a-variable" title="Permalink to this headline">#</a></h3>
<p>To create a variable, you need to provide an initial value. The <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> you create will inherit the same data type (<code class="docutils literal notranslate"><span class="pre">dtype</span></code>) as the provided initialization value <span id="id62">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a function to print bold text</span>
<span class="k">def</span> <span class="nf">print_bold</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[1m&quot;</span> <span class="o">+</span> <span class="n">text</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="c1"># Create variables with different data types as initializations</span>
<span class="n">int_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;int_variable&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">float_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">3.14</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;float_variable&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">bool_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bool_variable&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">string_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;Hello, TensorFlow!&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;string_variable&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
<span class="n">complex_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="n">j</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;complex_variable&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">)</span>

<span class="c1"># Perform operations with variables</span>
<span class="n">float_result</span> <span class="o">=</span> <span class="n">float_variable</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">bool_result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">bool_variable</span><span class="p">)</span>
<span class="n">string_concat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">string_variable</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s2">&quot; Welcome!&quot;</span><span class="p">)])</span>

<span class="c1"># Print information about the variables and results</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Integer Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">int_variable</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Float Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">float_variable</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Boolean Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bool_variable</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;String Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">string_variable</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Complex Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">complex_variable</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Result of Float Variable Multiplication:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">float_result</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Logical NOT of Boolean Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bool_result</span><span class="p">)</span>
<span class="n">print_bold</span><span class="p">(</span><span class="s2">&quot;Concatenated String Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">string_concat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Integer Variable:</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Variable &#39;int_variable:0&#39; shape=() dtype=int32, numpy=42&gt;
<span class=" -Color -Color-Bold">Float Variable:</span>
&lt;tf.Variable &#39;float_variable:0&#39; shape=() dtype=float32, numpy=3.14&gt;
<span class=" -Color -Color-Bold">Boolean Variable:</span>
&lt;tf.Variable &#39;bool_variable:0&#39; shape=() dtype=bool, numpy=True&gt;
<span class=" -Color -Color-Bold">String Variable:</span>
&lt;tf.Variable &#39;string_variable:0&#39; shape=() dtype=string, numpy=b&#39;Hello, TensorFlow!&#39;&gt;
<span class=" -Color -Color-Bold">Complex Variable:</span>
&lt;tf.Variable &#39;complex_variable:0&#39; shape=() dtype=complex64, numpy=(2+3j)&gt;
<span class=" -Color -Color-Bold">Result of Float Variable Multiplication:</span>
tf.Tensor(6.28, shape=(), dtype=float32)
<span class=" -Color -Color-Bold">Logical NOT of Boolean Variable:</span>
tf.Tensor(False, shape=(), dtype=bool)
<span class=" -Color -Color-Bold">Concatenated String Variable:</span>
tf.Tensor(b&#39;Hello, TensorFlow! Welcome!&#39;, shape=(), dtype=string)
</pre></div>
</div>
</div>
</div>
<p>Most tensor operations behave as expected when used with variables. However, it’s important to note that variables cannot be reshaped using standard tensor reshaping operations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a variable</span>
<span class="n">my_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;my_variable&quot;</span><span class="p">)</span>

<span class="c1"># Print the variable, its conversion to a tensor, and the index of the highest value</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A variable:&quot;</span><span class="p">,</span> <span class="n">my_variable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Viewed as a tensor:&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">my_variable</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Index of highest value:&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">my_variable</span><span class="p">))</span>

<span class="c1"># Attempt to reshape the variable</span>
<span class="c1"># This creates a new tensor; it does not reshape the variable.</span>
<span class="n">reshaped_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">my_variable</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Print the reshaped tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Copying and reshaping: &quot;</span><span class="p">,</span> <span class="n">reshaped_variable</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A variable: &lt;tf.Variable &#39;my_variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4])&gt;

Viewed as a tensor: tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)

Index of highest value: tf.Tensor(3, shape=(), dtype=int64)

Copying and reshaping:  tf.Tensor([[1 2 3 4]], shape=(1, 4), dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p>We create a variable <code class="docutils literal notranslate"><span class="pre">my_variable</span></code> with shape <code class="docutils literal notranslate"><span class="pre">(4,)</span></code>.</p></li>
<li><p>We print the variable itself, its conversion to a tensor using <code class="docutils literal notranslate"><span class="pre">tf.convert_to_tensor</span></code>, and the index of the highest value using <code class="docutils literal notranslate"><span class="pre">tf.math.argmax</span></code>.</p></li>
<li><p>We attempt to reshape the variable using <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code>, which actually creates a new tensor with the desired shape.</p></li>
<li><p>We print the reshaped tensor.</p></li>
</ul>
<p>This illustrates that while operations like reshaping are possible with tensors, they do not directly reshape variables; instead, they create new tensors with the specified shape.</p>
</div>
<div class="section" id="lifecycles-naming-and-watching">
<h3><span class="section-number">12.3.5.2. </span>Lifecycles, Naming, and Watching<a class="headerlink" href="#lifecycles-naming-and-watching" title="Permalink to this headline">#</a></h3>
<p>In TensorFlow for Python, instances of <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> share the same lifecycle characteristics as other standard Python objects. When no references to a variable remain, it is automatically deallocated by Python’s garbage collector. Furthermore, variables can be assigned names, facilitating the tracking and debugging process. It is possible to assign the same name to two separate variables <span id="id63">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a constant tensor</span>
<span class="n">my_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># Create variables a and b with the same name</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Mark&quot;</span><span class="p">)</span>
<span class="c1"># A new variable with the same name, but different value</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Mark&quot;</span><span class="p">)</span>

<span class="c1"># These are elementwise-unequal, despite having the same name</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor([False False False], shape=(3,), dtype=bool)
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p>We create a constant tensor <code class="docutils literal notranslate"><span class="pre">my_tensor</span></code>.</p></li>
<li><p>We create two variables, <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, both with the name “Mark”.</p></li>
<li><p>Variable <code class="docutils literal notranslate"><span class="pre">a</span></code> is initialized with <code class="docutils literal notranslate"><span class="pre">my_tensor</span></code>, while variable <code class="docutils literal notranslate"><span class="pre">b</span></code> is initialized with <code class="docutils literal notranslate"><span class="pre">my_tensor</span> <span class="pre">+</span> <span class="pre">1</span></code>.</p></li>
<li><p>Despite having the same name, variables <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> have different values, as shown by the comparison <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">==</span> <span class="pre">b</span></code>.</p></li>
</ul>
</div>
<div class="section" id="variable-names-saving-and-gradients">
<h3><span class="section-number">12.3.5.3. </span>Variable Names, Saving, and Gradients<a class="headerlink" href="#variable-names-saving-and-gradients" title="Permalink to this headline">#</a></h3>
<p>When saving and loading models in TensorFlow, variable names are preserved. By default, when variables are used within models, they are assigned unique names automatically. This eliminates the need for manual naming, unless you have specific requirements. While variables play a crucial role in differentiation, not all variables need to be included in the gradient computations. You can prevent gradients from being calculated for a variable by setting its <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute to <code class="docutils literal notranslate"><span class="pre">False</span></code> during creation. An example of a variable that typically doesn’t need gradients is a training step counter <span id="id64">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>Here’s a concise example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a non-trainable variable (training step counter)</span>
<span class="n">train_step_counter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Set trainable=False for a variable</span>
<span class="n">non_trainable_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Print information about the variables</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train step counter:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_step_counter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Non-trainable variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">non_trainable_variable</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train step counter:
&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=0&gt;
Non-trainable variable:
&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=42&gt;
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p>We create a non-trainable variable <code class="docutils literal notranslate"><span class="pre">train_step_counter</span></code> which will track the training steps.</p></li>
<li><p>We create another variable <code class="docutils literal notranslate"><span class="pre">non_trainable_variable</span></code> with <code class="docutils literal notranslate"><span class="pre">trainable=False</span></code>.</p></li>
<li><p>We print information about these variables. The <code class="docutils literal notranslate"><span class="pre">train_step_counter</span></code> variable, even though it’s a variable, is not intended for differentiation and is not trainable. The <code class="docutils literal notranslate"><span class="pre">non_trainable_variable</span></code> is explicitly set to be non-trainable.</p></li>
</ul>
</div>
<div class="section" id="placing-variables-and-tensors">
<h3><span class="section-number">12.3.5.4. </span>Placing Variables and Tensors<a class="headerlink" href="#placing-variables-and-tensors" title="Permalink to this headline">#</a></h3>
<p>For optimal performance, TensorFlow strives to place tensors and variables on the fastest compatible device according to their <code class="docutils literal notranslate"><span class="pre">dtype</span></code>. Consequently, most variables are placed on a GPU if one is available. However, you have the ability to override this placement behavior. In the following code snippet, a float tensor and a variable are explicitly placed on the CPU, even if a GPU is accessible. If you enable device placement logging, you can observe where the variable is actually placed <span id="id65">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<div class="warning admonition">
<p class="admonition-title">Note</p>
<p>Although manual placement is possible, employing distribution strategies can be a more streamlined and scalable approach to enhancing computation efficiency.</p>
</div>
<p>If you execute this code on different backends, with and without a GPU, you will observe varying log outputs. Keep in mind that enabling device placement logging must be done at the session’s outset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a float tensor placed on the CPU</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/CPU:0&#39;</span><span class="p">):</span>
    <span class="n">cpu_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="c1"># Create a variable placed on the CPU</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/CPU:0&#39;</span><span class="p">):</span>
    <span class="n">cpu_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

<span class="c1"># Print information about the placement</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CPU Tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cpu_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CPU Variable:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cpu_variable</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU Tensor:
tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)
CPU Variable:
&lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p>We explicitly place a float tensor <code class="docutils literal notranslate"><span class="pre">cpu_tensor</span></code> and a variable <code class="docutils literal notranslate"><span class="pre">cpu_variable</span></code> on the CPU using the <code class="docutils literal notranslate"><span class="pre">tf.device</span></code> context manager.</p></li>
<li><p>We print information about these CPU-placed objects.</p></li>
</ul>
<p>By reviewing the log outputs with device placement logging enabled, you can determine the actual placement of these variables and tensors.</p>
</div>
<div class="section" id="cross-device-placement">
<h3><span class="section-number">12.3.5.5. </span>Cross-Device Placement<a class="headerlink" href="#cross-device-placement" title="Permalink to this headline">#</a></h3>
<p>It is feasible to designate the location of a variable or tensor on one device while performing computations on another device. However, this approach introduces latency since data must be copied between devices. This strategy may be employed when dealing with scenarios involving multiple GPU workers, yet you desire only a single instance of the variables across these workers <span id="id66">[<a class="reference internal" href="../References.html#id120" title="TensorFlow Developers. Tensorflow documentation. https://www.tensorflow.org/guide, 2023. [Online; accessed 01-August-2023].">TensorFlow Developers, 2023</a>]</span>.</p>
<p>To illustrate, consider the following code snippet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Create a variable placed on the CPU</span>
<span class="n">cpu_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cpu_variable&quot;</span><span class="p">)</span>

<span class="c1"># Place the computation on a GPU</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/GPU:0&#39;</span><span class="p">):</span>
    <span class="n">gpu_result</span> <span class="o">=</span> <span class="n">cpu_variable</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="c1"># Print the GPU result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU Result:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpu_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU Result:
tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p>We create a variable <code class="docutils literal notranslate"><span class="pre">cpu_variable</span></code> placed on the CPU.</p></li>
<li><p>We then perform a computation, multiplying the <code class="docutils literal notranslate"><span class="pre">cpu_variable</span></code> by 2.0, within a <code class="docutils literal notranslate"><span class="pre">tf.device</span></code> context manager to place the computation on a GPU.</p></li>
<li><p>We print the GPU result.</p></li>
</ul>
<p>While such cross-device placement can be advantageous for specific distributed scenarios, it’s important to note the associated data copying overhead.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ENGG_680_C12S2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.2. </span>Fundamentals of Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="ENGG_680_C12S4.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12.4. </span>Building a linear Regression Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-basics">12.3.1. TensorFlow Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#array-and-tensor-connection">12.3.1.1. Array and Tensor Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-ties-to-numpy-arrays">12.3.1.2. Strong Ties to NumPy Arrays</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-or-rank-0-tensor">12.3.1.3. “Scalar” or “Rank-0” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-or-rank-1-tensor">12.3.1.4. “Vector” or “Rank-1” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-or-rank-2-tensor">12.3.1.5. “Matrix” or “Rank-2” Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-tensors-to-numpy-arrays">12.3.1.6. Converting Tensors to NumPy Arrays</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performing-basic-math-operations-on-tensors">12.3.1.7. Performing Basic Math Operations on Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-in-various-operations-ops">12.3.1.8. Tensors in Various Operations (Ops)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-conversion-flexibility">12.3.1.9. Tensor Conversion Flexibility</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shapes-and-indexing">12.3.2. Tensor Shapes and Indexing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-shapes-with-tf-tensorshape-objects">12.3.2.1. Accessing Tensor Shapes with <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> Objects</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-tensors">12.3.3. Indexing Tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-axis-indexing">12.3.3.1. Single-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-axis-indexing">12.3.3.2. Multi-Axis Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensor-shapes">12.3.3.3. Reshaping Tensor Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types-in-tensorflow">12.3.4. Data Types in TensorFlow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">12.3.4.1. Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-convert-to-tensor">12.3.4.2. tf.convert_to_tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ragged-tensors">12.3.4.3. Ragged Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#string-tensors">12.3.4.4. String Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-tensors">12.3.4.5. Sparse Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-variables">12.3.5. Introduction to Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-variable">12.3.5.1. Creating a Variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lifecycles-naming-and-watching">12.3.5.2. Lifecycles, Naming, and Watching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-names-saving-and-gradients">12.3.5.3. Variable Names, Saving, and Gradients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#placing-variables-and-tensors">12.3.5.4. Placing Variables and Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-device-placement">12.3.5.5. Cross-Device Placement</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>