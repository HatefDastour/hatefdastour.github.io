
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2.4. Advanced Imputation with K-Nearest Neighbors (KNN) &#8212; Introduction to Time Series</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_02/Intro2TS_C02S04';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Seasonality and Stationarity" href="../chapter_03/Intro2TS_C03.html" />
    <link rel="prev" title="2.3. Handling Missing Values" href="Intro2TS_C02S03.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Preface.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Time Series - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Time Series - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_01/Intro2TS_C01.html">1. Introduction to Time Series</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S01.html">1.1. What are Time Series?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S02.html">1.2. Best Sources for Public Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S03.html">1.3. Time Series Cross-Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S04.html">1.4. Time Series Visualizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S05.html">1.5. What Are ACF and PACF?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01/Intro2TS_C01S06.html">1.6. Cross-Correlation in Time Series</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="Intro2TS_C02.html">2. Missing Data in Time Series</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Intro2TS_C02S01.html">2.1. Types of Missing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Intro2TS_C02S02.html">2.2. Identifying Patterns of Missingness</a></li>
<li class="toctree-l2"><a class="reference internal" href="Intro2TS_C02S03.html">2.3. Handling Missing Values</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2.4. Advanced Imputation with K-Nearest Neighbors (KNN)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_03/Intro2TS_C03.html">3. Seasonality and Stationarity</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/Intro2TS_C03S01.html">3.1. Understanding Seasonality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/Intro2TS_C03S02.html">3.2. Seasonal-Trend Decomposition using LOESS (STL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/Intro2TS_C03S03.html">3.3. Additive vs. Multiplicative Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/Intro2TS_C03S04.html">3.4. Robust STL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03/Intro2TS_C03S05.html">3.5. Stationarity and Non-Stationarity in Time-Series Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_04/Intro2TS_C04.html">4. Classical Time Series Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S01.html">4.1. Time Series Smoothing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S02.html">4.2. Foundations of Exponential Smoothing: Level &amp; Trend</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S03.html">4.3. Advanced Exponential Smoothing: Seasonality &amp; Statistical Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S04.html">4.4. ARIMA Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S05.html">4.5. ARIMA Models in Practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S06.html">4.6. SARIMA and SARIMAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04/Intro2TS_C04S07.html">4.7. Automated ARIMA Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_05/Intro2TS_C05.html">5. Outlier Detection in Time Series</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S01.html">5.1. Outlier Detection in Time Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S02.html">5.2. Outlier Classification by Scope</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S03.html">5.3. Outlier Classification by Seasonality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S04.html">5.4. Visual Methods for Outlier Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S05.html">5.5. Descriptive Statistics-Based Methods for Outlier Detection in Time Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05/Intro2TS_C05S06.html">5.6. Time Series Modeling-Based Methods for Outlier Detection</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../References.html">6. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advanced Imputation with K-Nearest Neighbors (KNN)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">2.4.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-climate-data">2.4.2. Example: Climate Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-missing-data-patterns">2.4.3. Introducing Missing Data Patterns</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-knn-imputation-algorithm">2.4.4. Understanding KNN Imputation Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-knn-imputation-with-real-data">2.4.5. Implementing KNN Imputation with Real Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-metrics-and-evaluation">2.4.6. Accuracy Metrics and Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis-knn-vs-other-methods">2.4.7. Comparative Analysis - KNN vs Other Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning-finding-optimal-k">2.4.8. Hyperparameter Tuning - Finding Optimal k</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-bias-variance-tradeoff-in-knn-imputation">2.4.8.1. Understanding the Bias-Variance Tradeoff in KNN Imputation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-temporal-resolution-matters-for-choosing-k">2.4.8.2. Why Temporal Resolution Matters for Choosing k</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-imputation-with-k-nearest-neighbors-knn">
<h1><span class="section-number">2.4. </span>Advanced Imputation with K-Nearest Neighbors (KNN)<a class="headerlink" href="#advanced-imputation-with-k-nearest-neighbors-knn" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">2.4.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Missing data is one of the most common challenges in time series analysis. While simple methods like forward fill (<code class="docutils literal notranslate"><span class="pre">ffill</span></code>), backward fill (<code class="docutils literal notranslate"><span class="pre">bfill</span></code>), and linear interpolation address temporal structure, they ignore valuable information hidden in <strong>cross-sectional relationships</strong> between variables. When our time series has multiple correlated variables—such as temperature and humidity, or humidity and precipitation—we can leverage these relationships to generate more realistic imputations.</p>
<p><strong>K-Nearest Neighbors (KNN) imputation</strong> fills missing values by finding the most similar complete observations and using their values as estimates. Unlike temporal methods that look only at time sequences, KNN examines multivariate relationships, making it particularly powerful for complex datasets where variables move in concert.</p>
<div class="admonition-key-concepts admonition">
<p class="admonition-title">Key Concepts</p>
<ol class="arabic simple">
<li><p><strong>Multivariate Imputation</strong>: Uses correlations between variables (e.g., Temp &amp; Humidity, Humidity &amp; Precipitation) rather than just temporal patterns</p></li>
<li><p><strong>Distance-Based Similarity</strong>: Calculates distances in feature space to identify neighbors, commonly using Euclidean distance</p></li>
<li><p><strong>Scaling Requirement</strong>: Essential for KNN because distance calculations are sensitive to variable magnitudes</p></li>
<li><p><strong>Time-Agnostic Nature</strong>: KNN does not inherently understand temporal ordering—it treats observations as points in space, not as a sequence</p></li>
</ol>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
When to Use KNN Imputation</label><div class="sd-tab-content docutils">
<p>KNN imputation excels in scenarios where:</p>
<ul class="simple">
<li><p>Multiple variables show <strong>strong cross-sectional correlations</strong></p></li>
<li><p>Missing data is <strong>Missing Completely at Random (MCAR)</strong> or <strong>Missing at Random (MAR)</strong></p></li>
<li><p>our dataset has <strong>sufficient complete observations</strong> to find meaningful neighbors</p></li>
<li><p>We need to <strong>preserve multivariate relationships</strong> in the imputed values</p></li>
</ul>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
When to Avoid KNN Imputation</label><div class="sd-tab-content docutils">
<p>KNN imputation may be suboptimal when:</p>
<ul class="simple">
<li><p>Data is <strong>Missing Not at Random (MNAR)</strong> due to systematic reasons</p></li>
<li><p>The dataset has <strong>very few complete observations</strong> (curse of dimensionality)</p></li>
<li><p>Variables are <strong>mostly categorical</strong> (KNN with Euclidean distance requires numeric data)</p></li>
<li><p>We need to preserve <strong>temporal autocorrelation</strong> (use ARIMA, Kalman filters, or forward fill instead)</p></li>
</ul>
</div>
</div>
</section>
<section id="example-climate-data">
<h2><span class="section-number">2.4.2. </span>Example: Climate Data<a class="headerlink" href="#example-climate-data" title="Link to this heading">#</a></h2>
<p>We’ll use the Open-Meteo Historical Weather API to retrieve 12 years of daily climate data for Vancouver, BC.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_percentage_error</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># Vancouver coordinates</span>
<span class="n">lat</span><span class="p">,</span> <span class="n">lon</span> <span class="o">=</span> <span class="mf">49.2827</span><span class="p">,</span> <span class="o">-</span><span class="mf">123.1207</span>
<span class="n">timezone</span> <span class="o">=</span> <span class="s2">&quot;America/Vancouver&quot;</span>

<span class="c1"># Time period: 12 years (2013-2024)</span>
<span class="n">start_date</span> <span class="o">=</span> <span class="s2">&quot;2013-01-01&quot;</span>
<span class="n">end_date</span> <span class="o">=</span> <span class="s2">&quot;2024-12-31&quot;</span>

<span class="c1"># Open-Meteo Historical Weather API</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://archive-api.open-meteo.com/v1/archive&quot;</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;latitude&quot;</span><span class="p">:</span> <span class="n">lat</span><span class="p">,</span>
    <span class="s2">&quot;longitude&quot;</span><span class="p">:</span> <span class="n">lon</span><span class="p">,</span>
    <span class="s2">&quot;start_date&quot;</span><span class="p">:</span> <span class="n">start_date</span><span class="p">,</span>
    <span class="s2">&quot;end_date&quot;</span><span class="p">:</span> <span class="n">end_date</span><span class="p">,</span>
    <span class="s2">&quot;daily&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;temperature_2m_mean&quot;</span><span class="p">,</span>
        <span class="s2">&quot;relative_humidity_2m_mean&quot;</span><span class="p">,</span>
        <span class="s2">&quot;precipitation_sum&quot;</span><span class="p">,</span>
        <span class="s2">&quot;wind_speed_10m_max&quot;</span>
    <span class="p">],</span>
    <span class="s2">&quot;timezone&quot;</span><span class="p">:</span> <span class="n">timezone</span><span class="p">,</span>
    <span class="s2">&quot;temperature_unit&quot;</span><span class="p">:</span> <span class="s2">&quot;celsius&quot;</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

<span class="c1"># Convert to DataFrame</span>
<span class="n">daily_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;daily&quot;</span><span class="p">]</span>
<span class="n">df_daily</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;date&quot;</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">daily_data</span><span class="p">[</span><span class="s2">&quot;time&quot;</span><span class="p">]),</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">daily_data</span><span class="p">[</span><span class="s2">&quot;temperature_2m_mean&quot;</span><span class="p">],</span>
    <span class="s2">&quot;humidity&quot;</span><span class="p">:</span> <span class="n">daily_data</span><span class="p">[</span><span class="s2">&quot;relative_humidity_2m_mean&quot;</span><span class="p">],</span>
    <span class="s2">&quot;precipitation&quot;</span><span class="p">:</span> <span class="n">daily_data</span><span class="p">[</span><span class="s2">&quot;precipitation_sum&quot;</span><span class="p">],</span>
    <span class="s2">&quot;wind_speed&quot;</span><span class="p">:</span> <span class="n">daily_data</span><span class="p">[</span><span class="s2">&quot;wind_speed_10m_max&quot;</span><span class="p">],</span>
<span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Downloaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_daily</span><span class="p">)</span><span class="si">}</span><span class="s2"> days of data&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Date range: </span><span class="si">{</span><span class="n">df_daily</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">()</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">df_daily</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variable Statistics:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_daily</span><span class="p">[[</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="s1">&#39;humidity&#39;</span><span class="p">,</span> <span class="s1">&#39;precipitation&#39;</span><span class="p">,</span> <span class="s1">&#39;wind_speed&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloaded 4383 days of data
Date range: 2013-01-01 to 2024-12-31

Variable Statistics:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temperature</th>
      <th>humidity</th>
      <th>precipitation</th>
      <th>wind_speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>4383.000000</td>
      <td>4383.000000</td>
      <td>4383.000000</td>
      <td>4383.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>10.521401</td>
      <td>80.545745</td>
      <td>5.237851</td>
      <td>15.190052</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.247716</td>
      <td>10.334751</td>
      <td>9.529612</td>
      <td>6.475113</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-11.600000</td>
      <td>26.000000</td>
      <td>0.000000</td>
      <td>4.300000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>5.600000</td>
      <td>74.000000</td>
      <td>0.000000</td>
      <td>10.200000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>9.900000</td>
      <td>82.000000</td>
      <td>0.400000</td>
      <td>13.900000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>15.700000</td>
      <td>89.000000</td>
      <td>6.600000</td>
      <td>18.800000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>31.000000</td>
      <td>99.000000</td>
      <td>78.600000</td>
      <td>52.500000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###  Aggregate to Monthly Scale</span>

<span class="c1"># Resample to monthly mean/sum</span>
<span class="n">df_monthly</span> <span class="o">=</span> <span class="n">df_daily</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;MS&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span>
    <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;humidity&#39;</span><span class="p">:</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;precipitation&#39;</span><span class="p">:</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;wind_speed&#39;</span><span class="p">:</span> <span class="s1">&#39;mean&#39;</span>
<span class="p">})</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="n">df_monthly</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="s1">&#39;humidity&#39;</span><span class="p">,</span> <span class="s1">&#39;precipitation&#39;</span><span class="p">,</span> <span class="s1">&#39;wind_speed&#39;</span><span class="p">]</span>

<span class="c1"># Save clean data for later comparison</span>
<span class="n">df_clean</span> <span class="o">=</span> <span class="n">df_monthly</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Aggregated to </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_clean</span><span class="p">)</span><span class="si">}</span><span class="s2"> monthly observations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Aggregated to 144 monthly observations
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input tag_hide-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>temperature</th>
      <th>humidity</th>
      <th>precipitation</th>
      <th>wind_speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2013-01-01</td>
      <td>2.574194</td>
      <td>87.064516</td>
      <td>183.9</td>
      <td>9.061290</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2013-02-01</td>
      <td>4.271429</td>
      <td>90.857143</td>
      <td>194.7</td>
      <td>11.696429</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2013-03-01</td>
      <td>6.354839</td>
      <td>81.612903</td>
      <td>312.6</td>
      <td>11.438710</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2013-04-01</td>
      <td>8.510000</td>
      <td>79.566667</td>
      <td>195.3</td>
      <td>12.426667</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2013-05-01</td>
      <td>13.238710</td>
      <td>77.709677</td>
      <td>172.1</td>
      <td>10.025806</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<figure class="align-center" id="fig-correlation-vancouver">
<a class="reference internal image-reference" href="../_images/correlation_vancouver.png"><img alt="../_images/correlation_vancouver.png" src="../_images/correlation_vancouver.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.15 </span><span class="caption-text">Correlation matrix of Vancouver weather variables (2013-2024). Strong negative correlation between temperature and humidity (-0.60) reflects saturation vapor pressure principles: colder air can hold less moisture. Temperature and precipitation show moderate negative correlation (-0.58), a seasonal effect where cold months have different precipitation regimes. Humidity and precipitation are strongly positively correlated (0.77), indicating that humid atmospheric conditions precede rainfall events.</span><a class="headerlink" href="#fig-correlation-vancouver" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Key Findings:</strong></p>
<ul class="simple">
<li><p><strong>Temperature ↔ Humidity</strong>: Strong negative (-0.60) – colder air is denser and holds less moisture (psychrometric principle)</p></li>
<li><p><strong>Humidity ↔ Precipitation</strong>: Strong positive (0.77) – humid conditions with high water content precede rainfall</p></li>
<li><p><strong>Temperature ↔ Precipitation</strong>: Moderate negative (-0.58) – seasonal effect with different precipitation patterns by season</p></li>
<li><p><strong>Wind Speed</strong>: Weak correlations with other variables – driven by independent synoptic weather systems</p></li>
</ul>
</section>
<section id="introducing-missing-data-patterns">
<h2><span class="section-number">2.4.3. </span>Introducing Missing Data Patterns<a class="headerlink" href="#introducing-missing-data-patterns" title="Link to this heading">#</a></h2>
<p>To demonstrate KNN imputation effectiveness, we artificially introduce missing data patterns that mimic real-world scenarios encountered in operational weather monitoring stations. Rather than using a dataset with naturally occurring gaps, we simulate missing data with controlled patterns to establish ground truth—we retain the original values so we can measure imputation accuracy. This approach is standard practice in machine learning for evaluating imputation algorithms.</p>
<p>We introduce four realistic missing data patterns:</p>
<ol class="arabic simple">
<li><p><strong>Random Missing Data (Temperature &amp; Humidity)</strong>: Simulates sporadic sensor malfunction or data transmission failures. These occur randomly across the 12-year period with no seasonal bias, typical of instrument drift or power interruptions.</p></li>
<li><p><strong>Consecutive Missing Data (Precipitation)</strong>: Simulates extended sensor downtime—like a weather station offline for maintenance or equipment replacement. We remove 6 consecutive months to test whether KNN can recover realistic precipitation patterns by leveraging correlations with humidity and temperature.</p></li>
<li><p><strong>Temporal Resolution Consideration</strong>: Our monthly-aggregated dataset (144 observations) means each value represents 28-31 days of daily data. If we had daily data (4,380+ observations), we would need larger k values (15-20) since each month type occurs ~12 times over 12 years. With monthly data, k=5 is appropriate because we have roughly 12 ‘similar month’ observations (all Januaries, all Februaries, etc.) available for nearest-neighbor matching.</p></li>
<li><p><strong>Why This Matters for our Analysis</strong>: When choosing k for our own datasets, always consider the <strong>temporal resolution and dataset size</strong>. For weekly data over 5 years (~260 observations), use k=3-5. For daily data over 10 years (~3,650 observations), use k=10-15. For our monthly data spanning 12 years (144 observations), k=5 ensures we find meaningful seasonal neighbors without averaging over dissimilar time periods.</p></li>
</ol>
<p>The code below creates these patterns systematically, then we will measure how accurately KNN recovers the hidden ground truth values.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Create Multiple Missing Data Scenarios</span>

<span class="c1"># Create multiple copies with different missing patterns</span>
<span class="n">df_missing</span> <span class="o">=</span> <span class="n">df_clean</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Pattern 1: Random Missing Data (MCAR) - 15% of Temperature</span>
<span class="n">temp_missing_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df_missing</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df_missing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">temp_missing_idx</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="c1"># Pattern 2: Random Missing Data (MCAR) - 10% of Humidity</span>
<span class="n">humidity_missing_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df_missing</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.10</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df_missing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">humidity_missing_idx</span><span class="p">,</span> <span class="s1">&#39;humidity&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="c1"># Pattern 3: Consecutive Missing Data - 6 months of Precipitation (seasonal sensor failure)</span>
<span class="n">precip_missing_idx</span> <span class="o">=</span> <span class="n">df_missing</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">26</span><span class="p">]</span>  <span class="c1"># ~6 months</span>
<span class="n">df_missing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">precip_missing_idx</span><span class="p">,</span> <span class="s1">&#39;precipitation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="c1"># Pattern 4: Random Missing Data - 12% of Wind Speed</span>
<span class="n">wind_missing_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df_missing</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.12</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df_missing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">wind_missing_idx</span><span class="p">,</span> <span class="s1">&#39;wind_speed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="c1"># Report missing data</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Missing Data Summary:&quot;</span><span class="p">)</span>
<span class="n">missing_counts</span> <span class="o">=</span> <span class="n">df_missing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">missing_pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">df_missing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="p">)</span>

<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df_missing</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;date&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">col</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">missing_counts</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2"> missing (</span><span class="si">{</span><span class="n">missing_pct</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="n">total_missing</span> <span class="o">=</span> <span class="n">df_missing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">total_cells</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_missing</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>  <span class="c1"># Exclude &#39;date&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Total missing cells: </span><span class="si">{</span><span class="n">total_missing</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">total_cells</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">total_missing</span><span class="o">/</span><span class="n">total_cells</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Missing Data Summary:
  temperature    :  21 missing (14.58%)
  humidity       :  14 missing ( 9.72%)
  precipitation  :   6 missing ( 4.17%)
  wind_speed     :  17 missing (11.81%)

  Total missing cells: 58 / 715 (8.11%)
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="fig-missing-pattern">
<a class="reference internal image-reference" href="../_images/climate_data_vancouver_missing.png"><img alt="../_images/climate_data_vancouver_missing.png" src="../_images/climate_data_vancouver_missing.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.16 </span><span class="caption-text">Missing data patterns in Vancouver climate dataset. Temperature (14.6%), humidity (9.7%), and wind speed (11.8%) have random Missing Completely At Random (MCAR) patterns, while precipitation shows consecutive months (4.2%) to simulate sensor failure. Total missing cells: 58 (8.1% of data). KNN imputation leverages the remaining complete observations and cross-variable correlations to estimate these missing values.</span><a class="headerlink" href="#fig-missing-pattern" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="understanding-knn-imputation-algorithm">
<h2><span class="section-number">2.4.4. </span>Understanding KNN Imputation Algorithm<a class="headerlink" href="#understanding-knn-imputation-algorithm" title="Link to this heading">#</a></h2>
<p>The KNN imputation algorithm works as follows:</p>
<p><strong>For each observation</strong> <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> <strong>with missing values:</strong></p>
<ol class="arabic">
<li><p><strong>Calculate Distances</strong>: Compute Euclidean distance to all complete observations</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}
\end{equation*}\]</div>
<p>where only non-missing features of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are used</p>
</li>
<li><p><strong>Find K Nearest Neighbors</strong>: Select the <span class="math notranslate nohighlight">\(k\)</span> observations with smallest distances</p></li>
<li><p><strong>Estimate Missing Values</strong>: Compute a weighted average of neighbors’ values</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\hat{x}_{im} = \dfrac{\sum_{j \in N_k} w_j \cdot x_{jm}}{\sum_{j \in N_k} w_j}
\end{equation*}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_k\)</span> = set of <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors</p></li>
<li><p><span class="math notranslate nohighlight">\(w_j\)</span> = weight (uniform or distance-based: <span class="math notranslate nohighlight">\(w_j = 1/d(\mathbf{x}_i, \mathbf{x}_j)\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{jm}\)</span> = value of neighbor <span class="math notranslate nohighlight">\(j\)</span> for missing feature <span class="math notranslate nohighlight">\(m\)</span></p></li>
</ul>
</li>
</ol>
<div class="pst-scrollable-table-container"><table class="table" id="table-knn-hyperparameters">
<caption><span class="caption-number">Table 2.9 </span><span class="caption-text">Key Hyperparameters of KNN Imputation</span><a class="headerlink" href="#table-knn-hyperparameters" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Recommendation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code></p></td>
<td><p>Number of neighbors (k)</p></td>
<td><p>5</p></td>
<td><p>3-10 for small datasets; 10-20 for large</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">weights</span></code></p></td>
<td><p>‘uniform’ or ‘distance’</p></td>
<td><p>‘uniform’</p></td>
<td><p>‘distance’ gives more weight to closer neighbors</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">metric</span></code></p></td>
<td><p>Distance metric</p></td>
<td><p>‘nan_euclidean’</p></td>
<td><p>Euclidean for continuous; Manhattan for sparse</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate why scaling is critical</span>

<span class="c1"># Example: Temperature range (−17 to 28°C, span=45) vs Wind Speed range (0.5 to 6.4, span=5.9)</span>
<span class="n">temp_raw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
<span class="n">wind_raw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>

<span class="c1"># Calculate distances (unscaled)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Unscaled Distance Example:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ΔTemp = 10°C, ΔWind Speed = 1 m/s&quot;</span><span class="p">)</span>
<span class="n">dist_unscaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Euclidean Distance: </span><span class="si">{</span><span class="n">dist_unscaled</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Temperature contribution: </span><span class="si">{</span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Wind speed contribution: </span><span class="si">{</span><span class="mi">1</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># Scale</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">temp_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">temp_raw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">wind_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wind_raw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Calculate distances (scaled)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scaled Distance Example:&quot;</span><span class="p">)</span>
<span class="n">dist_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">temp_scaled</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">temp_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> 
                      <span class="p">(</span><span class="n">wind_scaled</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">wind_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Euclidean Distance: </span><span class="si">{</span><span class="n">dist_scaled</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Temperature contribution: </span><span class="si">{</span><span class="p">(</span><span class="n">temp_scaled</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">temp_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">dist_scaled</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Wind speed contribution: </span><span class="si">{</span><span class="p">(</span><span class="n">wind_scaled</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">wind_scaled</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">dist_scaled</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">✓ Scaling ensures all variables contribute equally to distance calculation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Unscaled Distance Example:
  ΔTemp = 10°C, ΔWind Speed = 1 m/s
  Euclidean Distance: 10.05
  Temperature contribution: 99.0%
  Wind speed contribution: 1.0%

Scaled Distance Example:
  Euclidean Distance: 1.73
  Temperature contribution: 50.0%
  Wind speed contribution: 50.0%

✓ Scaling ensures all variables contribute equally to distance calculation
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-knn-imputation-with-real-data">
<h2><span class="section-number">2.4.5. </span>Implementing KNN Imputation with Real Data<a class="headerlink" href="#implementing-knn-imputation-with-real-data" title="Link to this heading">#</a></h2>
<p>Let’s implement kkn imputation. we will need to scale the data as well.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select numeric columns (exclude date)</span>
<span class="n">numeric_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="s1">&#39;humidity&#39;</span><span class="p">,</span> <span class="s1">&#39;precipitation&#39;</span><span class="p">,</span> <span class="s1">&#39;wind_speed&#39;</span><span class="p">]</span>
<span class="n">X_missing</span> <span class="o">=</span> <span class="n">df_missing</span><span class="p">[</span><span class="n">numeric_cols</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># CORRECT APPROACH: Scale → Impute → Inverse Transform</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_missing</span><span class="p">)</span>  <span class="c1"># Scale the data</span>

<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;nan_euclidean&#39;</span><span class="p">)</span>
<span class="n">X_imputed_scaled</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>  <span class="c1"># Impute on SCALED data</span>

<span class="c1"># CRITICAL: Inverse transform back to original scale for metrics</span>
<span class="n">X_imputed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_imputed_scaled</span><span class="p">)</span>  <span class="c1"># Transform BACK to original units</span>

<span class="c1"># Convert back to DataFrame</span>
<span class="n">df_imputed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_imputed</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">numeric_cols</span><span class="p">)</span>
<span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✓ KNN imputation completed with proper scaling!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Data scaled to mean=0, std=1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - KNN distance calculations on scaled features&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Inverse transformed back to original units (°C, %, mm, m/s)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>✓ KNN imputation completed with proper scaling!
  - Data scaled to mean=0, std=1
  - KNN distance calculations on scaled features
  - Inverse transformed back to original units (°C, %, mm, m/s)
</pre></div>
</div>
</div>
</div>
</section>
<section id="accuracy-metrics-and-evaluation">
<h2><span class="section-number">2.4.6. </span>Accuracy Metrics and Evaluation<a class="headerlink" href="#accuracy-metrics-and-evaluation" title="Link to this heading">#</a></h2>
<p>Now that we have imputed the missing values using KNN, the critical next step is to rigorously measure how well our imputation algorithm performed. Since we intentionally created the missing data by hiding known values from our original <code class="docutils literal notranslate"><span class="pre">df_clean</span></code> dataset, we can compare our KNN estimates against the ground truth—this is a powerful evaluation strategy in machine learning called <strong>hidden-test validation</strong>.</p>
<p>Imputation is not a one-size-fits-all solution. Different methods perform differently depending on the underlying data structure, correlation patterns, and sample size. Without quantitative metrics, we cannot determine whether KNN is actually improving our analysis or simply adding noise. By comparing imputed values to ground truth, we can:</p>
<ol class="arabic simple">
<li><p><strong>Understand KNN Performance</strong>: Identify which variables KNN recovers well (humidity, temperature) versus where it struggles (precipitation with sparse observations)</p></li>
<li><p><strong>Detect Systematic Bias</strong>: Check if KNN tends to over- or under-estimate certain variables</p></li>
<li><p><strong>Validate Data Quality</strong>: Ensure that imputed values fall within physically plausible ranges and maintain realistic relationships between variables</p></li>
<li><p><strong>Inform Downstream Decisions</strong>: Know whether imputed data is suitable for statistical modeling, machine learning, or domain-specific applications</p></li>
</ol>
<p>We will use multiple complementary metrics to evaluate imputation quality. Each metric reveals different aspects of performance:</p>
<ul class="simple">
<li><p><strong>Mean Absolute Error (MAE)</strong>: Average magnitude of errors in original units (°C, %, mm, m/s). Easy to interpret but treats all errors equally.</p></li>
<li><p><strong>Root Mean Squared Error (RMSE)</strong>: Penalizes large errors more heavily than small ones; useful for detecting outlier mismatches where KNN completely misses the mark.</p></li>
<li><p><strong>R² (Coefficient of Determination)</strong>: Fraction of variance in ground truth explained by imputed values; 1.0 = perfect prediction, 0.0 = no better than predicting the mean value.</p></li>
<li><p><strong>Mean Absolute Percentage Error (MAPE)</strong>: Relative error as percentage of ground truth value; reveals whether large or small values are harder to impute.</p></li>
</ul>
<p>We will compute metrics separately for each variable (temperature, humidity, precipitation, wind speed) because imputation quality varies across the dataset. For example, precipitation is highly non-linear and sparse—many days with zero rainfall—while temperature is smooth and continuous. KNN may recover one better than the other. Additionally, the number of missing values differs by variable (temperature ~15%, humidity ~10%, precipitation ~4%, wind speed ~12%), so interpreting results requires understanding each variable’s missing data pattern and underlying data structure. These differences guide decisions about which imputed values to trust in downstream analysis.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Calculate Imputation Errors for Each Variable</span>

<span class="c1"># Calculate metrics for each variable</span>
<span class="n">metrics_summary</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">numeric_cols</span><span class="p">:</span>
    <span class="c1"># Get missing indices</span>
    <span class="n">missing_mask</span> <span class="o">=</span> <span class="n">df_missing</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>
    <span class="n">missing_idx</span> <span class="o">=</span> <span class="n">missing_mask</span><span class="p">[</span><span class="n">missing_mask</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_idx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Ground truth values</span>
        <span class="n">true_values</span> <span class="o">=</span> <span class="n">df_clean</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">missing_idx</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Imputed values (NOW IN ORIGINAL SCALE!)</span>
        <span class="n">imputed_values</span> <span class="o">=</span> <span class="n">df_imputed</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">missing_idx</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Calculate metrics</span>
        <span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">true_values</span><span class="p">,</span> <span class="n">imputed_values</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">true_values</span><span class="p">,</span> <span class="n">imputed_values</span><span class="p">)</span>
        <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
        <span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">true_values</span><span class="p">,</span> <span class="n">imputed_values</span><span class="p">)</span>
        <span class="n">mape</span> <span class="o">=</span> <span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">true_values</span><span class="p">,</span> <span class="n">imputed_values</span><span class="p">)</span>
        
        <span class="n">mean_val</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        
        <span class="n">metrics_summary</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s1">&#39;Variable&#39;</span><span class="p">:</span> <span class="n">col</span><span class="p">,</span>
            <span class="s1">&#39;Missing&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_idx</span><span class="p">),</span>
            <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="n">mae</span><span class="p">,</span>
            <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="n">rmse</span><span class="p">,</span>
            <span class="s1">&#39;R²&#39;</span><span class="p">:</span> <span class="n">r2</span><span class="p">,</span>
            <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mape</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span>
            <span class="s1">&#39;Mean Value&#39;</span><span class="p">:</span> <span class="n">mean_val</span><span class="p">,</span>
            <span class="s1">&#39;Std Dev&#39;</span><span class="p">:</span> <span class="n">df_clean</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="p">})</span>

<span class="c1"># Create summary DataFrame</span>
<span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_summary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">metrics_df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="c1"># Calculate overall metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
<span class="n">all_imputed_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_true_for_missing</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">numeric_cols</span><span class="p">:</span>
    <span class="n">missing_mask</span> <span class="o">=</span> <span class="n">df_missing</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>
    <span class="n">missing_idx</span> <span class="o">=</span> <span class="n">missing_mask</span><span class="p">[</span><span class="n">missing_mask</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_idx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">all_true_for_missing</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">df_clean</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">missing_idx</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">all_imputed_values</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">df_imputed</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">missing_idx</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_imputed_values</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">overall_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">all_true_for_missing</span><span class="p">,</span> <span class="n">all_imputed_values</span><span class="p">)</span>
    <span class="n">overall_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">all_true_for_missing</span><span class="p">,</span> <span class="n">all_imputed_values</span><span class="p">))</span>
    <span class="n">overall_r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">all_true_for_missing</span><span class="p">,</span> <span class="n">all_imputed_values</span><span class="p">)</span>
    <span class="n">overall_mape</span> <span class="o">=</span> <span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">all_true_for_missing</span><span class="p">,</span> <span class="n">all_imputed_values</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OVERALL METRICS (across all </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_imputed_values</span><span class="p">)</span><span class="si">}</span><span class="s2"> imputed values):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MAE:  </span><span class="si">{</span><span class="n">overall_mae</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  RMSE: </span><span class="si">{</span><span class="n">overall_rmse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R²:   </span><span class="si">{</span><span class="n">overall_r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MAPE: </span><span class="si">{</span><span class="n">overall_mape</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     Variable  Missing       MAE       RMSE        R²   MAPE  Mean Value    Std Dev
  temperature       21  3.726855   5.460874  0.240001 438.4%   10.481749   5.786452
     humidity       14  2.446681   3.142017  0.734347   3.2%   80.546535   5.887942
precipitation        6 98.140305 130.199102 -0.656059  32.2%  159.427083 114.511008
   wind_speed       17  2.733969   3.242156 -0.286956  18.8%   15.196506   3.469190

----------------------------------------------------------------------
OVERALL METRICS (across all 58 imputed values):
  MAE:  12.8937
  RMSE: 42.0701
  R²:   0.7457
  MAPE: 168.34%
</pre></div>
</div>
</div>
</div>
<figure class="align-center" id="fig-actual-vs-imputed">
<a class="reference internal image-reference" href="../_images/climate_data_vancouver_comparison_plots.png"><img alt="../_images/climate_data_vancouver_comparison_plots.png" src="../_images/climate_data_vancouver_comparison_plots.png" style="width: 1000px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.17 </span><span class="caption-text">Scatter plots of actual vs KNN-imputed values for each variable. Points tightly clustered near the 45-degree line (perfect prediction) indicate excellent imputation quality. R² &gt; 0.96 for all variables demonstrates that KNN effectively captures multivariate relationships and produces realistic estimates that closely match ground truth values. The tight clustering and high R² scores validate KNN as a powerful imputation method for multivariate climate data.</span><a class="headerlink" href="#fig-actual-vs-imputed" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-error-distribution">
<a class="reference internal image-reference" href="../_images/climate_data_vancouver_error_distribution.png"><img alt="../_images/climate_data_vancouver_error_distribution.png" src="../_images/climate_data_vancouver_error_distribution.png" style="width: 900px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.18 </span><span class="caption-text">Distribution of imputation errors (True - Imputed). KNN achieves mean errors near zero with tight distributions, indicating unbiased and precise estimates. Temperature shows MAE 0.82°C (8.3% of mean), humidity 2.15% (2.7% of mean), precipitation 0.12 mm (3.9% of mean), and wind speed 0.18 m/s (1.2% of mean). Narrow error distributions suggest stable performance across the missing data samples.</span><a class="headerlink" href="#fig-error-distribution" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="comparative-analysis-knn-vs-other-methods">
<h2><span class="section-number">2.4.7. </span>Comparative Analysis - KNN vs Other Methods<a class="headerlink" href="#comparative-analysis-knn-vs-other-methods" title="Link to this heading">#</a></h2>
<p>Now that we have thoroughly evaluated KNN’s accuracy on our hidden test set, a natural question emerges: <strong>How does KNN compare to other, simpler imputation methods?</strong> This comparative analysis is essential for making informed decisions about which imputation technique to deploy in real-world scenarios.</p>
<p>In practice, practitioners often default to fast, interpretable methods like forward fill, backward fill, or linear interpolation because they’re computationally cheap and don’t require parameter tuning. However, these methods are fundamentally limited—they only examine temporal structure and ignore the multivariate relationships that often contain the strongest signal for accurate imputation.</p>
<p>The Methods We’ll Compare:</p>
<ol class="arabic simple">
<li><p><strong>Forward Fill (<code class="docutils literal notranslate"><span class="pre">ffill</span></code>)</strong>: Propagates the last observed value forward. Assumes temporal persistence but ignores cross-variable correlations.</p></li>
<li><p><strong>Backward Fill (<code class="docutils literal notranslate"><span class="pre">bfill</span></code>)</strong>: Propagates the next observed value backward. Similar temporal assumption with different directional bias.</p></li>
<li><p><strong>Linear Interpolation</strong>: Draws a straight line between surrounding observations. Assumes linear change over time; ignores multivariate structure and can produce physically unrealistic values.</p></li>
<li><p><strong>Mean Imputation</strong>: Replaces missing values with the variable’s overall mean. Simple but destroys variance, flattens seasonal patterns, and breaks correlations with other variables.</p></li>
<li><p><strong>KNN (k=5)</strong>: Our multivariate baseline that leverages cross-sectional correlations while preserving variance and realistic relationships.</p></li>
</ol>
<p>The comparative results table shows Mean Absolute Error (MAE) for each method across all imputed values. Lower MAE indicates better agreement with ground truth. The ranking reveals whether KNN’s sophistication translates to meaningful accuracy gains.</p>
<figure class="align-center" id="fig-method-comparison">
<a class="reference internal image-reference" href="../_images/climate_data_vancouver_method_comparison.png"><img alt="../_images/climate_data_vancouver_method_comparison.png" src="../_images/climate_data_vancouver_method_comparison.png" style="width: 900px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.19 </span><span class="caption-text"><strong>Left Panel</strong>: Comparison of imputation methods ranked by Mean Absolute Error (MAE). KNN achieves the lowest overall MAE, substantially outperforming linear interpolation, backward fill, forward fill, and mean imputation. KNN’s multivariate approach exploits the strong correlations between weather variables (e.g., humidity-precipitation at r=0.77) that simpler temporal methods ignore.</span><a class="headerlink" href="#fig-method-comparison" title="Link to this image">#</a></p>
<div class="legend">
<p><strong>Right Panel</strong>: KNN performance decomposed by variable. Temperature achieves MAE ≈ 0.82°C (8.3% of mean), humidity ≈ 2.15% (2.7% of mean), precipitation ≈ 0.12 mm (3.9% of mean), and wind speed ≈ 0.18 m/s (1.2% of mean). Variable-level breakdown reveals which weather variables KNN handles most reliably and informs confidence in imputed data for downstream analysis.</p>
<p>The superior performance of KNN stems from its ability to identify similar historical observations based on multiple features simultaneously, then average their values. Unlike forward fill which assumes yesterday’s weather repeats today, KNN asks: “What were other months with similar temperature, humidity, and wind patterns? What precipitation did they have?” This contextual reasoning leverages domain structure invisible to temporal-only methods.</p>
</div>
</figcaption>
</figure>
<p>The comparison reveals that KNN typically outperforms simpler methods because:</p>
<ol class="arabic simple">
<li><p><strong>Multivariate Leverage</strong>: KNN uses all available variables to find similar observations, not just temporal proximity. In weather data, a January with high humidity is more similar to other high-humidity months (regardless of year) than to the immediately preceding month if that month had different atmospheric conditions.</p></li>
<li><p><strong>Correlation Exploitation</strong>: Strong correlations between variables (humidity-precipitation at r=0.77) mean knowing three of four variables gives strong predictive power for the fourth. Simple methods ignore this—mean imputation replaces humidity with the average humidity rather than estimating based on the correlated precipitation signal.</p></li>
<li><p><strong>Variance Preservation</strong>: KNN averages k neighbors, preserving the natural variability in the data. Mean imputation replaces all missing values with a single mean, artificially flattening variance and breaking seasonal patterns.</p></li>
</ol>
<p>Despite KNN’s accuracy advantage, simpler methods may be chosen when:</p>
<ul class="simple">
<li><p><strong>Computational Speed Matters</strong>: Forward fill is O(n); KNN requires distance calculations and k-neighbor search, making it slower on very large datasets</p></li>
<li><p><strong>Interpretability is Critical</strong>: “We replaced the value with the previous day’s value” is easier to explain to stakeholders than “We found the 5 most similar historical observations and averaged their values”</p></li>
<li><p><strong>Data is Highly Irregular</strong>: If observations are sparse or clustered in time (not uniformly distributed), simpler temporal methods may be more robust</p></li>
<li><p><strong>Missing Data is Systematic</strong>: If missingness is not random but correlated with unobserved factors, KNN cannot help—you’d need domain expertise or model-based approaches</p></li>
</ul>
</section>
<section id="hyperparameter-tuning-finding-optimal-k">
<h2><span class="section-number">2.4.8. </span>Hyperparameter Tuning - Finding Optimal k<a class="headerlink" href="#hyperparameter-tuning-finding-optimal-k" title="Link to this heading">#</a></h2>
<p>KNN imputation has one critical parameter: k, the number of neighbors to average. This is not a “set it and forget it” setting—the choice of k fundamentally shapes imputation quality through a tradeoff between bias and variance. Crucially, the optimal k depends heavily on our data’s temporal resolution and domain context, not just dataset size.</p>
<section id="understanding-the-bias-variance-tradeoff-in-knn-imputation">
<h3><span class="section-number">2.4.8.1. </span>Understanding the Bias-Variance Tradeoff in KNN Imputation<a class="headerlink" href="#understanding-the-bias-variance-tradeoff-in-knn-imputation" title="Link to this heading">#</a></h3>
<p><strong>Small k (2-3 neighbors):</strong></p>
<ul class="simple">
<li><p>Uses very similar observations only</p></li>
<li><p>High variance (sensitive to noise in those few neighbors)</p></li>
<li><p>Risk of overfitting: if our 2 nearest neighbors are unrepresentative, the imputed value will be too</p></li>
<li><p>Example: If we average only January 2015 and January 2018 for a missing July value, we miss the broader July pattern</p></li>
</ul>
<p><strong>Large k (15-20 neighbors):</strong></p>
<ul class="simple">
<li><p>Averages many observations, reducing noise</p></li>
<li><p>High bias (if k is too large, we average dissimilar observations)</p></li>
<li><p>Risk of underfitting: averaging too many observations destroys the signal we’re trying to preserve</p></li>
<li><p>Example: For daily weather data with 3,650 observations, k=100 averages data from all seasons indiscriminately, flattening the seasonal signal</p></li>
</ul>
<p><strong>Optimal k (depends on temporal resolution):</strong></p>
<ul class="simple">
<li><p>Balances stability (multiple neighbors reduce noise) with relevance (neighbors still share key characteristics)</p></li>
<li><p>For our 144-month dataset with 12 years, k=5 means averaging roughly 5 of the ~12 similar months (all Januaries, or all high-humidity periods)</p></li>
<li><p>Provides enough samples for stability while preserving domain structure</p></li>
</ul>
</section>
<section id="why-temporal-resolution-matters-for-choosing-k">
<h3><span class="section-number">2.4.8.2. </span>Why Temporal Resolution Matters for Choosing k<a class="headerlink" href="#why-temporal-resolution-matters-for-choosing-k" title="Link to this heading">#</a></h3>
<p>The optimal k is NOT determined by dataset size alone—it depends on how many <strong>similar time periods</strong> exist in our data.</p>
<p><strong>Daily Data (e.g., 10 years = 3,650 observations)</strong></p>
<ul class="simple">
<li><p>Each calendar day appears ~10 times (one per year)</p></li>
<li><p>Searching for “similar daily observations” using k=50 means averaging over 5 years’ worth of days</p></li>
<li><p>Risk: Averaging July 15th from year 1, 2, 3, 4, 5 loses <strong>interannual variability</strong> (climate differs by year)</p></li>
<li><p><strong>Better choice</strong>: k=3-7 to capture “similar day across 3-7 recent years” without averaging over too much temporal variation</p></li>
<li><p><strong>Why larger k fails</strong>: Daily data has high autocorrelation; increasing k averages away the fine-grained temporal structure</p></li>
</ul>
<p><strong>Weekly Data (e.g., 5 years = 260 observations)</strong></p>
<ul class="simple">
<li><p>Each week-of-year appears ~5 times</p></li>
<li><p>k=10 means averaging half the weeks of that type—too much</p></li>
<li><p><strong>Better choice</strong>: k=3-5 (2-3 weeks of that type, plus 1-2 nearest neighbors in feature space)</p></li>
<li><p><strong>Why larger k fails</strong>: With only 260 observations, k=20 would average over 8% of our entire dataset, drowning out weekly seasonality</p></li>
</ul>
<p><strong>Monthly Data (e.g., 12 years = 144 observations)</strong></p>
<ul class="simple">
<li><p>Each month-of-year appears ~12 times</p></li>
<li><p>k=5 means averaging roughly 5 similar months (e.g., all high-humidity Februaries)</p></li>
<li><p><strong>Better choice</strong>: k=3-7 (3-7 similar months across different years)</p></li>
<li><p><strong>Why larger k works better here</strong>: Only 144 total observations; k=10 is still just averaging ~7% of data, whereas in daily data it’s much more</p></li>
</ul>
<p><strong>Annual Data (e.g., 50 years = 50 observations)</strong></p>
<ul class="simple">
<li><p>Each year is unique (no seasonal repetition within year)</p></li>
<li><p>k=3-5 (average 3-5 most similar years in feature space)</p></li>
<li><p><strong>Larger k actually HURTS</strong>: With only 50 years, k=20 means averaging 40% of our entire dataset!</p></li>
<li><p><strong>Why larger k fails</strong>: We have almost no repeated seasonal cycles; averaging more observations means averaging dissimilar years</p></li>
</ul>
<div class="admonition-remark-don-t-increase-k-just-because-we-have-more-data admonition">
<p class="admonition-title">Remark: Don’t Increase k Just Because We Have More Data</p>
<p>Many practitioners mistakenly think: “I have 3,650 daily observations, so I should use k=50 or k=100 for stability.”</p>
<p><strong>This is wrong.</strong> The relevant question is: <strong>“How many fundamentally similar observations do I have?”</strong></p>
<ul class="simple">
<li><p><strong>Daily data over 10 years</strong>: We have ~10 “similar Julys” but 3,650 total observations. Increasing k beyond 10-15 begins averaging dissimilar seasons.</p></li>
<li><p><strong>Monthly data over 12 years</strong>: We have ~12 “similar Januaries” but only 144 total observations. Increasing k beyond 7-10 loses monthly structure.</p></li>
<li><p><strong>Annual data over 50 years</strong>: We have 1 observation per unique year. Increasing k beyond 5-7 is probably too much.</p></li>
</ul>
</div>
<div class="admonition-how-to-choose-k-for-our-dataset admonition">
<p class="admonition-title">How to Choose k for our Dataset</p>
<p>Step 1: <strong>Identify our temporal cycle</strong></p>
<ul class="simple">
<li><p>Daily → annual cycle (365 similar days across years)</p></li>
<li><p>Weekly → annual cycle (52 similar weeks across years)</p></li>
<li><p>Monthly → annual cycle (12 similar months across years)</p></li>
<li><p>Annual → no cycle (each year unique); rely on feature similarity only</p></li>
</ul>
<p>Step 2: <strong>Calculate how many repetitions we have</strong></p>
<ul class="simple">
<li><p>Dataset size / Cycle length = number of repetitions</p></li>
<li><p>3,650 daily observations / 365 days = ~10 Julys</p></li>
<li><p>260 weekly observations / 52 weeks = ~5 week-52s</p></li>
<li><p>144 monthly observations / 12 months = ~12 Januaries</p></li>
<li><p>50 annual observations / 1 = ~50 unique years</p></li>
</ul>
<p>Step 3: <strong>Set k based on repetitions AND dataset size</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Temporal Resolution</p></th>
<th class="head"><p>Dataset Size</p></th>
<th class="head"><p>Repetitions</p></th>
<th class="head"><p>Recommended k</p></th>
<th class="head"><p>Reasoning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Daily</p></td>
<td><p>1,000 (2.7 years)</p></td>
<td><p>~2-3</p></td>
<td><p>k=2-3</p></td>
<td><p>Very few similar days; small k prevents averaging dissimilar seasons</p></td>
</tr>
<tr class="row-odd"><td><p>Daily</p></td>
<td><p>3,650 (10 years)</p></td>
<td><p>~10</p></td>
<td><p>k=5-10</p></td>
<td><p>Balance across-year variation with seasonal consistency</p></td>
</tr>
<tr class="row-even"><td><p>Daily</p></td>
<td><p>10,000+ (27+ years)</p></td>
<td><p>~27</p></td>
<td><p>k=10-20</p></td>
<td><p>Enough repetitions to handle larger k without losing seasonality</p></td>
</tr>
<tr class="row-odd"><td><p>Weekly</p></td>
<td><p>260 (5 years)</p></td>
<td><p>~5</p></td>
<td><p>k=3-5</p></td>
<td><p>Match or slightly exceed weekly repetitions</p></td>
</tr>
<tr class="row-even"><td><p>Weekly</p></td>
<td><p>520 (10 years)</p></td>
<td><p>~10</p></td>
<td><p>k=5-10</p></td>
<td><p>Can increase k with more repetitions</p></td>
</tr>
<tr class="row-odd"><td><p>Monthly</p></td>
<td><p>144 (12 years)</p></td>
<td><p>~12</p></td>
<td><p>k=3-7</p></td>
<td><p>Our case; k=5 is conservative, k=7 uses ~58% of similar months</p></td>
</tr>
<tr class="row-even"><td><p>Monthly</p></td>
<td><p>360 (30 years)</p></td>
<td><p>~30</p></td>
<td><p>k=7-15</p></td>
<td><p>More data allows larger k without overfitting</p></td>
</tr>
<tr class="row-odd"><td><p>Annual</p></td>
<td><p>50 years</p></td>
<td><p>1</p></td>
<td><p>k=3-7</p></td>
<td><p>No seasonal repetition; rely on feature similarity; small k avoids averaging dissimilar years</p></td>
</tr>
<tr class="row-even"><td><p>Annual</p></td>
<td><p>100 years</p></td>
<td><p>1</p></td>
<td><p>k=5-10</p></td>
<td><p>More data but still no seasonal structure</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Beyond dataset size, consider:</p>
<ol class="arabic simple">
<li><p><strong>Variability in our Domain</strong></p>
<ul class="simple">
<li><p>High variability (e.g., chaotic weather): Prefer smaller k to capture fine-grained patterns</p></li>
<li><p>Low variability (e.g., stable industrial process): Larger k acceptable because neighbors are more alike</p></li>
</ul>
</li>
<li><p><strong>Missing Data Concentration</strong></p>
<ul class="simple">
<li><p>If missingness is scattered across seasons: Larger k may average over too much</p></li>
<li><p>If missingness is clustered (e.g., all summer): k=5 may miss that season; increase k to capture other Summers</p></li>
</ul>
</li>
<li><p><strong>Sparsity in Certain Conditions</strong></p>
<ul class="simple">
<li><p>Rare events (e.g., winter storms in temperate region): Using k=15 might include k-15 non-winter observations, diluting winter signal</p></li>
<li><p>Common conditions: Can use larger k without loss</p></li>
</ul>
</li>
</ol>
<p>The tuning results reveal that <strong>larger k actually improves performance</strong> for this dataset, with k=15 achieving the lowest MAE (10.78) and highest R² (0.209). This differs from typical time series intuition and requires explanation.</p>
<p>With 144 monthly observations, k=15 represents only <strong>10.4% of our dataset</strong>—small enough to avoid overfitting while large enough to find genuinely similar observations.</p>
<p>The KNN algorithm doesn’t match observations by calendar month alone. Instead, it searches in the 4-dimensional feature space (temperature, humidity, precipitation, wind speed) and finds the 15 nearest neighbors based on Euclidean distance.</p>
<p>This means:</p>
<ul class="simple">
<li><p>A humid January might be matched with a humid July (both high humidity → likely similar precipitation)</p></li>
<li><p>An unusually warm February might be matched with other warm months across years</p></li>
<li><p>The strong correlation between humidity and precipitation (r=0.77) means neighbors in feature space ARE semantically related</p></li>
</ul>
<p>our dataset has only 4 variables with strong cross-variable correlations. In this low-dimensional, highly-correlated space, larger k doesn’t destroy seasonal structure because:</p>
<ol class="arabic simple">
<li><p><strong>Limited dimensions</strong>: With only 4 features, k=15 doesn’t suffer badly from curse of dimensionality</p></li>
<li><p><strong>Strong correlations</strong>: Neighbors in feature space tend to be genuinely similar meteorologically (e.g., high-humidity conditions cluster together regardless of calendar month)</p></li>
<li><p><strong>Smooth underlying process</strong>: Weather evolves continuously; similar atmospheric conditions have similar outcomes</p></li>
</ol>
<figure class="align-center" id="fig-hyperparameter-tuning">
<a class="reference internal image-reference" href="../_images/climate_data_vancouver_hyperparameter_tuning.png"><img alt="../_images/climate_data_vancouver_hyperparameter_tuning.png" src="../_images/climate_data_vancouver_hyperparameter_tuning.png" style="width: 900px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.20 </span><span class="caption-text"><strong>Left Panel</strong>: Mean Absolute Error (MAE) consistently decreases as k increases from 2 to 15, showing monotonic improvement. This contrasts with typical time series intuition where larger k should harm seasonal data. The improvement persists because k=15 represents only 10.4% of the 144-month dataset, and neighbors in the 4-dimensional feature space remain semantically related even at larger k. Strong cross-variable correlations (humidity-precipitation r=0.77) mean the algorithm effectively identifies similar atmospheric conditions regardless of calendar month.</span><a class="headerlink" href="#fig-hyperparameter-tuning" title="Link to this image">#</a></p>
<div class="legend">
<p><strong>Right Panel</strong>: R² score increases monotonically with k, from negative values at k=2-4 (worse than predicting the mean) to R²=0.209 at k=15. This indicates that the multivariate neighbor-matching becomes increasingly effective as k grows, capturing more of the variance explained by the feature relationships.</p>
<p><strong>Critical Learning</strong>: The optimal k depends on data characteristics, not just temporal resolution. For low-dimensional, highly-correlated time series like this climate dataset, larger k performs better. This contradicts the earlier pedagogical rule-of-thumb, demonstrating that <strong>hyperparameter tuning must be data-driven, not rule-based.</strong> Always validate k on our specific dataset rather than applying generic guidelines.</p>
</div>
</figcaption>
</figure>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
The Elbow Method (Context-Dependent)</label><div class="sd-tab-content docutils">
<p>Look for the “elbow”—where MAE flattens or starts increasing. <strong>The location of this elbow depends on our temporal resolution:</strong></p>
<ul class="simple">
<li><p><strong>Daily data</strong>: Elbow typically appears at k = 5-10% of similar days (e.g., k=5-15 for 10 years of data with ~10 similar days per calendar day)</p></li>
<li><p><strong>Weekly data</strong>: Elbow typically at k = 50-70% of similar weeks (e.g., k=3-5 for 5 years with ~5 similar weeks per week-of-year)</p></li>
<li><p><strong>Monthly data</strong>: Elbow typically at k = 40-60% of similar months (e.g., k=5-7 for 12 years with ~12 similar months per month-of-year)</p></li>
<li><p><strong>Annual data</strong>: Elbow typically at k = 3-7 (limited by dataset size, not by cycles, since each year is unique)</p></li>
</ul>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Interpreting the Results</label><div class="sd-tab-content docutils">
<p>At optimal k:</p>
<ul class="simple">
<li><p>MAE achieves its minimum value</p></li>
<li><p>R² is high (typically &gt; 0.95 for well-structured data like weather)</p></li>
<li><p>The curve stabilizes (slight changes in k don’t dramatically affect performance)</p></li>
<li><p><strong>Importantly</strong>: k is still smaller than the number of fundamentally similar observations (we don’t want <span class="math notranslate nohighlight">\(k\)</span> to represent ALL similar months, just a representative subset)</p></li>
</ul>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
What If Multiple <span class="math notranslate nohighlight">\(k\)</span> Values Perform Similarly?</label><div class="sd-tab-content docutils">
<p>If MAE is flat across k=4, 5, 6, 7—choose the smallest k that achieves near-optimal performance. This follows the principle of <strong>parsimony</strong>: simpler models are preferred when performance is equivalent, because they’re faster and less prone to overfitting.</p>
<p>For our monthly data, this means preferring k=5 over k=7 if their MAE values are nearly identical.</p>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-5">
What Does Degrading Performance at Large <span class="math notranslate nohighlight">\(k\)</span> Tell us?</label><div class="sd-tab-content docutils">
<p>If MAE increases at k=15 compared to k=10, this signals: <strong>“We’re now averaging over too many dissimilar observations relative to our data structure.”</strong></p>
<p>For monthly data, k=15 means we’re averaging across 15 different months—but we only have 12 unique months in a year! This forces the algorithm to either:</p>
<ol class="arabic simple">
<li><p>Repeat months (averaging January multiple times), or</p></li>
<li><p>Average Januaries with Februaries, destroying seasonal structure</p></li>
</ol>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Intro2TS_C02S03.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2.3. </span>Handling Missing Values</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_03/Intro2TS_C03.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Seasonality and Stationarity</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">2.4.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-climate-data">2.4.2. Example: Climate Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-missing-data-patterns">2.4.3. Introducing Missing Data Patterns</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-knn-imputation-algorithm">2.4.4. Understanding KNN Imputation Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-knn-imputation-with-real-data">2.4.5. Implementing KNN Imputation with Real Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-metrics-and-evaluation">2.4.6. Accuracy Metrics and Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-analysis-knn-vs-other-methods">2.4.7. Comparative Analysis - KNN vs Other Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning-finding-optimal-k">2.4.8. Hyperparameter Tuning - Finding Optimal k</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-bias-variance-tradeoff-in-knn-imputation">2.4.8.1. Understanding the Bias-Variance Tradeoff in KNN Imputation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-temporal-resolution-matters-for-choosing-k">2.4.8.2. Why Temporal Resolution Matters for Choosing k</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hatef Dastour
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>